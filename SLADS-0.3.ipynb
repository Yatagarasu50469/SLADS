{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================\n",
    "#Program: SLADS_multichannel\n",
    "#Author(s): David Helminiak\n",
    "#Date Created: 4 October 2019\n",
    "#Date Last Modified: November 2019\n",
    "#Changelog: 0.1  - Migration, Integration for multichannel values - Oct. 2019\n",
    "#\n",
    "#==================================================================\n",
    "\n",
    "#==================================================================\n",
    "#ADDITIONAL NOTES:\n",
    "#==================================================================\n",
    "#Add Breakpoint anywhere in the program: \n",
    "#Tracer()()\n",
    "#\n",
    "#==================================================================\n",
    "\n",
    "#==================================================================\n",
    "#LIBRARY IMPORTS\n",
    "#==================================================================\n",
    "from __future__ import absolute_import, division, print_function\n",
    "#import tensorflow as tf\n",
    "#tf.enable_eager_execution() #Evaluate all operations without building graphs\n",
    "import cv2\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.matlib as matlib\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import os\n",
    "import pickle\n",
    "import PIL\n",
    "import PIL.ImageOps\n",
    "import math\n",
    "import natsort\n",
    "import glob\n",
    "import re\n",
    "import random\n",
    "import sys\n",
    "import scipy\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datetime import datetime\n",
    "from IPython import display\n",
    "from IPython.core.debugger import Tracer\n",
    "from joblib import Parallel, delayed\n",
    "from matplotlib.pyplot import figure\n",
    "from PIL import Image\n",
    "from scipy import misc\n",
    "from scipy import signal\n",
    "from scipy.io import loadmat\n",
    "from scipy.io import savemat\n",
    "from sklearn import linear_model\n",
    "from sklearn.utils import shuffle\n",
    "from skimage.util import view_as_windows as viewW\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from skimage.measure import compare_ssim\n",
    "from skimage import filters\n",
    "from sobol import *\n",
    "from tqdm.auto import tqdm\n",
    "#==================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#General information regarding samples, used for testing and best C value determination\n",
    "class Sample:\n",
    "    def __init__(self, name, images, massRanges, maskObject, mzWeights, resultsPath):\n",
    "        self.name = name\n",
    "        self.images = images\n",
    "        self.massRanges = massRanges\n",
    "        self.maskObject = maskObject\n",
    "        self.mzWeights = mzWeights\n",
    "        self.measuredImages = []\n",
    "        for rangeNum in range(0,len(massRanges)):\n",
    "            self.measuredImages.append(np.zeros([maskObject.width, maskObject.height]))\n",
    "        self.resultsPath = resultsPath\n",
    "\n",
    "#Sample information for training the regression model\n",
    "class trainingSample:\n",
    "    def __init__(self, name, images, maskObject, massRanges, measurementPerc, polyFeatures, reconImage, orderForRD, RD):\n",
    "        self.name = name\n",
    "        self.images = images\n",
    "        self.maskObject = maskObject\n",
    "        self.massRanges = massRanges\n",
    "        self.measurementPerc = measurementPerc\n",
    "        self.polyFeatures = polyFeatures\n",
    "        self.reconImage = reconImage\n",
    "        self.orderForRD = orderForRD\n",
    "        self.RD = RD\n",
    "\n",
    "class SLADSModel:\n",
    "    def __init__(self, massRange, theta, cValue):\n",
    "        self.massRange = massRange\n",
    "        self.theta = theta\n",
    "        self.cValue = cValue\n",
    "\n",
    "class Result():\n",
    "    def __init__(self, info, sample, avgImage, simulationFlag, animationFlag):\n",
    "        self.info = info\n",
    "        self.sample = sample\n",
    "        self.avgImage = avgImage\n",
    "        self.simulationFlag = simulationFlag\n",
    "        self.animationFlag = animationFlag\n",
    "        self.reconImages = []\n",
    "        self.masks = []\n",
    "        self.ERDValueNPs = []\n",
    "        self.TDList = []\n",
    "        self.MSEList = []\n",
    "        self.SSIMList = []\n",
    "        self.percMeasuredList = []\n",
    "        \n",
    "    def update(self, percMeasured, reconImage, maskObject, ERDValuesNP, iterNum, completedRunFlag):\n",
    "\n",
    "        #Save the model development\n",
    "        self.reconImages.append(reconImage)\n",
    "        self.masks.append(maskObject.mask.copy())\n",
    "        self.ERDValueNPs.append(ERDValuesNP.copy())\n",
    "\n",
    "        if self.simulationFlag:\n",
    "            \n",
    "            #Find statistics of interest\n",
    "            difference = np.sum(computeDifference(self.avgImage, reconImage, self.info.imageType))\n",
    "            TD = difference/maskObject.area\n",
    "            MSE = (np.sum((reconImage.astype(\"float\") - self.avgImage.astype(\"float\")) ** 2))/(float(maskObject.area))\n",
    "            SSIM = compare_ssim(reconImage.astype(\"float\"), self.avgImage.astype(\"float\"))\n",
    "\n",
    "            #Save them for each timestep\n",
    "            self.TDList.append(TD)\n",
    "            self.MSEList.append(MSE)\n",
    "            self.SSIMList.append(SSIM)\n",
    "            self.percMeasuredList.append(percMeasured)\n",
    "        \n",
    "        #If an animation will be produced and the run has completed\n",
    "        if self.animationFlag and completedRunFlag:\n",
    "            \n",
    "            #Setup directory addresses\n",
    "            dir_Animations = self.sample.resultsPath+ 'Animations/'\n",
    "            dir_AnimationVideos = dir_Animations + 'Videos/'\n",
    "            dir_AnimationFrames = dir_Animations + self.sample.name + '/'\n",
    "\n",
    "            if os.path.exists(dir_AnimationFrames): shutil.rmtree(dir_AnimationFrames)    \n",
    "            os.makedirs(dir_AnimationFrames)\n",
    "            \n",
    "        #If an animation should be produced and the run has completed for a simulation\n",
    "        if self.simulationFlag and self.animationFlag and completedRunFlag:\n",
    "            \n",
    "            #Normalize the ERDValuesNP arrays across entire series of images\n",
    "            ERDValuesNPMax = 0\n",
    "            for ERDValuesNP in self.ERDValueNPs:\n",
    "                if np.max(ERDValuesNP) > ERDValuesNPMax:\n",
    "                    ERDValuesNPMax = np.max(ERDValuesNP)\n",
    "            for ERDValuesNP in self.ERDValueNPs: ERDValuesNP *= (255.0/ERDValuesNPMax)\n",
    "             \n",
    "            #Generate each of the frames\n",
    "            for i in range(0, len(self.masks)):\n",
    "                saveLocation = dir_AnimationFrames + 'original_' + self.sample.name + '_iter_' + str(i+1) + '_perc_' + str(self.percMeasuredList[i]) + '.png'\n",
    "                \n",
    "                font = {'size' : 18}\n",
    "                plt.rc('font', **font)\n",
    "                f = plt.figure(figsize=(15,10))\n",
    "\n",
    "                f.subplots_adjust(top = 0.85)\n",
    "                f.subplots_adjust(wspace=0.15, hspace=0.2)\n",
    "                plt.suptitle(\"Percent Sampled: %.2f, Measurement Iteration: %.0f\\nSSIM: %.2f\" % (self.percMeasuredList[i], i+1, self.SSIMList[i]), fontsize=20, fontweight='bold', y = 0.95)\n",
    "                sub = f.add_subplot(2,2,1)\n",
    "                sub.imshow(self.avgImage * 255.0/self.avgImage.max(), cmap='hot')\n",
    "                sub.set_title('Ground-Truth')\n",
    "\n",
    "                sub = f.add_subplot(2,2,2)\n",
    "                sub.imshow(self.reconImages[i] * 255.0/self.reconImages[i].max(), cmap='hot')\n",
    "                sub.set_title('Reconstructed Image')\n",
    "\n",
    "                sub = f.add_subplot(2,2,3)\n",
    "                sub.imshow(self.masks[i], cmap='gray')\n",
    "                sub.set_title('Sampled Mask')\n",
    "\n",
    "                sub = f.add_subplot(2,2,4)\n",
    "                im = sub.imshow(self.ERDValueNPs[i], cmap='viridis', vmin=0, vmax=255)\n",
    "                sub.set_title('ERD Values')\n",
    "                cbar = f.colorbar(im, ax=sub, orientation='vertical', pad=0.01)\n",
    "                \n",
    "                plt.savefig(saveLocation, bbox_inches='tight')\n",
    "                plt.close()\n",
    "                \n",
    "                #Stretched variant\n",
    "                saveLocation = dir_AnimationFrames + 'stretched_' + self.sample.name + '_iter_' + str(i+1) + '_perc_' + str(self.percMeasuredList[i]) + '.png'\n",
    "                \n",
    "                font = {'size' : 18}\n",
    "                plt.rc('font', **font)\n",
    "                f = plt.figure(figsize=(15,15))\n",
    "\n",
    "                f.subplots_adjust(top = 0.85)\n",
    "                f.subplots_adjust(wspace=0.15, hspace=0.2)\n",
    "                plt.suptitle(\"Percent Sampled: %.2f, Measurement Iteration: %.0f\\nSSIM: %.2f\" % (self.percMeasuredList[i], i+1, self.SSIMList[i]), fontsize=20, fontweight='bold', y = 0.95)\n",
    "                sub = f.add_subplot(2,2,1)\n",
    "                sub.imshow(self.avgImage * 255.0/self.avgImage.max(), cmap='hot', aspect='auto')\n",
    "                sub.set_title('Ground-Truth')\n",
    "\n",
    "                sub = f.add_subplot(2,2,2)\n",
    "                sub.imshow(self.reconImages[i] * 255.0/self.reconImages[i].max(), cmap='hot', aspect='auto')\n",
    "                sub.set_title('Reconstructed Image')\n",
    "\n",
    "                sub = f.add_subplot(2,2,3)\n",
    "                sub.imshow(self.masks[i], cmap='gray', aspect='auto')\n",
    "                sub.set_title('Sampled Mask')\n",
    "\n",
    "                sub = f.add_subplot(2,2,4)\n",
    "                im = sub.imshow(self.ERDValueNPs[i], cmap='viridis', vmin=0, vmax=255, aspect='auto')\n",
    "                sub.set_title('ERD Values')\n",
    "                cbar = f.colorbar(im, ax=sub, orientation='vertical', pad=0.01)\n",
    "                \n",
    "                plt.savefig(saveLocation, bbox_inches='tight')\n",
    "                plt.close()\n",
    "            \n",
    "            #Form the full animation\n",
    "            dataFileNames = natsort.natsorted(glob.glob(dir_AnimationFrames + 'original_*.png'))\n",
    "            height, width, layers = cv2.imread(dataFileNames[0]).shape\n",
    "            animation = cv2.VideoWriter(dir_AnimationVideos + 'original' + self.sample.name + '.avi', cv2.VideoWriter_fourcc(*'MJPG'), 2, (width, height))\n",
    "            for specFileName in dataFileNames: animation.write(cv2.imread(specFileName))\n",
    "            animation.release()\n",
    "            animation = None\n",
    "            \n",
    "            dataFileNames = natsort.natsorted(glob.glob(dir_AnimationFrames + 'stretched_*.png'))\n",
    "            height, width, layers = cv2.imread(dataFileNames[0]).shape\n",
    "            animation = cv2.VideoWriter(dir_AnimationVideos + 'stretched_' + self.sample.name + '.avi', cv2.VideoWriter_fourcc(*'MJPG'), 2, (width, height))\n",
    "            for specFileName in dataFileNames: animation.write(cv2.imread(specFileName))\n",
    "            animation.release()\n",
    "            animation = None\n",
    "\n",
    "class Info:\n",
    "    def __init__(self, reconMethod, featReconMethod, neighborWeightsPower, numNeighbors, filterType, featDistCutoff, resolution, imageType, shouldUpdate, minRadius, maxRadius, incRadius, numNbrs):\n",
    "        self.reconMethod = reconMethod\n",
    "        self.featReconMethod = featReconMethod\n",
    "        self.neighborWeightsPower = neighborWeightsPower\n",
    "        self.numNeighbors = numNeighbors\n",
    "        self.filterType = filterType\n",
    "        self.featDistCutoff = featDistCutoff\n",
    "        self.resolution = resolution\n",
    "        self.imageType = imageType\n",
    "        self.shouldUpdate = shouldUpdate\n",
    "        self.minRadius = minRadius\n",
    "        self.maxRadius = maxRadius\n",
    "        self.incRadius = incRadius\n",
    "        self.numNbrs = numNbrs\n",
    "\n",
    "#Storage location for the stopping parameters\n",
    "class StopCondParams:\n",
    "    def __init__(self, area, threshold, JforGradient, minPercentage, maxPercentage):\n",
    "        if area<512**2+1:\n",
    "            self.beta = 0.001*(((18-math.log(area,2))/2)+1)\n",
    "        else:\n",
    "            self.beta = 0.001/(((math.log(area,2)-18)/2)+1)\n",
    "        self.threshold = threshold\n",
    "        self.JforGradient = JforGradient\n",
    "        self.minPercentage = minPercentage\n",
    "        self.maxPercentage = maxPercentage\n",
    "\n",
    "#Each sample needs a mask object\n",
    "class MaskObject():\n",
    "    def __init__(self, width, height, measurementPercs):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.area = width*height\n",
    "        self.initialMaskPts = []\n",
    "        self.percMasks = []\n",
    "        self.measuredIdxs = []\n",
    "        self.unMeasuredIdxs = []\n",
    "        self.initialMeasuredIdxs = []\n",
    "        self.initialUnMeasuredIdxs = []\n",
    "        self.unMeasuredIdxsList = []\n",
    "        self.measuredIdxsList = []\n",
    "        \n",
    "        #Generate a list of arrays contianing the x,y points that need to be scanned\n",
    "        self.linesToScan = []\n",
    "        for rowNum in np.arange(0,height,1):\n",
    "            line = []\n",
    "            for columnNum in np.arange(0, width, 1):\n",
    "                line.append(tuple([rowNum, columnNum]))\n",
    "            self.linesToScan.append(line)\n",
    "\n",
    "        #Generate the initial set of linesToScan\n",
    "        \n",
    "        #50%\n",
    "        self.originalLinesToScan = copy.copy(self.linesToScan)\n",
    "        self.initialMask = np.zeros([height, width])\n",
    "        lineIndex = int(height*0.5)\n",
    "        for pt in self.linesToScan[lineIndex]: \n",
    "            self.initialMask[tuple(pt)] = 1\n",
    "            self.initialMaskPts.append(pt)\n",
    "        self.delLine(lineIndex)\n",
    "        \n",
    "        #25%\n",
    "        lineIndex = int(height*0.25)\n",
    "        for pt in self.linesToScan[lineIndex]: \n",
    "            self.initialMask[tuple(pt)] = 1\n",
    "            self.initialMaskPts.append(pt)\n",
    "        self.delLine(lineIndex)\n",
    "        \n",
    "        #75%\n",
    "        lineIndex = int(height*0.75)\n",
    "        for pt in self.linesToScan[lineIndex]: \n",
    "            self.initialMask[tuple(pt)] = 1\n",
    "            self.initialMaskPts.append(pt)\n",
    "        self.delLine(lineIndex)\n",
    "        \n",
    "        self.initialLinesToScan = copy.copy(self.linesToScan)\n",
    "        self.initialMeasuredIdxs = np.transpose(np.where(self.initialMask == 1))\n",
    "        self.initialUnMeasuredIdxs = np.transpose(np.where(self.initialMask == 0))\n",
    "        \n",
    "        #Create random initial percentage masks using point measurements instead of full lines\n",
    "        for measurementPerc in measurementPercs:\n",
    "            self.mask = np.zeros([height, width])\n",
    "            self.mask = np.random.rand(height, width) < (measurementPerc/100)\n",
    "            self.percMasks.append(self.mask)\n",
    "            self.measuredIdxsList.append(np.transpose(np.where(self.mask == 1)))\n",
    "            self.unMeasuredIdxsList.append(np.transpose(np.where(self.mask == 0)))\n",
    "        \n",
    "        #Create random line masks, using full lines\n",
    "#        for measurementPerc in measurementPercs:\n",
    "#            self.mask = copy.copy(self.initialMask)\n",
    "#            self.linesToScan = copy.copy(self.initialLinesToScan)\n",
    "#            self.measuredIdxs = copy.copy(self.initialMeasuredIdxs)\n",
    "#            self.unMeasuredIdxs = copy.copy(self.initialUnMeasuredIdxs)\n",
    "#            while (np.sum(self.mask)/self.area)*100 < measurementPerc:\n",
    "#                lineIndex = int((np.random.rand(1)[0]*len(self.linesToScan)))\n",
    "#                for pt in self.linesToScan[lineIndex]: self.mask[tuple(pt)] = 1\n",
    "#                self.delLine(lineIndex)\n",
    "#            self.percMasks.append(self.mask)\n",
    "#            self.measuredIdxsList.append(np.transpose(np.where(self.mask == 1)))\n",
    "#            self.unMeasuredIdxsList.append(np.transpose(np.where(self.mask == 0)))\n",
    "    \n",
    "    #Update the mask given a set of new measurement locations\n",
    "    def update(self, newIdxs):\n",
    "        for pt in newIdxs: self.mask[tuple(pt)] = 1\n",
    "        self.measuredIdxs = np.transpose(np.where(self.mask == 1))\n",
    "        self.unMeasuredIdxs = np.transpose(np.where(self.mask == 0))\n",
    "    \n",
    "    #Reset the training sample's mask and linesToScan to nothing having been scanned\n",
    "    def reset(self):\n",
    "        self.mask = np.zeros([self.height, self.width])\n",
    "        self.linesToScan = copy.copy(self.originalLinesToScan)\n",
    "        self.measuredIdxs = np.transpose(np.where(self.mask == 1))\n",
    "        self.unMeasuredIdxs = np.transpose(np.where(self.mask == 0))\n",
    "            \n",
    "    def delLine(self, index):\n",
    "        self.linesToScan = np.delete(self.linesToScan, index, 0)\n",
    "        \n",
    "    def delPoints(self, pts):\n",
    "        for i in range(0,len(self.linesToScan)):\n",
    "            indexes = []\n",
    "            for pt in pts:\n",
    "                indexes.append([i for i, j in enumerate(self.linesToScan[i]) if j == pt])\n",
    "            indexes = [x for x in np.asarray(indexes).flatten().tolist() if x != []]\n",
    "            if len(indexes) > 0:\n",
    "                self.linesToScan[i] = np.delete(self.linesToScan[i], indexes,0).tolist()\n",
    "\n",
    "def runSLADS(info, sample, maskObject, theta, stopPerc, simulationFlag, trainPlotFlag, animationFlag):\n",
    "    \n",
    "    #Reinitialize the mask state as blank\n",
    "    maskObject.reset()\n",
    "    \n",
    "    #Has the stopping condition been met yet\n",
    "    completedRunFlag = False\n",
    "    \n",
    "    #Current iteration\n",
    "    iterNum = 1\n",
    "    \n",
    "    #Assume variable Classify=='N' (artifact from original implementation)\n",
    "    \n",
    "    #Perform weighted averaging for the ground-truth image\n",
    "    if simulationFlag:\n",
    "        npImages = []\n",
    "        for image in sample.images: npImages.append(np.asarray(image))\n",
    "        avgImage = np.average(np.asarray(npImages), axis=0, weights=sample.mzWeights)\n",
    "\n",
    "    \n",
    "    #Initialize stopping condition object\n",
    "    stopCondParams = StopCondParams(maskObject.area, 0, 50, 2, stopPerc)\n",
    "    \n",
    "    #Determine stoppingCondition function value\n",
    "    stopCondFuncVal = np.zeros((int((maskObject.area)*(stopCondParams.maxPercentage)/100)+10,2))\n",
    "    \n",
    "    #Perform the initial measurements\n",
    "    sample, maskObject = performMeasurements(sample, maskObject, maskObject.initialMeasuredIdxs, simulationFlag)\n",
    "    \n",
    "    #Perform initial reconstruction and ERD calculation\n",
    "    reconImage, reconValues, ERDValues, ERDValuesNP, reconImageList, reconValuesList, ERDValueList = avgReconAndERD(sample, info, iterNum, maskObject, theta, reconImage=None, reconValues=None, ERDValues=None, ERDValuesNP=None, newIdxs=None, maxIdxsVect=None, reconImageList=None, reconValuesList=None, ERDValueList=None)\n",
    "\n",
    "    #Determine percentage pixels measured initially\n",
    "    percMeasured = (np.sum(maskObject.mask)/maskObject.area)*100\n",
    "\n",
    "    #Retrieve the averaged image's measured values\n",
    "    measuredValues = np.asarray(avgImage)[maskObject.mask == 1]\n",
    "    \n",
    "    #Check for completion state here just in case, prior to loop!\n",
    "    completedRunFlag = checkStopCondFuncThreshold(stopCondParams, stopCondFuncVal, maskObject, measuredValues, iterNum)\n",
    "    \n",
    "    #Additional stopping condition for if there are no more linesToScan\n",
    "    if len(maskObject.linesToScan) == 0: completedRunFlag = True\n",
    "    \n",
    "    \n",
    "    #Initialize a result object\n",
    "    result = Result(info, sample, avgImage, simulationFlag, animationFlag)\n",
    "    result.update(percMeasured, reconImage, maskObject, ERDValuesNP, iterNum, completedRunFlag)\n",
    "    \n",
    "    #Until the stopping criteria has been met\n",
    "    with tqdm(total = float(stopPerc), desc = '% Sampled', leave = True) as pbar:\n",
    "        \n",
    "        #Initialize progress bar state according to % measured\n",
    "        pbar.n = round(percMeasured,2)\n",
    "        pbar.refresh()\n",
    "        \n",
    "        #Until the program has completed\n",
    "        while not completedRunFlag:\n",
    "            \n",
    "            #Step the iteration counter\n",
    "            iterNum += 1\n",
    "            \n",
    "            #Make a duplicate of the ReconValues for stop condition gradient test\n",
    "            oldReconValues = reconValues.copy()\n",
    "            \n",
    "            #Find next measurement locations\n",
    "            maskObject, newIdxs, maxIdxsVect = findNewMeasurementIdxs(info, maskObject, measuredValues, theta, reconValues, reconImage, ERDValues, ERDValuesNP)\n",
    "            \n",
    "            #Perform measurements\n",
    "            sample, maskObject = performMeasurements(sample, maskObject, newIdxs, simulationFlag)\n",
    "            \n",
    "            #Perform reconstruction and ERD calculation\n",
    "            reconImage, reconValues, ERDValues, ERDValuesNP, reconImageList, reconValuesList, ERDValueList = avgReconAndERD(sample, info, iterNum, maskObject, theta, reconImage, reconValues, ERDValues, ERDValuesNP, newIdxs, maxIdxsVect, reconImageList, reconValuesList, ERDValueList)\n",
    "            \n",
    "            #Update the percentage of pixels that have beene measured\n",
    "            percMeasured = (np.sum(maskObject.mask)/maskObject.area)*100\n",
    "            \n",
    "            #Evaluate the stop condition value \n",
    "            stopCondFuncVal = computeStopCondFuncVal(oldReconValues, measuredValues, stopCondParams, info, stopCondFuncVal, maxIdxsVect, iterNum, maskObject)\n",
    "            \n",
    "            #Retrieve the averaged image's measured values\n",
    "            measuredValues = np.asarray(avgImage)[maskObject.mask == 1]\n",
    "            \n",
    "            #Check the stopping condition\n",
    "            completedRunFlag = checkStopCondFuncThreshold(stopCondParams, stopCondFuncVal, maskObject, measuredValues, iterNum)\n",
    "            \n",
    "            #Additional stopping condition for if there are no more linesToScan\n",
    "            if len(maskObject.linesToScan) == 0: completedRunFlag = True\n",
    "            \n",
    "            #Store information to the resultsObject \n",
    "            result.update(percMeasured, reconImage, maskObject, ERDValuesNP, iterNum, completedRunFlag)\n",
    "\n",
    "            #Update the progress bar\n",
    "            pbar.n = round(percMeasured,2)\n",
    "            pbar.refresh()\n",
    "    \n",
    "    return result\n",
    "\n",
    "def findNewMeasurementIdxs(info, maskObject, measuredValues, theta, reconValues, reconImage, ERDValues, ERDValuesNP):\n",
    "    newIdxs = []\n",
    "    maxIdxsVect = []\n",
    "    \n",
    "    #Assuming manual angle selection; sum ERD for all lines\n",
    "    lineERDValues = []\n",
    "    for line in maskObject.linesToScan:\n",
    "        lineERDValues.append(np.nansum(ERDValuesNP[tuple([x[0] for x in line]), tuple([y[1] for y in line])]))\n",
    "    \n",
    "    lineToScanIdx = np.nanargmax(lineERDValues)\n",
    "    \n",
    "    #Set threshold for what ERD Values are worth scanning the locations of\n",
    "    threshold = 0\n",
    "    #threshold = np.mean(ERDValuesNP[np.where((ERDValuesNP > 0))])\n",
    "    \n",
    "    #Determine start and end points to measure between on the line with greatest sum ERD\n",
    "    startIDXFound = False\n",
    "    endIDXFound = False  \n",
    "    \n",
    "    #filteredIdxs = np.argwhere(ERDValuesNP[lineToScanIdx] > threshold)\n",
    "    #if len(filteredIdxs) > 0:\n",
    "    #    startIDX, endIDX = min(filteredIdxs), max(filteredIdxs)\n",
    "    #    newIdxs = filteredIdxs[startIDX:endIdx]\n",
    "    #else:\n",
    "    #     \n",
    "    \n",
    "    if lineERDValues[lineToScanIdx] > 0:\n",
    "        for idx in maskObject.linesToScan[lineToScanIdx]:\n",
    "            if ERDValuesNP[tuple(idx)] > threshold and not startIDXFound:\n",
    "                startIDX = idx\n",
    "                startIDXFound = True\n",
    "            if ERDValuesNP[tuple(idx)] > threshold:\n",
    "                endIDX = idx\n",
    "\n",
    "        #Add points between start and end idxs to the list if they were determined\n",
    "        if (startIDXFound):\n",
    "            for idx in maskObject.linesToScan[lineToScanIdx]:\n",
    "                if idx[0] >= startIDX[0] and idx[1] >= startIDX[1] and idx[0] <= endIDX[0] and idx[1] <= endIDX[1]:\n",
    "                    newIdxs.append(idx)\n",
    "        else:\n",
    "            #No location was found over the threshold, just scan the full line\n",
    "            newIdxs = maskObject.linesToScan[lineToScanIdx]\n",
    "    else:\n",
    "        #Just choose a line to scan...\n",
    "        newIdxs = maskObject.linesToScan[lineToScanIdx]\n",
    "\n",
    "    #Remove the line selected from further consideration\n",
    "    maskObject.delLine(lineToScanIdx)\n",
    "\n",
    "    ptArray = np.asarray(newIdxs).tolist()\n",
    "    for k in range(0, len(newIdxs)):\n",
    "        pt = ptArray[k]\n",
    "        if pt in maskObject.unMeasuredIdxs.tolist():\n",
    "            maxIdxsVect.append(maskObject.unMeasuredIdxs.tolist().index(pt))\n",
    "\n",
    "    newIdxs = np.asarray(newIdxs) #Actual coordinates to measure next\n",
    "    maxIdxsVect = np.asarray(maxIdxsVect) #Positions of those coordinates within the unmeasured listing\n",
    "\n",
    "    return maskObject, newIdxs, maxIdxsVect\n",
    "\n",
    "def avgReconAndERD(sample, info, iterNum, maskObject, theta, reconImage, reconValues, ERDValues, ERDValuesNP, newIdxs, maxIdxsVect, reconImageList, reconValuesList, ERDValueList):\n",
    "    \n",
    "    #Find neighbor information\n",
    "    neighborIndices, neighborWeights, neighborDistances = findNeighbors(info, maskObject.measuredIdxs, maskObject.unMeasuredIdxs)\n",
    "    \n",
    "    if iterNum >= 1:\n",
    "        #For all of the images in the sample, find the recon Images, Values, and ERD Values\n",
    "        reconImageList = []\n",
    "        reconValuesList = []\n",
    "        ERDValueList = []\n",
    "        for image in sample.measuredImages:\n",
    "            \n",
    "            #Retrieve the measured values\n",
    "            measuredValues = np.asarray(image)[maskObject.mask == 1]\n",
    "            \n",
    "            #Find neighborhood values\n",
    "            neighborValues = findNeighborValues(measuredValues, neighborIndices)\n",
    "\n",
    "            #Compute reconstructions\n",
    "            reconValues, reconImage = computeRecons(info, maskObject, maskObject.unMeasuredIdxs, maskObject.measuredIdxs, neighborValues, neighborWeights, measuredValues)\n",
    "\n",
    "            #Compute full ERD Values\n",
    "            ERDValues = computeFullERD(info, maskObject, measuredValues, reconValues, reconImage, theta, neighborValues, neighborWeights, neighborDistances)\n",
    "            \n",
    "            #Store results\n",
    "            reconImageList.append(reconImage)\n",
    "            reconValuesList.append(reconValues)\n",
    "            ERDValueList.append(ERDValues)\n",
    "\n",
    "    else:\n",
    "        sys.exit('Error! - Turned off the computeUpdateERD function!')\n",
    "        #Perform updates to the individual reconValues reconImages and ERDValues\n",
    "        for imageNum in range(0,len(sample.measuredImages)):\n",
    "            #Retrieve the measured values\n",
    "            measuredValues = np.asarray(sample.measuredImages[imageNum])[maskObject.mask == 1]\n",
    "            \n",
    "            #Update the ERD Values\n",
    "            reconValuesList[imageNum], reconImageList[imageNum], ERDValueList[imageNum] = computeUpdateERD(maskObject, measuredValues, theta, info, newIdxs, reconValuesList[imageNum], reconImageList[imageNum], ERDValueList[imageNum], maxIdxsVect)\n",
    "\n",
    "    #Perform weighted averaging\n",
    "    reconImage = np.average(np.asarray(reconImageList), axis=0, weights=sample.mzWeights)\n",
    "    reconValues = np.average(np.asarray(reconValuesList), axis=0, weights=sample.mzWeights)\n",
    "    ERDValues = np.average(np.asarray(ERDValueList), axis=0, weights=sample.mzWeights)\n",
    "\n",
    "    #Convert ERDValues from 1D to 2D\n",
    "    ERDValuesNP = makeERDArray(ERDValues, maskObject, reconImage)\n",
    "\n",
    "    return reconImage, reconValues, ERDValues, ERDValuesNP, reconImageList, reconValuesList, ERDValueList\n",
    "\n",
    "def computeFullERD(info, maskObject, measuredValues, reconValues, reconImage, theta, neighborValues, neighborWeights, neighborDistances):\n",
    "\n",
    "    # Compute features\n",
    "    polyFeatures = computeFeatures(maskObject.unMeasuredIdxs, maskObject.area, neighborValues, neighborWeights, neighborDistances, info, reconValues, reconImage)\n",
    "    \n",
    "    # Compute ERD\n",
    "    ERDValues = polyFeatures.dot(theta)\n",
    "    \n",
    "    return(ERDValues)\n",
    "\n",
    "def computeUpdateERD(maskObject, measuredValues, theta, info, newIdxs, reconValues, reconImage, ERDValues, maxIdxsVect):  \n",
    "\n",
    "    ERDValues = np.delete(ERDValues,(maxIdxsVect))\n",
    "    \n",
    "    reconValues = np.delete(reconValues,(maxIdxsVect))\n",
    "    \n",
    "    suggestedRadius = int(np.sqrt((1/np.pi)*(maskObject.area*info.numNbrs/np.shape(measuredValues)[0])))\n",
    "    \n",
    "    updateRadiusTemp = np.max([suggestedRadius, info.minRadius]);\n",
    "    \n",
    "    updateRadius = int(np.min([info.maxRadius, updateRadiusTemp]));\n",
    "\n",
    "    updateRadiusMat = np.zeros((maskObject.height, maskObject.width))\n",
    "    \n",
    "    while(True):\n",
    "        for newIdxNum in range(0,len(newIdxs)):\n",
    "            #height and width might be reversed\n",
    "            updateRadiusMat[max(newIdxs[newIdxNum][0]-updateRadius,0):min(newIdxs[newIdxNum][0]+updateRadius,maskObject.height)][:,max(newIdxs[newIdxNum][1]-updateRadius,0):min(newIdxs[newIdxNum][1]+updateRadius,maskObject.width)]=1\n",
    "        updateIdxs = np.where(updateRadiusMat[maskObject.mask==0]==1)\n",
    "        smallUnMeasuredIdxs = np.transpose(np.where(np.logical_and(maskObject.mask==0, updateRadiusMat==1)))\n",
    "        if smallUnMeasuredIdxs.size == 0:\n",
    "            updateRadius = int(updateRadius*info.incRadius)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    #Determine neighborhood information\n",
    "    smallNeighborIndicies, smallNeighborWeights, smallNeighborDistances = findNeighbors(info, maskObject.measuredIdxs, smallUnMeasuredIdxs)\n",
    "    \n",
    "    smallNeighborValues = findNeighborValues(measuredValues, smallNeighborIndicies)\n",
    "    \n",
    "    # Perform reconstruction\n",
    "    smallReconValues = computeWeightedMRecons(smallNeighborValues, smallNeighborWeights, info)\n",
    "    \n",
    "    reconImage[(np.logical_and(maskObject.mask==0, updateRadiusMat==1))] = smallReconValues\n",
    "    reconImage[maskObject.measuredIdxs[:,0], maskObject.measuredIdxs[:,1]] = measuredValues\n",
    "\n",
    "    # Compute features\n",
    "    smallPolyFeatures = computeFeatures(smallUnMeasuredIdxs, maskObject.area, smallNeighborValues, smallNeighborWeights, smallNeighborDistances, info, smallReconValues, reconImage)\n",
    "\n",
    "    # Compute ERD\n",
    "    smallERDValues = smallPolyFeatures.dot(theta)\n",
    "    reconValues[updateIdxs[0]] = smallReconValues\n",
    "    ERDValues[updateIdxs] = smallERDValues\n",
    "    \n",
    "    return(reconValues, reconImage, ERDValues)\n",
    "\n",
    "def makeERDArray(ERDValues, maskObject, reconImage):\n",
    "    #Rearrange ERD values into array; those that have already been measured have 0 ERD\n",
    "    ERDValuesNP = np.zeros([maskObject.height, maskObject.width])\n",
    "    \n",
    "    #Copy over ERD values for unmeasured points\n",
    "    for i in range(0, len(maskObject.unMeasuredIdxs)): ERDValuesNP[maskObject.unMeasuredIdxs[i][0], maskObject.unMeasuredIdxs[i][1]] = ERDValues[i]\n",
    "\n",
    "    #Remove values that are less than those already scanned (0 ERD)\n",
    "    ERDValuesNP[np.where((ERDValuesNP < 0))] = 0\n",
    "    \n",
    "    #The following masks the ERD Values array by the reconstruction image\n",
    "    #Dr. Ye has requested that this masking procedure, which limits scanning by the determined structure\n",
    "    #be deactivated\n",
    "    #ERDValuesNP[np.where((reconImage < np.std(reconImage)))] = 0\n",
    "    \n",
    "    return ERDValuesNP\n",
    "\n",
    "def checkStopCondFuncThreshold(stopCondParams, StopCondFuncVal, maskObject, measuredValues, iterNum):\n",
    "    \n",
    "    if stopCondParams.threshold == 0:\n",
    "        if np.shape(measuredValues)[0] >= round(maskObject.area*stopCondParams.maxPercentage/100):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        if np.shape(measuredValues)[0] >= round(maskObject.area*stopCondParams.maxPercentage/100):\n",
    "            return True\n",
    "        else:\n",
    "            if np.logical_and(((maskObject.area)*stopCondParams.minPercentage/100)<np.shape(measuredValues)[0], stopCondFuncVal[iterNum,0]<stopCondParams.threshold):\n",
    "                gradStopCondFunc = np.mean(stopCondFuncVal[iterNum,0]-stopCondFuncVal[iterNum-stopCondParams.JforGradient:iterNum-1,0])\n",
    "                if gradStopCondFunc < 0:\n",
    "                    return True\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "def computeStopCondFuncVal(reconValues, measuredValues, stopCondParams, info, stopCondFuncVal, maxIdxsVect, iterNum, maskObject):\n",
    "    \n",
    "    #Calculate the difference in values between the previous reconstruction values against the measured values\n",
    "    diff = 0\n",
    "    for i in range(0, len(maxIdxsVect)):\n",
    "        a = reconValues[maxIdxsVect[i]]\n",
    "        b = measuredValues[len(measuredValues)-len(maxIdxsVect)+i]\n",
    "        diff = computeDifference(a, b, info.imageType) + diff\n",
    "    diff = diff/len(measuredValues)\n",
    "    \n",
    "    if iterNum == 1:\n",
    "        stopCondFuncVal[iterNum,0] = stopCondParams.beta*diff\n",
    "    else:\n",
    "        stopCondFuncVal[iterNum,0] = ((1-stopCondParams.beta)*stopCondFuncVal[iterNum-1,0] + stopCondParams.beta*diff)\n",
    "    \n",
    "    stopCondFuncVal[iterNum,1] = np.shape(measuredValues)[0]\n",
    "    \n",
    "    return stopCondFuncVal               \n",
    "\n",
    "def findNeighbors(info, measuredIdxs, unMeasuredIdxs):\n",
    "    \n",
    "    neigh = NearestNeighbors(n_neighbors=info.numNeighbors)\n",
    "    neigh.fit(measuredIdxs)\n",
    "    neighborDistances, neighborIndices = neigh.kneighbors(unMeasuredIdxs)\n",
    "    neighborDistances = neighborDistances*info.resolution\n",
    "    unNormNeighborWeights = 1/np.power(neighborDistances, info.neighborWeightsPower)\n",
    "    sumOverRow = (np.sum(unNormNeighborWeights, axis=1))\n",
    "    neighborWeights = unNormNeighborWeights/sumOverRow[:, np.newaxis]\n",
    "    \n",
    "    return neighborIndices, neighborWeights, neighborDistances\n",
    "\n",
    "def findNeighborValues(measuredValues, neighborIndices):\n",
    "    return measuredValues[neighborIndices]\n",
    "\n",
    "def computeRecons(info, maskObject, unMeasuredIdxs, measuredIdxs, neighborValues, neighborWeights, measuredValues):\n",
    "    reconValues = computeWeightedMRecons(neighborValues, neighborWeights, info)\n",
    "    reconImage = np.zeros((maskObject.height, maskObject.width))\n",
    "    reconImage[unMeasuredIdxs[:,0], unMeasuredIdxs[:,1]] = reconValues\n",
    "    reconImage[measuredIdxs[:,0], measuredIdxs[:,1]] = measuredValues\n",
    "    return(reconValues, reconImage)\n",
    "\n",
    "def computeWeightedMRecons(neighborValues, neighborWeights, info):\n",
    "    if info.featReconMethod=='DWM':\n",
    "        classLabels = np.unique(neighborValues)\n",
    "        classWeightSums = np.zeros((np.shape(neighborWeights)[0], np.shape(classLabels)[0]))\n",
    "        for i in range(0,np.shape(classLabels)[0]):\n",
    "            tempFeats = np.zeros((np.shape(neighborWeights)[0], np.shape(neighborWeights)[1]))\n",
    "            np.copyto(tempFeats, neighborWeights)\n",
    "            tempFeats[neighborValues != classLabels[i]]=0\n",
    "            classWeightSums[:,i] = np.sum(tempFeats, axis=1)\n",
    "        reconValues = classLabels[np.argmax(classWeightSums, axis=1)]\n",
    "    elif info.featReconMethod=='CWM':\n",
    "        reconValues = np.sum(neighborValues*neighborWeights, axis=1)\n",
    "    return reconValues\n",
    "\n",
    "def computeFeatures(unMeasuredIdxs, area, neighborValues, neighborWeights, neighborDistances, info, reconValues, reconImage):\n",
    "    feature = np.zeros((np.shape(unMeasuredIdxs)[0],6))\n",
    "\n",
    "    # Compute std div features\n",
    "    diffVect = computeDifference(neighborValues, np.transpose(np.matlib.repmat(reconValues, np.shape(neighborValues)[1],1)), info.imageType)\n",
    "    feature[:,0] = np.sum(neighborWeights*diffVect, axis=1)\n",
    "    feature[:,1] = np.sqrt((1/info.numNeighbors)*np.sum(np.power(diffVect,2),axis=1))\n",
    "    \n",
    "    # Compute distance/density features\n",
    "    cutoffDist = np.ceil(np.sqrt((info.featDistCutoff/100)*(area/np.pi)))\n",
    "    feature[:,2] = neighborDistances[:,0]\n",
    "    neighborsInCircle= np.sum(neighborDistances <= cutoffDist, axis=1)\n",
    "    feature[:,3] = (1+(np.pi*(np.power(cutoffDist, 2))))/(1+np.sum(neighborDistances <= cutoffDist, axis=1))\n",
    "\n",
    "    # Compute gradient features\n",
    "    gradientImageX, gradientImageY = np.gradient(reconImage)\n",
    "\n",
    "    #Assume continuous features\n",
    "    gradientImageX = abs(gradientImageX)\n",
    "    gradientImageY = abs(gradientImageY)\n",
    "    feature[:,4] = gradientImageY[unMeasuredIdxs[:,0], unMeasuredIdxs[:,1]]\n",
    "    feature[:,5] = gradientImageX[unMeasuredIdxs[:,0], unMeasuredIdxs[:,1]]\n",
    "    \n",
    "    #Compute polyfeatures\n",
    "    polyFeatures = np.hstack([np.ones((np.shape(feature)[0],1)), feature])\n",
    "    for i in range(0, np.shape(feature)[1]):\n",
    "        for j in range(i, np.shape(feature)[1]):\n",
    "            temp = feature[:,i]*feature[:,j]\n",
    "            polyFeatures = np.column_stack([polyFeatures, feature[:,i]*feature[:,j]])\n",
    "    return polyFeatures\n",
    "\n",
    "def computeDifference(array1, array2, imageType):\n",
    "    if imageType == 'C':\n",
    "        return abs(array1-array2)\n",
    "    elif imageType == 'D':\n",
    "        difference = array1 != array2\n",
    "        return difference.astype(float)\n",
    "    else:\n",
    "        sys.exit('Error! - Unexpected imageType declared')\n",
    "        return 0\n",
    "    \n",
    "def cls(): #Clear console screen\n",
    "    os.system('cls' if os.name=='nt' else 'clear')   \n",
    "\n",
    "def cAreaCalc(c, neighborDistances, unMeasuredIdxs, windowSize, imgAsBlocksOnlyUnmeasured):\n",
    "    temp = np.zeros((windowSize*windowSize,len(unMeasuredIdxs)))\n",
    "    sigma = neighborDistances[:,0]/c\n",
    "    area = []\n",
    "    for index in tqdm(range(0,len(unMeasuredIdxs)), desc = 'Kernel'):     \n",
    "            ax = np.linspace(-(windowSize - 1) / 2., (windowSize - 1) / 2., windowSize)\n",
    "            xx, yy = np.meshgrid(ax, ax)\n",
    "            area.append(imgAsBlocksOnlyUnmeasured[:,index]*np.ravel(np.exp(-0.5 * (np.square(xx) + np.square(yy)) / np.square(sigma[index]))))\n",
    "\n",
    "    for i in range (0,len(unMeasuredIdxs)): temp[:,i] = area[i]\n",
    "    \n",
    "    #Determine how much \"area of uncertainty\" is possibly removed for a c value\n",
    "    return np.sum(temp, axis=0)\n",
    "    \n",
    "def generateGaussianKernel(sigma, windowSize):\n",
    "    GK1 = signal.gaussian(windowSize, std=sigma)\n",
    "    return np.outer(GK1, GK1).flatten()\n",
    "\n",
    "def percResults(results, perc_testingResults, precision):\n",
    "    percents = np.arange(min(np.hstack(perc_testingResults)),max(np.hstack(perc_testingResults))+precision, precision)\n",
    "    averages = []\n",
    "    finalPercents = []\n",
    "    for percent in percents:\n",
    "        values = []\n",
    "        for i in range(0,len(results)):\n",
    "            percList = np.array(perc_testingResults[i])\n",
    "            idx = np.argmin(np.abs(np.asarray(percList)-percent))\n",
    "            values.append(results[i][idx])\n",
    "        averageValue = np.mean(values)\n",
    "        if len(averages) == 0 or averageValue != averages[len(averages)-1]:\n",
    "            averages.append(np.mean(values))\n",
    "            finalPercents.append(percent)\n",
    "    return finalPercents, averages\n",
    "\n",
    "def testing_parhelper(testingSampleFolder, info, bestTheta, stopPerc, animationGeneration):\n",
    "    dataSampleName = os.path.basename(testingSampleFolder)\n",
    "    \n",
    "    #Obtain testing images\n",
    "    images = []\n",
    "    for imageFileName in natsort.natsorted(glob.glob(testingSampleFolder + '/*.' + 'csv'), reverse=False):\n",
    "        images.append(np.nan_to_num(np.loadtxt(imageFileName, delimiter=',')))\n",
    "        #images.append(Image.fromarray((libtiff.TIFF.open(imageFileName).read_image()*255.0).astype('uint8')).convert('L') )\n",
    "    \n",
    "    #Read in the width and height; when RAW files are supported\n",
    "    #width, height = pd.read_csv(testingSampleFolder+'/dimensions.csv', sep=',', header=None).values[0].tolist()\n",
    "    height, width = images[0].shape\n",
    "    \n",
    "    #Create a new maskObject\n",
    "    maskObject = MaskObject(width, height, measurementPercs=[])\n",
    "    \n",
    "    #How should the mz ranges be weighted (all equal for now)\n",
    "    mzWeights = np.ones(len(desiredMassRanges))/len(desiredMassRanges)\n",
    "    \n",
    "    #Define information as a new Sample object\n",
    "    testingSample = Sample(dataSampleName, images, desiredMassRanges, maskObject, mzWeights, dir_TestingResults)\n",
    "    \n",
    "    #Run SLADS till its stopping condition has been met\n",
    "    return runSLADS(info, testingSample, maskObject, bestTheta, stopPerc, simulationFlag=True, trainPlotFlag=False, animationFlag=animationGeneration)\n",
    "    \n",
    "def performMeasurements(sample, maskObject, newIdxs, simulationFlag):\n",
    "    #Update the maskObject according to the newIdxs\n",
    "    maskObject.update(newIdxs)\n",
    "    \n",
    "    #If performing training\n",
    "    if simulationFlag:\n",
    "        #Obtain values from the stored image information\n",
    "        for imageNum in range(0,len(sample.measuredImages)):\n",
    "            temp = np.asarray(sample.images[imageNum]).copy()\n",
    "            temp[maskObject.mask == 0] = 0\n",
    "            sample.measuredImages[imageNum] = temp.copy()\n",
    "    else:\n",
    "        sys.exit('ERROR! - Runtime variation for perform measurements has not yet been implemented!')\n",
    "        #For pt in newIdxs\n",
    "        #Read in the designated locations tuple(pt) to a .RAW file\n",
    "        #Perform discretizations for sample.mzRanges\n",
    "        #For each of the discretizations to be stored\n",
    "        #for imageNum in range(0,len(sample.measuredImages)):\n",
    "            #sample.measuredImages[imageNum][tuple(pt)] = correspondingValue\n",
    "    return sample, maskObject\n",
    "\n",
    "#==================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ESSENTIALLY NON-ESSENTIAL PARAMETERS\n",
    "#==================================================================\n",
    "#Running in a console/True, notebook/False\n",
    "consoleRunning = False\n",
    "\n",
    "#ESSENTIALLY NON-ESSENTIAL SETUP\n",
    "#==================================================================\n",
    "#Clear the screen\n",
    "cls()\n",
    "\n",
    "#Determine console size if applicable\n",
    "if consoleRunning:\n",
    "    consoleRows, consoleColumns = os.popen('stty size', 'r').read().split()\n",
    "else:\n",
    "    consoleRows, consoleColumns = 40, 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================\n",
    "#MAIN PROGRAM\n",
    "#==================================================================\n",
    "\n",
    "#GENERAL PARAMETERS: L-01\n",
    "#==================================================================\n",
    "#Is training of a model to be performed\n",
    "trainingModel = True\n",
    "\n",
    "#Is testing of a model to be performed\n",
    "testingModel = False\n",
    "\n",
    "#STATIC VARIABLE SETUP\n",
    "#==================================================================\n",
    "\n",
    "#What mass ranges should be used for the testing data\n",
    "desiredMassRanges = [[754.5212, 754.5513],\n",
    "      [782.5519, 782.5832],\n",
    "      [804.5358, 804.568],\n",
    "      [806.5514, 806.5837],\n",
    "      [808.567, 808.5994],\n",
    "      [810.5826, 810.6151],\n",
    "      [828.5353, 828.5685],\n",
    "      [830.5509, 830.5842],\n",
    "      [832.5665, 832.5999],\n",
    "      [856.5661, 856.6003]]\n",
    "\n",
    "#Sampling percentages for training\n",
    "measurementPercs = [1,5,10,20,30,40]\n",
    "\n",
    "# Possible c values for RD approximation\n",
    "cValues = np.array([1,2,4,8,16,32,64,128])\n",
    "\n",
    "#Type of Images: D - for discrete (binary) image; C - for continuous\n",
    "imageType = 'C'\n",
    "\n",
    "#Symmetric window size for approximate RD summation - 15 for 512x512\n",
    "windowSize = 27\n",
    "\n",
    "#Should a stopping threshold be found that corresponds to the best determined c value\n",
    "findStopThresh = False\n",
    "\n",
    "#ERD update parameters\n",
    "shouldUpdate = True\n",
    "minRadius = 3\n",
    "maxRadius = 10\n",
    "incRadius = 1.5\n",
    "numNbrs = 10\n",
    "\n",
    "#Initialize the information object for training and testing\n",
    "#reconMethod, featReconMethod, neighborWeightsPower, numNeighbors, filterType, featDistCutoff, resolution\n",
    "info = Info('CWM', 'CWM', 2, 10, 'Gaussian', 0.25, 1, imageType, shouldUpdate, minRadius, maxRadius, incRadius, numNbrs)\n",
    "\n",
    "#Should animations be generated during testing/implementation\n",
    "animationGeneration = True\n",
    "\n",
    "#Percent of reduction in distrotion limit for numRandomChoice determination\n",
    "percOfRD = 50\n",
    "\n",
    "#Stopping percentage of pixels\n",
    "stopPerc = 50\n",
    "\n",
    "#Set the number of available CPU threads\n",
    "num_threads = multiprocessing.cpu_count()-2 #Leave a couple to spare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PATH/DIRECTORY SETUP\n",
    "#==================================================================\n",
    "dir_ResultsAndData = '.' + os.path.sep + 'ResultsAndData' + os.path.sep\n",
    "dir_InputData = dir_ResultsAndData + 'InputData' + os.path.sep\n",
    "\n",
    "dir_TrainingData = dir_InputData + 'TrainingData' + os.path.sep\n",
    "dir_TestingData = dir_InputData + 'TestingData' + os.path.sep\n",
    "\n",
    "dir_Results = dir_ResultsAndData + 'Results' + os.path.sep\n",
    "\n",
    "dir_TrainingResults = dir_Results + 'TrainingResults' + os.path.sep\n",
    "dir_TrainingResultsImages = dir_TrainingResults + 'Images' + os.path.sep\n",
    "\n",
    "dir_TestingResults = dir_Results + 'TestingResults' + os.path.sep\n",
    "dir_TestingResultsImages = dir_TestingResults + 'Images' + os.path.sep\n",
    "\n",
    "#Check general directory structure\n",
    "if not os.path.exists(dir_ResultsAndData):\n",
    "    sys.exit('Error - dir_ResultsAndData does not exist')\n",
    "\n",
    "#Input data directories\n",
    "if not os.path.exists(dir_InputData):\n",
    "    sys.exit('Error - dir_InputData does not exist')\n",
    "if not os.path.exists(dir_TrainingData) and trainingModel:\n",
    "    sys.exit('Error - dir_TrainingData does not exist')\n",
    "if not os.path.exists(dir_TestingData) and testingModel:\n",
    "    sys.exit('Error - dir_InputData does not exist')\n",
    "\n",
    "#Results directories - reset results folders for new runs\n",
    "if not os.path.exists(dir_Results):\n",
    "    sys.exit('Error - dir_Results does not exist')\n",
    "if trainingModel:\n",
    "    if os.path.exists(dir_TrainingResults):\n",
    "        shutil.rmtree(dir_TrainingResults)\n",
    "    os.makedirs(dir_TrainingResults)\n",
    "    os.makedirs(dir_TrainingResultsImages)    \n",
    "    \n",
    "if testingModel:\n",
    "    if os.path.exists(dir_TestingResults):\n",
    "        shutil.rmtree(dir_TestingResults)    \n",
    "    os.makedirs(dir_TestingResults)\n",
    "    os.makedirs(dir_TrainingResultsImages)\n",
    "\n",
    "if animationGeneration:\n",
    "    dir_Animations = dir_TestingResults + 'Animations/'\n",
    "    dir_AnimationVideos = dir_Animations + 'Videos/'\n",
    "\n",
    "    if os.path.exists(dir_Animations): shutil.rmtree(dir_Animations)    \n",
    "    os.makedirs(dir_Animations)\n",
    "    if os.path.exists(dir_AnimationVideos): shutil.rmtree(dir_AnimationVideos)    \n",
    "    os.makedirs(dir_AnimationVideos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'consoleColumns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-aad1c2052696>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'#'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconsoleColumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'PERFORMING INTITIAL COMPUTATIONS'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'#'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconsoleColumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#For each of the training samples, import their images in order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'consoleColumns' is not defined"
     ]
    }
   ],
   "source": [
    "print('\\n\\n' + ('#' * int(consoleColumns)))\n",
    "print('PERFORMING INTITIAL COMPUTATIONS')\n",
    "print(('#' * int(consoleColumns)) + '\\n')\n",
    "\n",
    "#For each of the training samples, import their images in order\n",
    "#Can switch this section to use .RAW file discretization method instead\n",
    "\n",
    "#Create a set of training samples for each possible c Value\n",
    "trainingDatabase = []\n",
    "for cNum in range(0,len(cValues)): trainingDatabase.append([])\n",
    "\n",
    "trainingSamples = []\n",
    "sortedTrainingSampleFolders = natsort.natsorted(glob.glob(dir_TrainingData + '/*'), reverse=False)    \n",
    "for sampleNum in tqdm(range(0,len(sortedTrainingSampleFolders)), desc = 'Training Samples', leave = True):\n",
    "    trainingSampleFolder = sortedTrainingSampleFolders[sampleNum]\n",
    "    \n",
    "    dataSampleName = os.path.basename(trainingSampleFolder)\n",
    "    images = []\n",
    "    massRanges = []\n",
    "\n",
    "    #Import each of the images according to their mz range order\n",
    "    for imageFileName in natsort.natsorted(glob.glob(trainingSampleFolder + '/*.' + 'csv'), reverse=False):\n",
    "        images.append(np.nan_to_num(np.loadtxt(imageFileName, delimiter=',')))\n",
    "        #images.append(Image.fromarray((libtiff.TIFF.open(imageFileName).read_image()*255.0).astype('uint8')).convert('L') )\n",
    "        massRanges.append([os.path.basename(imageFileName)[2:10], os.path.basename(imageFileName)[11:19]])\n",
    "    maskObject = MaskObject(images[0].shape[1], images[0].shape[0], measurementPercs)\n",
    "\n",
    "    #How should the mz ranges be weighted (all equal for now)\n",
    "    mzWeights = np.ones(len(images))/len(images)\n",
    "    \n",
    "    #Append the basic information for each of the provided samples for use in determining best c Value\n",
    "    trainingSamples.append(Sample(dataSampleName, images, massRanges, maskObject, mzWeights, dir_TrainingResults))\n",
    "    \n",
    "    #For each of the measurement percentages, extract features and initial RD values for each image\n",
    "    for measurementPercNum in tqdm(range(0,len(measurementPercs)), desc = 'Measurement %', leave = True):\n",
    "        measurementPerc = measurementPercs[measurementPercNum]\n",
    "        \n",
    "        #Retreive relevant mask information\n",
    "        mask = maskObject.percMasks[measurementPercNum]\n",
    "        measuredIdxs = maskObject.measuredIdxsList[measurementPercNum]\n",
    "        unMeasuredIdxs = maskObject.unMeasuredIdxsList[measurementPercNum]\n",
    "        \n",
    "        #Find neighbor information\n",
    "        neighborIndices, neighborWeights, neighborDistances = findNeighbors(info, measuredIdxs, unMeasuredIdxs)\n",
    "        \n",
    "        #Calculate the sigma values for each possible c\n",
    "        sigmaValues = []\n",
    "        for c in cValues: sigmaValues.append(neighborDistances[:,0]/c)\n",
    "        \n",
    "        #Flatten 2D mask array to 1D\n",
    "        maskVect = np.ravel(mask)\n",
    "        \n",
    "        #Create a random distribution, depending on # of unmeasured points\n",
    "        numRandChoices = int((100.0-measurementPercs[measurementPercNum])*percOfRD*maskObject.area/math.pow(10,math.ceil(math.log10(maskObject.area))))\n",
    "\n",
    "        orderForRD = random.sample(range(0,np.transpose(np.where(mask==0)).shape[0]), numRandChoices) \n",
    "        \n",
    "        #Form reconstructions and determine polyFeatures for each of the images\n",
    "        reconImageList = []\n",
    "        polyFeaturesList = []\n",
    "        for imNum in tqdm(range(0,len(images)), desc = 'Image Features', leave = True):\n",
    "            image = np.asarray(images[imNum])\n",
    "            \n",
    "            #Obtain the measured values\n",
    "            measuredValues = image[mask==1]\n",
    "            \n",
    "            #Find neighborhood values\n",
    "            neighborValues = findNeighborValues(measuredValues, neighborIndices)\n",
    "            \n",
    "            #Compute reconstructions\n",
    "            reconValues, reconImage = computeRecons(info, maskObject, unMeasuredIdxs, measuredIdxs, neighborValues, neighborWeights, measuredValues)\n",
    "            \n",
    "            #Compute features\n",
    "            polyFeatures = computeFeatures(unMeasuredIdxs, maskObject.area, neighborValues, neighborWeights, neighborDistances, info, reconValues, reconImage)\n",
    "            \n",
    "            #Extract random set of the polyFeatures\n",
    "            polyFeatures = polyFeatures[orderForRD,:]\n",
    "        \n",
    "            #Append to set for employment of averaging method\n",
    "            reconImageList.append(reconImage)\n",
    "            polyFeaturesList.append(polyFeatures)\n",
    "        \n",
    "        #Average the reconstructions according to the mzWeights\n",
    "        reconImage = np.average(np.asarray(reconImageList), axis=0, weights=mzWeights)\n",
    "        \n",
    "        #Average the polyFeatures according to the mzWeights\n",
    "        polyFeatures = np.average(np.asarray(polyFeaturesList), axis=0, weights=mzWeights)\n",
    "        \n",
    "        #Compute the difference between the original and reconstructed images\n",
    "        RDPP = computeDifference(image, reconImage, info.imageType)\n",
    "\n",
    "        #Convert differences to int\n",
    "        RDPP.astype(int)\n",
    "\n",
    "        #Pad with zeros\n",
    "        RDPPWithZeros = np.lib.pad(RDPP,(int(np.floor(windowSize/2)),int(np.floor(windowSize/2))), 'constant', constant_values=0)\n",
    "\n",
    "        #Convert image to an array; faster than im2col implementation\n",
    "        N = RDPPWithZeros.shape[1] - windowSize + 1\n",
    "        M = RDPPWithZeros.shape[0] - windowSize + 1\n",
    "        imgAsBlocks = viewW(RDPPWithZeros, (M,N)).reshape(-1,M*N)[:,::1]\n",
    "\n",
    "        #Identify the pixels that have not yet been measured\n",
    "        imgAsBlocksOnlyUnmeasured = imgAsBlocks[:,np.logical_not(maskVect)]\n",
    "\n",
    "        #Determine RD values for each of the c Values to finalize formation of the training dataset\n",
    "        temp = np.zeros((windowSize*windowSize, len(orderForRD)))\n",
    "        for cNum in tqdm(range(0,len(cValues)), desc = 'c Values', leave = True):\n",
    "            sigma = sigmaValues[cNum]\n",
    "            c = cValues[cNum]\n",
    "            #For each of the selected unmeasured points calculate the captured \"area\"\n",
    "            for index in tqdm(range(0,len(orderForRD)), desc = 'Gaussian', leave = True):\n",
    "                temp[:,index] = imgAsBlocksOnlyUnmeasured[:,orderForRD[index]]*generateGaussianKernel(sigma[orderForRD[index]], windowSize)\n",
    "            \n",
    "            #Determine RD; the \"area of uncertainty\" removed\n",
    "            RD = np.sum(temp, axis=0)\n",
    "            \n",
    "            #Add resultant information to the final trainingSamples database for the appropriate c Value\n",
    "            trainingDatabase[cNum].append(trainingSample(dataSampleName, images, maskObject, massRanges, measurementPerc, polyFeatures, reconImage, orderForRD, RD))\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('\\n\\n' + ('#' * int(consoleColumns)))\n",
    "print('TRAINING MODEL(S)')\n",
    "print(('#' * int(consoleColumns)) + '\\n')\n",
    "#There exists a single training sample for numCValues*numMeasurementPercs*numTrainingSamples\n",
    "\n",
    "#Find a SLADS model for each of the c values\n",
    "trainingModels = []\n",
    "for cNum in tqdm(range(0,len(cValues)), desc = 'c Values', leave = True): #For each of the proposed c values\n",
    "    trainingDataset = trainingDatabase[cNum]\n",
    "    for sampleNum in tqdm(range(0,len(trainingDataset)), desc = 'Training Data', leave = True):\n",
    "        trainingSample = trainingDataset[sampleNum]      \n",
    "        if sampleNum == 0: #First loop\n",
    "            if info.imageType == 'C':\n",
    "                bigPolyFeatures = trainingSample.polyFeatures\n",
    "                bigRD = trainingSample.RD\n",
    "            elif info.imageType == 'D':\n",
    "                bigPolyFeatures = np.column_stack((trainingSample.polyFeatures[:,0:25], trainingSample.polyFeatures[:,26]))\n",
    "                bigRD = trainingSample.RD\n",
    "        else: #Subsequent loops\n",
    "            if info.imageType == 'C':\n",
    "                bigPolyFeatures = np.row_stack((bigPolyFeatures, trainingSample.polyFeatures))\n",
    "                bigRD = np.append(bigRD, trainingSample.RD)\n",
    "            elif info.imageType == 'D':\n",
    "                tempPolyFeatures = np.column_stack((trainingSample.polyFeatures[:,0:25], trainingSample.polyFeatures[:,26]))\n",
    "                bigPolyFeatures = np.row_stack((bigPolyFeatures, tempPolyFeatures))\n",
    "                bigRD = np.append(bigRD, trainingSample.RD)\n",
    "\n",
    "    #Create least-squares regression model\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(bigPolyFeatures, bigRD)\n",
    "    \n",
    "    #Extract resultant theta values\n",
    "    if info.imageType == 'C':\n",
    "        theta = regr.coef_\n",
    "    elif info.imageType == 'D':\n",
    "        theta = np.zeros((trainingSample.polyFeatures.shape[1]))\n",
    "        theta[0:24] = regr.coef_[0:24]\n",
    "        theta[26] = regr.coef_[25]\n",
    "    \n",
    "    #Store the generated model\n",
    "    trainingModels.append(theta.copy())\n",
    "\n",
    "#Save the end models and the matched cValue order array\n",
    "np.save(dir_TrainingResults + 'cValues', cValues)\n",
    "pickle.dump(trainingSamples, open(dir_TrainingResults + 'trainingSamples.p', 'wb'))\n",
    "np.save(dir_TrainingResults + 'trainedModels', trainingModels)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\n\\n\\n' + ('-' * int(consoleColumns)))\n",
    "print('DETERMINING BEST C')\n",
    "print('-' * int(consoleColumns) + '\\n')\n",
    "\n",
    "#Load pre-trained data\n",
    "cValues = np.load(dir_TrainingResults + 'cValues.npy')\n",
    "trainingSamples = pickle.load(open(dir_TrainingResults + 'trainingSamples.p', 'rb'))\n",
    "trainingModels = np.load(dir_TrainingResults + 'trainedModels.npy')\n",
    "\n",
    "areaUnderCurveList = []\n",
    "#For each of the proposed c values\n",
    "for cNum in tqdm(range(0, len(cValues)), desc = 'c Values', leave = True):\n",
    "    theta = trainingModels[cNum]\n",
    "    \n",
    "    #Determine the sum total distortion remaining in the samples\n",
    "    areaUnderCurve = 0\n",
    "\n",
    "    #Run SLADS on all of the samples\n",
    "    results = Parallel(n_jobs=num_threads)(delayed(runSLADS)(info, trainingSamples[sampleNum], trainingSamples[sampleNum].maskObject, theta, stopPerc, simulationFlag=True, trainPlotFlag=False, animationFlag=False) for sampleNum in tqdm(range(0,len(trainingSamples)), desc = 'Training Samples', leave = True))\n",
    "\n",
    "    #Add resulting area under the curve to the rolling sum\n",
    "    for result in results: areaUnderCurve += np.trapz(result.TDList, result.percMeasuredList)\n",
    "    \n",
    "    #for sampleNum in tqdm(range(0,len(trainingSamples)), desc = 'Training Samples', leave = True):\n",
    "        \n",
    "        #Run SLADS till its stopping condition has been met\n",
    "        #result = runSLADS(info, trainingSamples[sampleNum], trainingSamples[sampleNum].maskObject, theta, stopPerc, simulationFlag=True, trainPlotFlag=False, animationFlag=False)\n",
    "        \n",
    "        #Add on the area under the curve to the rolling sum\n",
    "        #areaUnderCurve = areaUnderCurve + np.trapz(result.TDList, result.percMeasuredList)\n",
    "\n",
    "    #Append the total area under the curve\n",
    "    areaUnderCurveList.append(areaUnderCurve)\n",
    "\n",
    "#Select the c value and corresponding theta set that minimizes the summed total distortion across the samples\n",
    "bestIndex = np.argmin(areaUnderCurveList)\n",
    "bestC = cValues[bestIndex]\n",
    "bestTheta = trainingModels[bestIndex]\n",
    "\n",
    "# Find the Threshold on stopping condition that corresponds to the desired total distortion (TD) value set above\n",
    "if findStopThresh:   \n",
    "    sys.error('ERROR! - Automatic determination of a stopping threshold has not yet been fully implemented!')\n",
    "    #threshold = findStoppingThreshold(trainingDataPath,NumImagesForSLADS,Best_c,PercentageInitialMask,DesiredTD,reconPercVector,SizeImage)\n",
    "    #np.save(dir_TrainingResults + 'foundThreshold', threshold) \n",
    "\n",
    "#Save the best model\n",
    "np.save(dir_TrainingResults + 'bestC', bestC)\n",
    "np.save(dir_TrainingResults + 'bestTheta', bestTheta)\n",
    "\n",
    "#Load the best model\n",
    "bestC = np.load(dir_TrainingResults + 'bestC.npy')\n",
    "bestTheta = np.load(dir_TrainingResults + 'bestTheta.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\n' + ('#' * int(consoleColumns)))\n",
    "print('PLOT TRAINING CONVERGENCE ')\n",
    "print(('#' * int(consoleColumns)) + '\\n')\n",
    "#Not implemented at this time\n",
    "\n",
    "print('\\n\\n' + ('#' * int(consoleColumns)))\n",
    "print('PERFORMING TESTING')\n",
    "print(('#' * int(consoleColumns)) + '\\n')\n",
    "\n",
    "testingSamples = []\n",
    "sortedTestingSampleFolders = natsort.natsorted(glob.glob(dir_TestingData + '/*'), reverse=False) \n",
    "\n",
    "#Run SLADS on each of the testing samples\n",
    "results = Parallel(n_jobs=num_threads)(delayed(testing_parhelper)(sortedTestingSampleFolders[sampleNum], info, bestTheta, stopPerc, animationGeneration) for sampleNum in tqdm(range(0,len(sortedTestingSampleFolders)), desc = 'Testing Images', leave = True))\n",
    "    \n",
    "#Create holding arrays for all of the results\n",
    "MSE_testingResults = []\n",
    "SSIM_testingResults = []\n",
    "TD_testingResults = []\n",
    "perc_testingResults = []\n",
    "\n",
    "#Extract results from returned object; for every line scanned there is a value\n",
    "for result in results: \n",
    "    MSE_testingResults.append(result.MSEList)\n",
    "    SSIM_testingResults.append(result.SSIMList)\n",
    "    TD_testingResults.append(result.TDList)\n",
    "    perc_testingResults.append(result.percMeasuredList)\n",
    "\n",
    "precision = 0.01\n",
    "\n",
    "percents, testingSSIM_mean = percResults(SSIM_testingResults, perc_testingResults, precision)\n",
    "\n",
    "#Save average SSIM per percentage data\n",
    "np.savetxt(dir_TestingResults+'testingAverageSSIM_Percentage.csv', np.transpose([percents, testingSSIM_mean]), delimiter=',')\n",
    "\n",
    "#Save average SSIM per percentage data in plot\n",
    "font = {'size' : 18}\n",
    "plt.rc('font', **font)\n",
    "f = plt.figure(figsize=(20,8))\n",
    "ax1 = f.add_subplot(1,1,1)    \n",
    "ax1.plot(percents, testingSSIM_mean,color='black') \n",
    "ax1.set_xlabel('% Pixels Measured')\n",
    "ax1.set_ylabel('Average SSIM')\n",
    "plt.savefig(dir_TestingResults + 'testingAverageSSIM_Percentage' + '.png')\n",
    "plt.close()\n",
    "\n",
    "#Find the final results for each image\n",
    "lastSSIMResult = []\n",
    "lastMSEResult = []\n",
    "lastTDResult = []\n",
    "percLinesScanned = []\n",
    "percPixelsScanned = []\n",
    "for i in range(0, len(SSIM_testingResults)):\n",
    "    lastSSIMResult.append(SSIM_testingResults[i][len(SSIM_testingResults[i])-1])\n",
    "    lastMSEResult.append(MSE_testingResults[i][len(MSE_testingResults[i])-1])\n",
    "    lastTDResult.append(TD_testingResults[i][len(TD_testingResults[i])-1])\n",
    "    percLinesScanned.append((len(SSIM_testingResults[i])/len(maskObject.linesToScan))*100)\n",
    "    percPixelsScanned.append(perc_testingResults[i][len(perc_testingResults[i])-1])\n",
    "\n",
    "#Printout final results \n",
    "dataPrintout = []\n",
    "dataPrintout.append(['Average SSIM:', np.mean(lastSSIMResult), '+/-', np.std(lastSSIMResult)])\n",
    "dataPrintout.append(['Average MSE:', np.mean(lastMSEResult), '+/-', np.std(lastMSEResult)])\n",
    "dataPrintout.append(['Average TD:', np.mean(lastTDResult), '+/-', np.std(lastTDResult)])\n",
    "dataPrintout.append(['Average % Lines Scanned:', np.mean(percLinesScanned),'+/-', np.std(percLinesScanned)])\n",
    "dataPrintout.append(['Average % Pixels Scanned:', np.mean(percPixelsScanned),'+/-',np.std(percPixelsScanned)])\n",
    "pd.DataFrame(dataPrintout).to_csv(dir_TestingResults + 'dataPrintout.csv')\n",
    "\n",
    "#AFTER INTENDED PROCEDURES (TRAINING/TESTING) HAVE BEEN PERFORMED\n",
    "print('\\n\\n\\n' + ('#' * int(consoleColumns)))\n",
    "print('PROGRAM COMPLETE')\n",
    "print('#' * int(consoleColumns) + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#NonParallized testing Version; keep for troubleshooting purposes\n",
    "results = []\n",
    "for sampleNum in tqdm(range(0,len(sortedTestingSampleFolders)), desc = 'Testing Samples', leave = True):\n",
    "    testingSampleFolder = sortedTestingSampleFolders[sampleNum]\n",
    "    dataSampleName = os.path.basename(testingSampleFolder)\n",
    "    \n",
    "    #Obtain testing images\n",
    "    images = []\n",
    "    for imageFileName in natsort.natsorted(glob.glob(testingSampleFolder + '/*.' + 'tiff'), reverse=False):\n",
    "        #Temporary solution for reading in the .TIFF files; replace with reading direct from a csv...\n",
    "        images.append(Image.fromarray((libtiff.TIFF.open(imageFileName).read_image()*255.0).astype('uint8')).convert('L') )\n",
    "    \n",
    "    #Read in the width and height; when RAW files are supported\n",
    "    #width, height = pd.read_csv(testingSampleFolder+'/dimensions.csv', sep=',', header=None).values[0].tolist()\n",
    "    width, height = images[0].size\n",
    "    \n",
    "    #Create a new maskObject\n",
    "    maskObject = MaskObject(width, height, measurementPercs=[])\n",
    "    \n",
    "    #How should the mz ranges be weighted (all equal for now)\n",
    "    mzWeights = np.ones(len(desiredMassRanges))/len(desiredMassRanges)\n",
    "    \n",
    "    #Define information as a new Sample object\n",
    "    testingSample = Sample(dataSampleName, images, desiredMassRanges, maskObject, mzWeights, dir_TestingResults)\n",
    "    \n",
    "    #Run SLADS till its stopping condition has been met\n",
    "    result = runSLADS(info, testingSample, maskObject, bestTheta, stopPerc, simulationFlag=True, trainPlotFlag=False, animationFlag=animationGeneration)\n",
    "    \n",
    "    #Store the result\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
