{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================\n",
    "#Program: SLADS_TensorFlow_Simulation\n",
    "#Author(s): David Helminiak\n",
    "#Date Created: 13 February 2019\n",
    "#Date Last Modified: March 2019\n",
    "#Changelog: 0.1 - Combined Structure            - February 2019\n",
    "#           0.2 - Combined Train/Test           - February 2019\n",
    "#           0.3 - Gaussian CPU Multi-Threading  - February 2019\n",
    "#           0.4 - Restructuring of dir vars     - March 2019\n",
    "#           0.5 - Clearer code progress viz     - March 2019\n",
    "#           0.6 - Plotting Statistics           - March 2019\n",
    "#           0.7 - Line Scanning\n",
    "#           0.8 - .RAW usage\n",
    "#           0.9 - Continuous value prediction\n",
    "#==================================================================\n",
    "#==================================================================\n",
    "\n",
    "#==================================================================\n",
    "#ADDITIONAL NOTES:\n",
    "#==================================================================\n",
    "#Add Breakpoint anywhere in the program: \n",
    "#from IPython.core.debugger import Tracer; Tracer()() \n",
    "#==================================================================\n",
    "#==================================================================\n",
    "\n",
    "#==================================================================\n",
    "#LIBRARY IMPORTS\n",
    "#==================================================================\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution() #Evaluate all operations without building graphs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.matlib as matlib\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import os\n",
    "import PIL\n",
    "import math\n",
    "import glob\n",
    "import re\n",
    "import random\n",
    "import sys\n",
    "import scipy\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from IPython import display\n",
    "from joblib import Parallel, delayed\n",
    "from matplotlib.pyplot import figure\n",
    "from PIL import Image\n",
    "from scipy import misc\n",
    "from scipy.io import loadmat\n",
    "from scipy.io import savemat\n",
    "from sklearn import linear_model\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from skimage.measure import compare_ssim\n",
    "from tqdm.auto import tqdm\n",
    "from tensorflow import keras\n",
    "#=================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================\n",
    "#CLASS AND FUNCTION DEFINITIONS\n",
    "#==================================================================\n",
    "def plotErrorData(savePlotLocation, savePlotSuffix, trainingResultsAverageObject, xAxisValues, plotTitle, plotXLabel): #Plot and save the error data obtained during training\n",
    "\n",
    "    #Format the base plot\n",
    "    font = {'size' : 18}\n",
    "    plt.rc('font', **font)\n",
    "\n",
    "    #Add plot\n",
    "    f = plt.figure(figsize=(20,8))\n",
    "    ax1 = f.add_subplot()\n",
    "\n",
    "    #Add error plots as training data is recorded\n",
    "    if (len(trainingResultsAverageObject.mseAverageErrors) > 0):\n",
    "        plt.plot(xAxisValues, trainingResultsAverageObject.mseAverageErrors, color='black', linestyle='--') \n",
    "        plt.scatter(xAxisValues, trainingResultsAverageObject.mseAverageErrors, color='black') \n",
    "\n",
    "    #Label plot\n",
    "    plt.title(plotTitle)\n",
    "    plt.xlabel(plotXLabel)\n",
    "    plt.ylabel('Average MSE')\n",
    "\n",
    "    #Export image\n",
    "    plt.savefig(savePlotLocation + 'MSE' + savePlotSuffix + '.png')\n",
    "\n",
    "    #Add plot\n",
    "    f = plt.figure(figsize=(20,8))\n",
    "    ax1 = f.add_subplot()\n",
    "\n",
    "    #Add error plots as training data is recorded\n",
    "    if (len(trainingResultsAverageObject.ssimAverageErrors) > 0):\n",
    "        plt.plot(xAxisValues, trainingResultsAverageObject.ssimAverageErrors, color='black', linestyle='--') \n",
    "        plt.scatter(xAxisValues, trainingResultsAverageObject.ssimAverageErrors, color='black') \n",
    "\n",
    "    #Label plot\n",
    "    plt.title(plotTitle)\n",
    "    plt.xlabel(plotXLabel)\n",
    "    plt.ylabel('Average SSIM')\n",
    "\n",
    "    #Export image\n",
    "    plt.savefig(savePlotLocation + 'SSIM' + savePlotSuffix + '.png')\n",
    "\n",
    "    #Add plot\n",
    "    f = plt.figure(figsize=(20,8))\n",
    "    ax1 = f.add_subplot()\n",
    "\n",
    "    #Add error plots as training data is recorded\n",
    "    if (len(trainingResultsAverageObject.distortionAverageErrors) > 0):\n",
    "        plt.plot(xAxisValues, trainingResultsAverageObject.distortionAverageErrors, color='black', linestyle='--') \n",
    "        plt.scatter(xAxisValues, trainingResultsAverageObject.distortionAverageErrors, color='black') \n",
    "\n",
    "    #Label plot\n",
    "    plt.title(plotTitle)\n",
    "    plt.xlabel(plotXLabel)\n",
    "    plt.ylabel('Average % Total Distortion')\n",
    "\n",
    "    #Export image\n",
    "    plt.savefig(savePlotLocation + 'TotalDistortion' + savePlotSuffix + '.png')\n",
    "\n",
    "def ttPlotAverageErrors(savePlotLocation, StopPercentageSLADSArr, StopPercentageTestingSLADSArr, trainTestAverageErrors):\n",
    "\n",
    "    ttMSE = []\n",
    "    ttSSIM = []\n",
    "    ttTD = []\n",
    "\n",
    "    for i in range(0, len(trainTestAverageErrors)):\n",
    "        ttMSE.append(trainTestAverageErrors[i].mseAverageErrors)\n",
    "        ttSSIM.append(trainTestAverageErrors[i].ssimAverageErrors)\n",
    "        ttTD.append(trainTestAverageErrors[i].distortionAverageErrors)\n",
    "\n",
    "    xAxisValues = StopPercentageTestingSLADSArr.tolist()\n",
    "    labels = []\n",
    "    for i in StopPercentageSLADSArr.tolist(): labels.append(str(i))\n",
    "    linestyles = ['--', '-.', ':', '-']\n",
    "\n",
    "    #TD\n",
    "    font = {'size' : 18}\n",
    "    plt.rc('font', **font)\n",
    "    f = plt.figure(figsize=(20,8))\n",
    "    ax1 = f.add_subplot()\n",
    "    if (len(ttMSE) > 0):\n",
    "        for y, label, linestyle in zip(ttTD, labels, linestyles):\n",
    "            plt.plot(xAxisValues, y, color='black', linestyle = linestyle, label = label) \n",
    "            plt.scatter(xAxisValues, y, color='black') \n",
    "    plt.xlabel('% Sampled')\n",
    "    plt.ylabel('Average Total Distortion')\n",
    "    if (len(labels) > 1):\n",
    "        plt.legend(title='Training Sampling (%)')\n",
    "    plt.savefig(savePlotLocation + 'ttPlotAverageTD' + '.png')\n",
    "\n",
    "    #MSE\n",
    "    font = {'size' : 18}\n",
    "    plt.rc('font', **font)\n",
    "    f = plt.figure(figsize=(20,8))\n",
    "    ax1 = f.add_subplot()\n",
    "    if (len(ttMSE) > 0):\n",
    "        for y, label, linestyle in zip(ttMSE, labels, linestyles):\n",
    "            plt.plot(xAxisValues, y, color='black', linestyle = linestyle, label = label) \n",
    "            plt.scatter(xAxisValues, y, color='black') \n",
    "    plt.xlabel('% Sampled')\n",
    "    plt.ylabel('Average MSE')\n",
    "    if (len(labels) > 1):\n",
    "        plt.legend(title='Training Sampling (%)')\n",
    "    plt.savefig(savePlotLocation + 'ttPlotAverageMSE' + '.png')\n",
    "    \n",
    "    #SSIM\n",
    "    font = {'size' : 18}\n",
    "    plt.rc('font', **font)\n",
    "    f = plt.figure(figsize=(20,8))\n",
    "    ax1 = f.add_subplot()\n",
    "    if (len(ttMSE) > 0):\n",
    "        for y, label, linestyle in zip(ttSSIM, labels, linestyles):\n",
    "            plt.plot(xAxisValues, y, color='black', linestyle = linestyle, label = label) \n",
    "            plt.scatter(xAxisValues, y, color='black') \n",
    "    plt.xlabel('% Sampled')\n",
    "    plt.ylabel('Average SSIM')\n",
    "    if (len(labels) > 1):\n",
    "        plt.legend(title='Training Sampling (%)')\n",
    "    plt.savefig(savePlotLocation + 'ttPlotAverageSSIM' + '.png')\n",
    "    \n",
    "class simulationResults: #Object to hold local and global training information values for training convergence\n",
    "    def initialize(self):\n",
    "        self.mseAverageErrors = []\n",
    "        self.ssimAverageErrors = []\n",
    "        self.distortionAverageErrors = []\n",
    "    def saveErrorData(self, mseValue, ssimValue, distortValue): #Store training error information\n",
    "        self.mseError = mseValue\n",
    "        self.ssimError = ssimValue\n",
    "        self.totalDistortion = distortValue\n",
    "    def saveAverageErrorData(self, mseAverageValue, ssimAverageValue, distortionAverageValue): #Store training error information\n",
    "        self.mseAverageErrors.append(mseAverageValue)\n",
    "        self.ssimAverageErrors.append(ssimAverageValue)\n",
    "        self.distortionAverageErrors.append(distortionAverageValue)\n",
    "        \n",
    "#Storage location for training information definition\n",
    "class TrainingInfo:\n",
    "    def initialize(self,ReconMethod,FeatReconMethod, p, NumNbrs, FilterType, FilterC, FeatDistCutoff, MaxWindowForTraining,*args):\n",
    "        self.ReconMethod = ReconMethod\n",
    "        self.FeatReconMethod = FeatReconMethod\n",
    "        self.p = p\n",
    "        self.NumNbrs = NumNbrs        \n",
    "        self.FilterType = FilterType\n",
    "        self.FilterC = FilterC\n",
    "        self.FeatDistCutoff = FeatDistCutoff\n",
    "        self.MaxWindowForTraining=MaxWindowForTraining\n",
    "        if args:\n",
    "            self.PAP_Iter=args[0]\n",
    "            self.PAP_Beta=args[1]\n",
    "            self.PAP_InitType=args[2]\n",
    "            self.PAP_ScaleMax=args[3]\n",
    "        \n",
    "class TestingInfo:\n",
    "    def initialize(self):\n",
    "        self.mseTestingError = [] #Storage location for MSE testing error data\n",
    "        self.ssimTestingError = [] #Storage location for SSIM testing error data\n",
    "    def saveTestingErrorData(self, mseValue, ssimValue): #Store training error information\n",
    "        self.mseTestingError.append(mseValue)\n",
    "        self.ssimTestingError.append(ssimValue)\n",
    "    def plotTestingErrorData(self, savePlotLocation): #Plot and save the error data obtained during training\n",
    "        x = 1\n",
    "        #Export image\n",
    "        plt.savefig(savePlotLocation + 'testingStatistics.png')       \n",
    "\n",
    "        \n",
    "#Storage location for the initial mask definition\n",
    "class InitialMask:\n",
    "    def initialize(self,RowSz,ColSz,MaskType,MaskNumber,Percentage):\n",
    "        self.RowSz = RowSz\n",
    "        self.ColSz = ColSz\n",
    "        self.MaskType = MaskType\n",
    "        self.MaskNumber = MaskNumber\n",
    "        self.Percentage = Percentage\n",
    "\n",
    "#Storage location for the training stopping parameters\n",
    "class StopCondParams:\n",
    "    def initialize(self,Beta,Threshold,JforGradient,MinPercentage,MaxPercentage):\n",
    "        self.Beta = Beta\n",
    "        self.Threshold = Threshold\n",
    "        self.JforGradient = JforGradient\n",
    "        self.MinPercentage = MinPercentage\n",
    "        self.MaxPercentage = MaxPercentage\n",
    "\n",
    "#Storage location for ERD parameters\n",
    "class UpdateERDParams:\n",
    "    def initialize(self,Do,MinRadius,MaxRadius,IncreaseRadiusBy):\n",
    "        self.Do = Do\n",
    "        self.MinRadius = MinRadius\n",
    "        self.MaxRadius = MaxRadius\n",
    "        self.IncreaseRadiusBy = IncreaseRadiusBy\n",
    "\n",
    "#Storage location for batch sampling parameters\n",
    "class BatchSamplingParams:\n",
    "    def initialize(self,Do,NumSamplesPerIter):\n",
    "        self.Do = Do\n",
    "        self.NumSamplesPerIter = NumSamplesPerIter\n",
    "\n",
    "#Storage object for image data\n",
    "class imageData:\n",
    "    def __init__(self, data, name):\n",
    "        self.name = name\n",
    "        self.data = data\n",
    "        \n",
    "#Convert an image into a series of columns\n",
    "def im2col(Matrix,WidowSize):\n",
    "    M,N = Matrix.shape\n",
    "    col_extent = N - WidowSize[1] + 1\n",
    "    row_extent = M - WidowSize[0] + 1\n",
    "    start_idx = np.arange(WidowSize[0])[:,None]*N + np.arange(WidowSize[1])\n",
    "    offset_idx = np.arange(row_extent)[:,None]*N + np.arange(col_extent)\n",
    "    out = np.take (Matrix,start_idx.ravel()[:,None] + offset_idx.ravel())\n",
    "    return(out)\n",
    "    # http://stackoverflow.com/questions/30109068/implement-matlabs-im2col-sliding-in-python\n",
    "\n",
    "def generateGaussianKernel(sigma,WindowSize):\n",
    "    FilterMat = np.ones((WindowSize[0],WindowSize[1]))\n",
    "    for i in range(0,WindowSize[0]):\n",
    "        for j in range(0,WindowSize[1]):\n",
    "            FilterMat[i][j]=np.exp( -(1/(2*sigma**2)) * np.absolute( ( (i-np.floor(WindowSize[0]/2))**2 +  (j-np.floor(WindowSize[1]/2))**2 ) )  )\n",
    "    FilterMat = FilterMat/np.amax(FilterMat)\n",
    "    FilterMat = np.transpose(FilterMat)\n",
    "    Filter=np.ravel(FilterMat)\n",
    "    return Filter\n",
    "\n",
    "def computeFullERD(MeasuredValues,MeasuredIdxs,UnMeasuredIdxs,Theta,SizeImage,TrainingInfoObject,Resolution,ImageType):\n",
    "\n",
    "    NeighborValues,NeighborWeights,NeighborDistances = FindNeighbors(TrainingInfoObject,MeasuredIdxs,UnMeasuredIdxs,MeasuredValues,Resolution)\n",
    "\n",
    "    ReconValues,ReconImage = ComputeRecons(TrainingInfoObject,NeighborValues,NeighborWeights,SizeImage,UnMeasuredIdxs,MeasuredIdxs,MeasuredValues)\n",
    "    \n",
    "    # Compute features\n",
    "    PolyFeatures=computeFeatures(UnMeasuredIdxs,SizeImage,NeighborValues,NeighborWeights,NeighborDistances,TrainingInfoObject,ReconValues,ReconImage,Resolution,ImageType)\n",
    "    \n",
    "    # Compute ERD\n",
    "    ERDValues = PolyFeatures.dot(Theta)\n",
    "    \n",
    "    return(ERDValues,ReconValues,ReconImage)\n",
    "\n",
    "def updateERD(Mask,MeasuredIdxs,UnMeasuredIdxs,MeasuredValues,Theta,SizeImage,TrainingInfoObject,Resolution,ImageType,NewIdxs,NumSamples,UpdateERDParamsObject,ReconValues,ReconImage,ERDValues,MaxIdxsVect,BatchSamplingParamsObject):\n",
    "\n",
    "    ERDValues=np.delete(ERDValues,(MaxIdxsVect))\n",
    "    ReconValues=np.delete(ReconValues,(MaxIdxsVect))\n",
    "    SuggestedRadius = int(np.sqrt((1/np.pi)*(SizeImage[0]*SizeImage[1]*TrainingInfoObject.NumNbrs/NumSamples)))\n",
    "    UpdateRadiusTemp=np.max([SuggestedRadius,UpdateERDParamsObject.MinRadius]);\n",
    "    UpdateRadius=int(np.min([UpdateERDParamsObject.MaxRadius,UpdateRadiusTemp]));\n",
    "\n",
    "    updateRadiusMat = np.zeros((SizeImage[0],SizeImage[1]))\n",
    "    Done=0\n",
    "    while(Done==0):\n",
    "        if BatchSamplingParamsObject.Do == 'N':\n",
    "            updateRadiusMat[max(NewIdxs[0]-UpdateRadius,0):min(NewIdxs[0]+UpdateRadius,SizeImage[0])][:,max(NewIdxs[1]-UpdateRadius,0):min(NewIdxs[1]+UpdateRadius,SizeImage[1])]=1\n",
    "        else:\n",
    "            for b in range(0,BatchSamplingParamsObject.NumSamplesPerIter):\n",
    "                updateRadiusMat[max(NewIdxs[b][0]-UpdateRadius,0):min(NewIdxs[b][0]+UpdateRadius,SizeImage[0])][:,max(NewIdxs[b][1]-UpdateRadius,0):min(NewIdxs[b][1]+UpdateRadius,SizeImage[1])]=1\n",
    "    \n",
    "        updateIdxs = np.where(updateRadiusMat[Mask==0]==1)\n",
    "        \n",
    "        SmallUnMeasuredIdxs = np.transpose(np.where(np.logical_and(Mask==0,updateRadiusMat==1)))\n",
    "        if SmallUnMeasuredIdxs.size==0:\n",
    "            UpdateRadius=int(UpdateRadius*UpdateERDParamsObject.IncreaseRadiusBy)\n",
    "        else:\n",
    "            Done=1\n",
    "\n",
    "    \n",
    "    # Find neighbors of unmeasured locations\n",
    "    SmallNeighborValues,SmallNeighborWeights,SmallNeighborDistances = FindNeighbors(TrainingInfoObject,MeasuredIdxs,SmallUnMeasuredIdxs,MeasuredValues,Resolution)\n",
    "    \n",
    "    # Perform reconstruction\n",
    "    SmallReconValues=computeWeightedMRecons(SmallNeighborValues,SmallNeighborWeights,TrainingInfoObject)\n",
    "    \n",
    "    ReconImage[(np.logical_and(Mask==0,updateRadiusMat==1))]=SmallReconValues\n",
    "    ReconImage[MeasuredIdxs[:,0],MeasuredIdxs[:,1]]=MeasuredValues\n",
    "\n",
    "    # Compute features\n",
    "    SmallPolyFeatures=computeFeatures(SmallUnMeasuredIdxs,SizeImage,SmallNeighborValues,SmallNeighborWeights,SmallNeighborDistances,TrainingInfoObject,SmallReconValues,ReconImage,Resolution,ImageType)\n",
    "\n",
    "    # Compute ERD\n",
    "    SmallERDValues = SmallPolyFeatures.dot(Theta)\n",
    "\n",
    "    ReconValues[updateIdxs] = SmallReconValues\n",
    "    ERDValues[updateIdxs] = SmallERDValues\n",
    "\n",
    "    return(ERDValues,ReconValues)\n",
    "\n",
    "\n",
    "def FindNeighbors(TrainingInfoObject,MeasuredIdxs,UnMeasuredIdxs,MeasuredValues,Resolution):\n",
    "\n",
    "    # Find neighbors of unmeasured locations\n",
    "    Neigh = NearestNeighbors(n_neighbors=TrainingInfoObject.NumNbrs)\n",
    "    Neigh.fit(MeasuredIdxs)\n",
    "    NeighborDistances, NeighborIndices = Neigh.kneighbors(UnMeasuredIdxs)\n",
    "    NeighborDistances=NeighborDistances*Resolution\n",
    "    NeighborValues=MeasuredValues[NeighborIndices]\n",
    "    NeighborWeights=computeNeighborWeights(NeighborDistances,TrainingInfoObject)\n",
    "    \n",
    "    return(NeighborValues,NeighborWeights,NeighborDistances) \n",
    "\n",
    "def ComputeRecons(TrainingInfoObject,NeighborValues,NeighborWeights,SizeImage,UnMeasuredIdxs,MeasuredIdxs,MeasuredValues):\n",
    "    \n",
    "    # Perform reconstruction\n",
    "    ReconValues=computeWeightedMRecons(NeighborValues,NeighborWeights,TrainingInfoObject)\n",
    "    ReconImage = np.zeros((SizeImage[0],SizeImage[1]))\n",
    "    ReconImage[UnMeasuredIdxs[:,0],UnMeasuredIdxs[:,1]]=ReconValues\n",
    "    ReconImage[MeasuredIdxs[:,0],MeasuredIdxs[:,1]]=MeasuredValues\n",
    "    return(ReconValues,ReconImage)\n",
    "\n",
    "\n",
    "def computeNeighborWeights(NeighborDistances,TrainingInfoObject):\n",
    "    \n",
    "    UnNormNeighborWeights=1/np.power(NeighborDistances,TrainingInfoObject.p)\n",
    "    SumOverRow = (np.sum(UnNormNeighborWeights,axis=1))\n",
    "    NeighborWeights=UnNormNeighborWeights/SumOverRow[:, np.newaxis]\n",
    "    return NeighborWeights\n",
    "\n",
    "def computeWeightedMRecons(NeighborValues,NeighborWeights,TrainingInfoObject):\n",
    "    \n",
    "    # Weighted Mode Computation\n",
    "    if TrainingInfoObject.FeatReconMethod=='DWM':\n",
    "        ClassLabels = np.unique(NeighborValues)\n",
    "        ClassWeightSums = np.zeros((np.shape(NeighborWeights)[0],np.shape(ClassLabels)[0]))\n",
    "        for i in range(0,np.shape(ClassLabels)[0]):\n",
    "            TempFeats=np.zeros((np.shape(NeighborWeights)[0],np.shape(NeighborWeights)[1]))\n",
    "            np.copyto(TempFeats,NeighborWeights)\n",
    "            TempFeats[NeighborValues!=ClassLabels[i]]=0\n",
    "            ClassWeightSums[:,i]=np.sum(TempFeats,axis=1)\n",
    "        IdxOfMaxClass = np.argmax(ClassWeightSums,axis=1)\n",
    "        ReconValues = ClassLabels[IdxOfMaxClass]\n",
    "\n",
    "    # Weighted Mean Computation\n",
    "    elif TrainingInfoObject.FeatReconMethod=='CWM':\n",
    "        ReconValues=np.sum(NeighborValues*NeighborWeights,axis=1)\n",
    "\n",
    "    return ReconValues\n",
    "\n",
    "\n",
    "def computeDifference(array1,array2,type):\n",
    "    if type == 'D':\n",
    "        difference=array1!=array2\n",
    "        difference = difference.astype(float)\n",
    "    if type == 'C':\n",
    "        difference=abs(array1-array2)\n",
    "\n",
    "\n",
    "    return difference\n",
    "\n",
    "def computeFeatures(UnMeasuredIdxs,SizeImage,NeighborValues,NeighborWeights,NeighborDistances,TrainingInfoObject,ReconValues,ReconImage,Resolution,ImageType):\n",
    "    Feature=np.zeros((np.shape(UnMeasuredIdxs)[0],6))\n",
    "\n",
    "    # Compute st div features\n",
    "    Feature[:,0],Feature[:,1]=computeStDivFeatures(NeighborValues,NeighborWeights,TrainingInfoObject,ReconValues,ImageType)\n",
    "    \n",
    "    # Compute distance/density features\n",
    "    Feature[:,2],Feature[:,3]=computeDensityDistanceFeatures(NeighborDistances,NeighborWeights,SizeImage,TrainingInfoObject,ReconValues,ImageType)\n",
    "\n",
    "    # Compute gradient features\n",
    "    GradientImageX,GradientImageY=computeGradientFeatures(ReconImage,ImageType)\n",
    "    Feature[:,4] = GradientImageY[UnMeasuredIdxs[:,0],UnMeasuredIdxs[:,1]]\n",
    "    Feature[:,5] = GradientImageX[UnMeasuredIdxs[:,0],UnMeasuredIdxs[:,1]]\n",
    "\n",
    "    PolyFeatures = computePolyFeatures(Feature)\n",
    "    return PolyFeatures\n",
    "\n",
    "def computeGradientFeatures(ReconImage,ImageType):\n",
    "    GradientImageX,GradientImageY = np.gradient(ReconImage)\n",
    "    if ImageType=='D':\n",
    "        GradientImageX[GradientImageX!=0]=1\n",
    "        GradientImageY[GradientImageY!=0]=1\n",
    "    elif ImageType=='C':\n",
    "        GradientImageX=abs(GradientImageX)\n",
    "        GradientImageY=abs(GradientImageY)\n",
    "    return(GradientImageX,GradientImageY)\n",
    "\n",
    "\n",
    "def computeStDivFeatures(NeighborValues,NeighborWeights,TrainingInfoObjecto,ReconValues,ImageType):\n",
    "    DiffVect = computeDifference(NeighborValues,np.transpose(np.matlib.repmat(ReconValues,np.shape(NeighborValues)[1],1)),ImageType)\n",
    "    Feature_0 = np.sum(NeighborWeights*DiffVect,axis=1)\n",
    "    Feature_1 = np.sqrt((1/TrainingInfoObject.NumNbrs)*np.sum(np.power(DiffVect,2),axis=1))\n",
    "    return(Feature_0,Feature_1)\n",
    "\n",
    "def computeDensityDistanceFeatures(NeighborDistances,NeighborWeights,SizeImage,TrainingInfoObject,ReconValues,ImageType):\n",
    "    CutoffDist = np.ceil(np.sqrt((TrainingInfoObject.FeatDistCutoff/100)*(SizeImage[0]*SizeImage[1]/np.pi)))\n",
    "    Feature_2 = NeighborDistances[:,0]\n",
    "    NeighborsInCircle=np.sum(NeighborDistances<=CutoffDist,axis=1)\n",
    "    Feature_3 = (1+(np.pi*(np.power(CutoffDist,2))))/(1+NeighborsInCircle)\n",
    "    return(Feature_2,Feature_3)\n",
    "\n",
    "def computePolyFeatures(Feature):\n",
    "    PolyFeatures = np.hstack([np.ones((np.shape(Feature)[0],1)),Feature])\n",
    "    for i in range(0,np.shape(Feature)[1]):\n",
    "        for j in range(i,np.shape(Feature)[1]):\n",
    "            Temp = Feature[:,i]*Feature[:,j]\n",
    "            PolyFeatures = np.column_stack([PolyFeatures,Feature[:,i]*Feature[:,j]])\n",
    "\n",
    "    return PolyFeatures\n",
    "\n",
    "def performSLADStoFindC(codePath,trainingDataPath,ImageSet,ImageType,ImageExtension,TrainingInfoObject,SizeImage,StopPercentage,Resolution,c_vec,UpdateERDParamsObject,InitialMaskObject,MaskType,reconPercVector,Classify,directImagePath,consoleRows,cPlot,savePlotLocation):\n",
    "    sys.path.append('code')\n",
    "    SimulationRun = 0\n",
    "    \n",
    "    # Initialize stopping condition variable\n",
    "    Beta = computeBeta(SizeImage)\n",
    "    StopCondParamsObject = StopCondParams()\n",
    "    StopCondParamsObject.initialize(Beta,0,50,2,StopPercentage)\n",
    "    \n",
    "    SavePathSLADS = trainingDataPath + 'SLADSResults' \n",
    "    PlotResult = 'N'\n",
    "\n",
    "    # Batch Sampling\n",
    "    PercOfSamplesPerIter = 0\n",
    "    NumSamplesPerIter = int(PercOfSamplesPerIter*SizeImage[0]*SizeImage[1]/100)\n",
    "    BatchSample = 'N'\n",
    "    BatchSamplingParamsObject = BatchSamplingParams()\n",
    "    if BatchSample=='N':\n",
    "        BatchSamplingParamsObject.initialize(BatchSample,1)\n",
    "    else:\n",
    "        BatchSamplingParamsObject.initialize(BatchSample,NumSamplesPerIter)\n",
    "\n",
    "    if not os.path.exists(SavePathSLADS):\n",
    "        os.makedirs(SavePathSLADS)\n",
    "        \n",
    "    AreaUnderCurve = np.zeros(c_vec.shape[0])\n",
    "    Idx_c = 0\n",
    "    \n",
    "    loadPathImage = trainingDataPath + 'Images' + os.path.sep\n",
    "    loadPathInitialMask = resultsDataPath + 'InitialSamplingMasks' # Load initial measurement mask\n",
    "    Mask = loadOrGenerateInitialMask(loadPathInitialMask,MaskType,InitialMaskObject,SizeImage)\n",
    "    imageNames = glob.glob(loadPathImage + '*' + ImageExtension)\n",
    "    NumImages = np.size(imageNames)\n",
    "    AvgTDArr = []\n",
    "    AvgMSEArr = []\n",
    "    AvgSSIMArr = []\n",
    "    for i in tqdm(range(0, len(c_vec)), desc = 'c Values', leave = True): #For each of the proposed c values\n",
    "        c = c_vec[i]\n",
    "        LoadPath_c = trainingDataPath + 'c_' + str(c)\n",
    "        TrainingInfoObject.FilterC=c\n",
    "        Theta = np.load(LoadPath_c + os.path.sep + 'Theta' + '_StopPerc_' + str(StopPercentageSLADS) + '.npy')\n",
    "        ImageTDArr = []\n",
    "        ImageMSEArr = []\n",
    "        ImageSSIMArr = []\n",
    "        for ImNum in tqdm(range(0, NumImages), desc = 'Images', leave = True): #For each of the images\n",
    "            img = misc.imread(imageNames[ImNum])\n",
    "            SavePathSLADS_c_ImNum = SavePathSLADS +  os.path.sep + 'Image_' + str(ImNum) + '_c_'+ str(c)\n",
    "            if not os.path.exists(SavePathSLADS_c_ImNum):\n",
    "                os.makedirs(SavePathSLADS_c_ImNum) \n",
    "\n",
    "            SavePath = SavePathSLADS + os.path.sep + 'Image_' + str(ImNum) + '_c_'+ str(c) + os.path.sep\n",
    "            isRunningParallel = True\n",
    "            runSLADSSimulationOnce(NumImages,Mask,codePath,ImageSet,SizeImage,StopCondParamsObject,Theta,TrainingInfoObject,TestingInfoObject,Resolution,ImageType,UpdateERDParamsObject,BatchSamplingParamsObject,SavePath,SimulationRun,ImNum,ImageExtension,PlotResult,Classify,directImagePath,falseFlag,isRunningParallel)\n",
    "            \n",
    "            MeasuredValuesFull=np.load(SavePath + 'MeasuredValues.npy')\n",
    "            MeasuredIdxsFull=np.load(SavePath + 'MeasuredIdxs.npy')\n",
    "            UnMeasuredIdxsFull=np.load(SavePath + 'UnMeasuredIdxs.npy')    \n",
    "            Difference = np.zeros(reconPercVector.shape[0])\n",
    "            idx=0\n",
    "            \n",
    "            for j in tqdm(range(0, len(reconPercVector)), desc = '% Reconstructed', leave = True): #For each of the reconstruction percentages\n",
    "                p = reconPercVector[j]\n",
    "                NumMeasurements = int(p*SizeImage[0]*SizeImage[1]/100)\n",
    "                MeasuredValues = MeasuredValuesFull[0:NumMeasurements]\n",
    "                MeasuredIdxs = MeasuredIdxsFull[0:NumMeasurements][:]\n",
    "                temp1 = MeasuredIdxsFull[NumMeasurements+1:MeasuredValuesFull.shape[0]][:]\n",
    "                temp2 = UnMeasuredIdxsFull\n",
    "                UnMeasuredIdxs = np.concatenate((temp1, temp2), axis=0)\n",
    "                Difference[idx], ReconImage = performReconOnce(SavePath,TrainingInfoObject,Resolution,SizeImage,ImageType,codePath,ImageSet,ImNum,ImageExtension,SimulationRun,MeasuredIdxs,UnMeasuredIdxs,MeasuredValues,directImagePath)\n",
    "                idx = idx+1\n",
    "            \n",
    "            TD = Difference/(SizeImage[0]*SizeImage[1])\n",
    "            MSE = (np.sum((ReconImage.astype(\"float\") - img.astype(\"float\")) ** 2))/(float(ReconImage.shape[0] * ReconImage.shape[1]))\n",
    "            SSIM = compare_ssim(ReconImage.astype(\"float\"), img.astype(\"float\"))\n",
    "\n",
    "            np.save(SavePath + 'TD', TD)\n",
    "            np.save(SavePath + 'MSE', MSE)\n",
    "            np.save(SavePath + 'SSIM', SSIM)\n",
    "            \n",
    "            ImageTDArr.append(TD)\n",
    "            ImageMSEArr.append(MSE)\n",
    "            ImageSSIMArr.append(SSIM)\n",
    "            \n",
    "            AreaUnderCurve[Idx_c]=AreaUnderCurve[Idx_c]+np.trapz(TD,x=reconPercVector)\n",
    "            \n",
    "        AvgTDArr.append(np.mean(ImageTDArr))\n",
    "        AvgMSEArr.append(np.mean(ImageMSEArr))\n",
    "        AvgSSIMArr.append(np.mean(ImageSSIMArr))\n",
    "        \n",
    "        Idx_c = Idx_c +1\n",
    "        \n",
    "    Best_c = c_vec[np.argmin(AreaUnderCurve)]\n",
    "    if cPlot:\n",
    "        #TD\n",
    "        x = c_vec\n",
    "        font = {'size' : 18}\n",
    "        plt.rc('font', **font)\n",
    "        f = plt.figure(figsize=(20,8))\n",
    "        ax1 = f.add_subplot()\n",
    "        plt.plot(c_vec, AvgTDArr, color='black', linestyle='--') \n",
    "        plt.scatter(c_vec, AvgTDArr, color='black') \n",
    "        plt.xlabel('c')\n",
    "        plt.ylabel('Total Distortion')\n",
    "        plt.savefig(savePlotLocation + 'cPlotTD.png')\n",
    "        \n",
    "        #MSE\n",
    "        font = {'size' : 18}\n",
    "        plt.rc('font', **font)\n",
    "        f = plt.figure(figsize=(20,8))\n",
    "        ax1 = f.add_subplot()\n",
    "        plt.plot(c_vec, AvgMSEArr, color='black', linestyle='--') \n",
    "        plt.scatter(c_vec, AvgMSEArr, color='black') \n",
    "        plt.xlabel('c')\n",
    "        plt.ylabel('MSE')\n",
    "        plt.savefig(savePlotLocation + 'cPlotMSE.png')\n",
    "        \n",
    "        #SSIM\n",
    "        font = {'size' : 18}\n",
    "        plt.rc('font', **font)\n",
    "        f = plt.figure(figsize=(20,8))\n",
    "        ax1 = f.add_subplot()\n",
    "        plt.plot(c_vec, AvgSSIMArr, color='black', linestyle='--') \n",
    "        plt.scatter(c_vec, AvgSSIMArr, color='black') \n",
    "        plt.xlabel('c')\n",
    "        plt.ylabel('SSIM')\n",
    "        plt.savefig(savePlotLocation + 'cPlotSSIM.png')\n",
    "    \n",
    "    sys.path.pop() #Hopefully remove the 'sys.path.append('code') flag at the top of this definitionr\n",
    "    return Best_c, NumImages\n",
    "\n",
    "\n",
    "def runSLADSSimulationOnce(NumImages,Mask,codePath,ImageSet,SizeImage,StopCondParamsObject,Theta,TrainingInfoObject,TestingInfoObject,Resolution,ImageType,UpdateERDParamsObject,BatchSamplingParamsObject,SavePath,SimulationRun,ImNum,ImageExtension,PlotResult,Classify,directImagePath,errorPlot,isRunningParallel):\n",
    "    sys.path.append('code')\n",
    "    MeasuredIdxs = np.transpose(np.where(Mask==1))\n",
    "    UnMeasuredIdxs = np.transpose(np.where(Mask==0))\n",
    "\n",
    "    ContinuousMeasuredValues = perfromInitialMeasurements(codePath,ImageSet,ImNum,ImageExtension,Mask,SimulationRun,directImagePath)\n",
    "    if Classify=='2C':\n",
    "        Threshold = filters.threshold_otsu(ContinuousMeasuredValues)\n",
    "        print('Threhold found using the Otsu method for 2 Class classification = ' + str(Threshold))\n",
    "        MeasuredValues = ContinuousMeasuredValues < Threshold\n",
    "        MeasuredValues = MeasuredValues+0\n",
    "#    elif Classify=='MC':\n",
    "        #### Classification function to output NewValues ##################\n",
    "        # NewValues is the vector of measured values post classification\n",
    "    elif Classify=='N':\n",
    "        MeasuredValues=ContinuousMeasuredValues\n",
    "\n",
    "    # Perform SLADS\n",
    "    IterNum=0\n",
    "    NumSamples = np.shape(MeasuredValues)[0]\n",
    "    StopCondFuncVal=np.zeros(( int((SizeImage[0]*SizeImage[1])*(StopCondParamsObject.MaxPercentage)/100)+10,2 ))\n",
    "    pixelCount = SizeImage[0]*SizeImage[1]\n",
    "\n",
    "    with tqdm(total = 100, desc = '% Sampled', leave = True, disable = isRunningParallel) as pbar:\n",
    "        while (checkStopCondFuncThreshold(StopCondParamsObject,StopCondFuncVal,NumSamples,IterNum,SizeImage) != 1):\n",
    "            if IterNum==0:\n",
    "                Mask,MeasuredValues,ERDValues,ReconValues,ReconImage,NewIdxs,MaxIdxsVect=updateERDandFindNewLocationFirst(Mask,MeasuredValues,MeasuredIdxs,UnMeasuredIdxs,Theta,SizeImage,TrainingInfoObject,Resolution,ImageType,NumSamples,UpdateERDParamsObject,BatchSamplingParamsObject)           \n",
    "            else:\n",
    "                Mask,MeasuredValues,ERDValues,ReconValues,ReconImage,NewIdxs,MaxIdxsVect=updateERDandFindNewLocationAfter(Mask,MeasuredValues,MeasuredIdxs,UnMeasuredIdxs,Theta,SizeImage,TrainingInfoObject,Resolution,ImageType,UpdateERDParamsObject,BatchSamplingParamsObject,StopCondFuncVal,IterNum,NumSamples,NewIdxs,ReconValues,ReconImage,ERDValues,MaxIdxsVect)\n",
    "\n",
    "            NewContinuousValues = performMeasurements(NewIdxs,codePath,ImageSet,ImNum,ImageExtension,MeasuredIdxs,BatchSamplingParamsObject,SimulationRun,directImagePath)\n",
    "            ContinuousMeasuredValues = np.hstack((ContinuousMeasuredValues,NewContinuousValues))\n",
    "            if Classify=='2C':           \n",
    "                NewValues = NewContinuousValues > Threshold\n",
    "                NewValues = NewValues+0\n",
    "    #        elif Classify=='MC':\n",
    "                #### Classification function to output NewValues ##################\n",
    "                # NewValues is the vector of measured values post classification            \n",
    "            elif Classify=='N':\n",
    "                NewValues=NewContinuousValues    \n",
    "\n",
    "            Mask,MeasuredValues,MeasuredIdxs,UnMeasuredIdxs = updateMeasurementArrays(NewIdxs,MaxIdxsVect,Mask,MeasuredValues,MeasuredIdxs,UnMeasuredIdxs,NewValues,BatchSamplingParamsObject)\n",
    "\n",
    "            NumSamples = np.shape(MeasuredValues)[0]\n",
    "\n",
    "            StopCondFuncVal=computeStopCondFuncVal(ReconValues,MeasuredValues,StopCondParamsObject,ImageType,StopCondFuncVal,MaxIdxsVect,NumSamples,IterNum,BatchSamplingParamsObject)\n",
    "\n",
    "            #if PlotResult=='Y' and np.remainder(NumSamples,round(0.01*SizeImage[0]*SizeImage[1])) ==0:\n",
    "                #print(str(np.round(NumSamples*100/(SizeImage[0]*SizeImage[1]))) + ' Percent Sampled')\n",
    "            IterNum += 1\n",
    "            pbar.n = round((NumSamples/pixelCount)*100)\n",
    "            pbar.refresh()\n",
    "        np.save(SavePath + 'MeasuredValues', MeasuredValues)\n",
    "        np.save(SavePath + 'MeasuredIdxs', MeasuredIdxs)\n",
    "        np.save(SavePath + 'UnMeasuredIdxs', UnMeasuredIdxs)\n",
    "        np.save(SavePath + 'StopCondFuncVal',StopCondFuncVal)\n",
    "        np.save(SavePath + 'ContinuousMeasuredValues',ContinuousMeasuredValues)\n",
    "        savemat(SavePath + 'MeasuredIdxs.mat',dict(MeasuredIdxs=MeasuredIdxs))\n",
    "        savemat(SavePath + 'MeasuredValues.mat',dict(MeasuredValues=MeasuredValues))\n",
    "        savemat(SavePath + 'UnMeasuredIdxs.mat',dict(UnMeasuredIdxs=UnMeasuredIdxs))\n",
    "        savemat(SavePath + 'StopCondFuncVal.mat',dict(StopCondFuncVal=StopCondFuncVal))\n",
    "        savemat(SavePath + 'ContinuousMeasuredValues.mat',dict(ContinuousMeasuredValues=ContinuousMeasuredValues))\n",
    "\n",
    "        if errorPlot or (PlotResult=='Y'):\n",
    "            percentSampled = np.round(NumSamples*100/(SizeImage[0]*SizeImage[1]))\n",
    "            Difference,ReconImage = performReconOnce(SavePath,TrainingInfoObject,Resolution,SizeImage,ImageType,codePath,ImageSet,ImNum,ImageExtension,SimulationRun,MeasuredIdxs,UnMeasuredIdxs,MeasuredValues,directImagePath)\n",
    "            TD = Difference/(SizeImage[0]*SizeImage[1])\n",
    "            if (SimulationRun==1):\n",
    "                img = misc.imread(directImagePath)\n",
    "            else: \n",
    "                img = loadTestImage(codePath,ImageSet,ImNum,ImageExtension,SimulationRun)  \n",
    "            #print('')\n",
    "            #print('')\n",
    "            #print('######################################')\n",
    "            #print('Total Distortion = ' + str(TD))\n",
    "\n",
    "            #Moved out plotting code into only function it is used\n",
    "            MSE = (np.sum((ReconImage.astype(\"float\") - img.astype(\"float\")) ** 2))/(float(ReconImage.shape[0] * ReconImage.shape[1]))\n",
    "            SSIM = compare_ssim(ReconImage.astype(\"float\"), img.astype(\"float\"))\n",
    "\n",
    "            if errorPlot:\n",
    "                resultObject = simulationResults()\n",
    "                resultObject.saveErrorData(MSE, SSIM, TD) #Store resulting error information\n",
    " \n",
    "            if PlotResult=='Y': \n",
    "                #Set plot formatting\n",
    "                font = {'size' : 18}\n",
    "                plt.rc('font', **font)\n",
    "                f = plt.figure(figsize=(20,8))\n",
    "                plt.suptitle(\"MSE: %.2f, SSIM: %.2f, TD: %.2f, Percent Sampled: %.2f\" % (MSE, SSIM, TD, percentSampled), fontsize=20, fontweight='bold', y = 0.9)\n",
    "                ax1 = f.add_subplot(131)\n",
    "                ax1.imshow(Mask, cmap='gist_heat')\n",
    "                ax1.set_title('Sampled Mask')\n",
    "                ax2 = f.add_subplot(132)       \n",
    "                ax2.imshow(ReconImage, cmap='gist_heat')\n",
    "                ax2.set_title('Reconstructed Image')\n",
    "                ax3 = f.add_subplot(133)\n",
    "                ax3.imshow(img, cmap='gist_heat')\n",
    "                ax3.set_title('Ground-truth Image')\n",
    "                plt.savefig(SavePath + '.png')\n",
    "\n",
    "            #pylab.show()\n",
    "        sys.path.pop() #Hopefully remove the 'sys.path.append('code') flag at the top of this definition\n",
    "        if errorPlot:\n",
    "            return resultObject\n",
    "\n",
    "def runSLADSOnce(Mask,codePath,SizeImage,StopCondParamsObject,Theta,TrainingInfoObject,Resolution,ImageType,UpdateERDParamsObject,BatchSamplingParamsObject,SavePath,SimulationRun,ImNum,PlotResult,Classify):\n",
    "    sys.path.append('code')\n",
    "    MeasuredIdxs = np.transpose(np.where(Mask==1))\n",
    "    UnMeasuredIdxs = np.transpose(np.where(Mask==0))\n",
    "    \n",
    "    ##################################################################\n",
    "    # CODE HERE\n",
    "    # Plug in Your Measurement Routine\n",
    "    # Please use 'MeasuredValues' as output variable\n",
    "    # ContinuousMeasuredValues = performMeasurements(Mask)\n",
    "    ##################################################################\n",
    "    \n",
    "    if Classify=='2C':\n",
    "        Threshold = filters.threshold_otsu(ContinuousMeasuredValues)\n",
    "        print('Threhold found using the Otsu method for 2 Class classification = ' + str(Threshold))\n",
    "        MeasuredValues = ContinuousMeasuredValues < Threshold\n",
    "        MeasuredValues = MeasuredValues+0\n",
    "#    elif Classify=='MC':\n",
    "        #### Classification function to output NewValues ##################\n",
    "        # NewValues is the vector of measured values post classification\n",
    "    elif Classify=='N':\n",
    "        MeasuredValues=ContinuousMeasuredValues\n",
    "    \n",
    "    # Perform SLADS\n",
    "    IterNum=0\n",
    "    Stop=0\n",
    "    NumSamples = np.shape(MeasuredValues)[0]\n",
    "    StopCondFuncVal=np.zeros(( int((SizeImage[0]*SizeImage[1])*(StopCondParamsObject.MaxPercentage)/100)+10,2 ))\n",
    "    while Stop !=1:\n",
    "        \n",
    "        if IterNum==0:\n",
    "            Mask,MeasuredValues,ERDValues,ReconValues,ReconImage,NewIdxs,MaxIdxsVect=updateERDandFindNewLocationFirst(Mask,MeasuredValues,MeasuredIdxs,UnMeasuredIdxs,Theta,SizeImage,TrainingInfoObject,Resolution,ImageType,NumSamples,UpdateERDParamsObject,BatchSamplingParamsObject)           \n",
    "        else:\n",
    "            Mask,MeasuredValues,ERDValues,ReconValues,ReconImage,NewIdxs,MaxIdxsVect=updateERDandFindNewLocationAfter(Mask,MeasuredValues,MeasuredIdxs,UnMeasuredIdxs,Theta,SizeImage,TrainingInfoObject,Resolution,ImageType,UpdateERDParamsObject,BatchSamplingParamsObject,StopCondFuncVal,IterNum,NumSamples,NewIdxs,ReconValues,ReconImage,ERDValues,MaxIdxsVect)\n",
    "        \n",
    "        ##################################################################\n",
    "        # CODE HERE\n",
    "        # Plug in Your Measurement Routine\n",
    "        # Please use 'NewContValues' as output variable\n",
    "        # NewContinuousValues = performMeasurements(NewIdxs)\n",
    "        ##################################################################    \n",
    "        \n",
    "        ContinuousMeasuredValues = np.hstack((ContinuousMeasuredValues,NewContinuousValues))\n",
    "        if Classify=='2C':           \n",
    "            NewValues = NewContinuousValues > Threshold\n",
    "            NewValues = NewValues+0\n",
    "#        elif Classify=='MC':\n",
    "            #### Classification function to output NewValues ##################\n",
    "            # NewValues is the vector of measured values post classification    \n",
    "        elif Classify=='N':\n",
    "            NewValues=NewContinuousValues    \n",
    "\n",
    "\n",
    "        Mask,MeasuredValues,MeasuredIdxs,UnMeasuredIdxs = updateMeasurementArrays(NewIdxs,MaxIdxsVect,Mask,MeasuredValues,MeasuredIdxs,UnMeasuredIdxs,NewValues,BatchSamplingParamsObject)\n",
    "    \n",
    "        NumSamples = np.shape(MeasuredValues)[0]\n",
    "    \n",
    "        StopCondFuncVal=computeStopCondFuncVal(ReconValues,MeasuredValues,StopCondParamsObject,ImageType,StopCondFuncVal,MaxIdxsVect,NumSamples,IterNum,BatchSamplingParamsObject)\n",
    "            \n",
    "        Stop = checkStopCondFuncThreshold(StopCondParamsObject,StopCondFuncVal,NumSamples,IterNum,SizeImage)\n",
    "        if PlotResult=='Y' and np.remainder(NumSamples,round(0.01*SizeImage[0]*SizeImage[1])) ==0:\n",
    "            print(str(np.round(NumSamples*100/(SizeImage[0]*SizeImage[1]))) + ' Percent Sampled')\n",
    "        IterNum += 1\n",
    "        \n",
    "    \n",
    "    np.save(SavePath + 'MeasuredValues', MeasuredValues)\n",
    "    np.save(SavePath + 'MeasuredIdxs', MeasuredIdxs)\n",
    "    np.save(SavePath + 'UnMeasuredIdxs', UnMeasuredIdxs)\n",
    "    np.save(SavePath + 'StopCondFuncVal',StopCondFuncVal)\n",
    "    np.save(SavePath + 'ContinuousMeasuredValues',ContinuousMeasuredValues)\n",
    "    savemat(SavePath + 'MeasuredIdxs.mat',dict(MeasuredIdxs=MeasuredIdxs))\n",
    "    savemat(SavePath + 'MeasuredValues.mat',dict(MeasuredValues=MeasuredValues))\n",
    "    savemat(SavePath + 'UnMeasuredIdxs.mat',dict(UnMeasuredIdxs=UnMeasuredIdxs))\n",
    "    savemat(SavePath + 'StopCondFuncVal.mat',dict(StopCondFuncVal=StopCondFuncVal))\n",
    "    savemat(SavePath + 'ContinuousMeasuredValues.mat',dict(ContinuousMeasuredValues=ContinuousMeasuredValues))\n",
    "    \n",
    "    if PlotResult=='Y': \n",
    "        print(str(np.round(NumSamples*100/(SizeImage[0]*SizeImage[1]))) + ' Percent Sampled before stopping')\n",
    "        plotFigure = plotAfterSLADS(Mask,ReconImage)\n",
    "        #pylab.show()\n",
    "    sys.path.pop() #Hopefully remove the 'sys.path.append('code') flag at the top of this definition\n",
    "\n",
    "\n",
    "def perfromInitialMeasurements(codePath,ImageSet,ImNum,ImageExtension,Mask,SimulationRun,directImagePath):\n",
    "\n",
    "    if (SimulationRun==1):\n",
    "        Img = misc.imread(directImagePath)\n",
    "    else: \n",
    "        Img = loadTestImage(codePath,ImageSet,ImNum,ImageExtension,SimulationRun)  \n",
    "    if Mask.shape[0]!=Img.shape[0] or Mask.shape[1]!=Img.shape[1]:\n",
    "        sys.exit('Error!!! The dimensions you entered in \"SizeImage\" do not match the dimensions of the testing image in ./ResultsAndData/TestingImages/TestingImageSet_' + ImageSet)\n",
    "    MeasuredValues = Img[Mask==1]\n",
    "    return(MeasuredValues)\n",
    "\n",
    "def performMeasurements(NewIdxs,codePath,ImageSet,ImNum,ImageExtension,MeasuredIdxs,BatchSamplingParamsObject,SimulationRun,directImagePath):\n",
    "    if (SimulationRun==1):\n",
    "        Img=misc.imread(directImagePath)\n",
    "    else: \n",
    "        Img = loadTestImage(codePath,ImageSet,ImNum,ImageExtension,SimulationRun)\n",
    "    if BatchSamplingParamsObject.Do == 'N':\n",
    "        NewValues = Img[NewIdxs[0],NewIdxs[1]]\n",
    "    else:\n",
    "        NewValues = Img[NewIdxs[:,0],NewIdxs[:,1]]\n",
    "    return NewValues\n",
    "\n",
    "def updateMeasurementArrays(NewIdxs,MaxIdxsVect,Mask,MeasuredValues,MeasuredIdxs,UnMeasuredIdxs,NewValues,BatchSamplingParamsObject):\n",
    "    \n",
    "    if BatchSamplingParamsObject.Do == 'N':\n",
    "        Mask[NewIdxs[0],NewIdxs[1]]=1\n",
    "        MeasuredValues = np.hstack((MeasuredValues,NewValues))\n",
    "        MeasuredIdxs = np.vstack((MeasuredIdxs,[NewIdxs[0],NewIdxs[1]]))\n",
    "        UnMeasuredIdxs = np.delete(UnMeasuredIdxs,(MaxIdxsVect), axis=0)\n",
    "    else:\n",
    "        for i in range(0,BatchSamplingParamsObject.NumSamplesPerIter):\n",
    "            Mask[NewIdxs[i,0],NewIdxs[i,1]]=1\n",
    "        MeasuredValues = np.hstack((MeasuredValues,NewValues))\n",
    "        MeasuredIdxs = np.vstack((MeasuredIdxs,NewIdxs))\n",
    "        UnMeasuredIdxs = np.delete(UnMeasuredIdxs,(MaxIdxsVect), axis=0)\n",
    "    return(Mask,MeasuredValues,MeasuredIdxs,UnMeasuredIdxs)\n",
    "\n",
    "def findNewMeasurementIdxs(Mask,MeasuredIdxs,UnMeasuredIdxs,MeasuredValues,Theta,SizeImage,TrainingInfoObject,Resolution,ImageType,NumSamples,UpdateERDParamsObject,ReconValues,ReconImage,ERDValues,ActualBatchSamplingParams):\n",
    "    \n",
    "    if ActualBatchSamplingParams.Do == 'N':\n",
    "        MaxIdxsVect = np.argmax(ERDValues)\n",
    "        NewIdxs = UnMeasuredIdxs[MaxIdxsVect,:]\n",
    "    else:\n",
    "        NewIdxs = np.zeros((ActualBatchSamplingParams.NumSamplesPerIter,2),dtype=np.int)\n",
    "        MaxIdxsVect = np.zeros((ActualBatchSamplingParams.NumSamplesPerIter,1),dtype=np.int)\n",
    "        MaxIdxsVect[0] = np.argmax(ERDValues)\n",
    "        NewIdxs[0,:] = UnMeasuredIdxs[MaxIdxsVect[0],:]\n",
    "        \n",
    "        TempNewIdxs = np.zeros((ActualBatchSamplingParams.NumSamplesPerIter,2),dtype=np.int)\n",
    "        TempMaxIdxsVect = np.zeros((ActualBatchSamplingParams.NumSamplesPerIter,1),dtype=np.int)\n",
    "        TempMaxIdxsVect[0] = np.argmax(ERDValues)\n",
    "        TempNewIdxs[0,:] = UnMeasuredIdxs[MaxIdxsVect[0],:]\n",
    "        \n",
    "        TempBatchSamplingParams = BatchSamplingParams()\n",
    "        TempBatchSamplingParams.initialize('N',1)\n",
    "        OrigUnMeasuredIdxs=np.zeros((np.shape(UnMeasuredIdxs)))\n",
    "        np.copyto(OrigUnMeasuredIdxs,UnMeasuredIdxs)                \n",
    "        for i in range(1,ActualBatchSamplingParams.NumSamplesPerIter):\n",
    "            NewValues = ReconValues[TempMaxIdxsVect[i-1]]\n",
    "            Mask,MeasuredValues,MeasuredIdxs,UnMeasuredIdxs = updateMeasurementArrays(TempNewIdxs[i-1,:],TempMaxIdxsVect[i-1],Mask,MeasuredValues,MeasuredIdxs,UnMeasuredIdxs,NewValues,TempBatchSamplingParams)\n",
    "            \n",
    "            ERDValues,ReconValues=updateERD(Mask,MeasuredIdxs,UnMeasuredIdxs,MeasuredValues,Theta,SizeImage,TrainingInfoObject,Resolution,ImageType,TempNewIdxs[i-1,:],NumSamples,UpdateERDParamsObject,ReconValues,ReconImage,ERDValues,TempMaxIdxsVect[i-1],TempBatchSamplingParams)\n",
    "            TempMaxIdxsVect[i] = np.argmax(ERDValues)\n",
    "            TempNewIdxs[i,:] = UnMeasuredIdxs[TempMaxIdxsVect[i],:]\n",
    "            NewIdxs[i,:] = TempNewIdxs[i,:]\n",
    "            MaxIdxsVect[i]=np.where(np.all(OrigUnMeasuredIdxs==TempNewIdxs[i,:],axis=1))\n",
    "    return(NewIdxs,MaxIdxsVect)\n",
    "\n",
    "def loadTestImage(codePath,ImageSet,ImNum,ImageExtension,SimulationRun):\n",
    "    \n",
    "    if SimulationRun==1:\n",
    "        sys.exit('ERROR!!! Direct loading of test image has been bypassed')\n",
    "    else:\n",
    "        #SEE IF THIS LINE IS EVER USED AS PLOTTING DURING TRAINING IS TURNED OFF\n",
    "        loadPathImage = trainingDataPath + 'Images' + os.path.sep\n",
    "        #ImagesToFindC Line\n",
    "        #loadPathImage = resultsDataPath + 'InputData' + os.path.sep + 'TrainingDB_' + ImageSet + os.path.sep + 'ImagesToFindC' + os.path.sep     \n",
    "    cnt = 0\n",
    "\n",
    "    for image_path in glob.glob(loadPathImage + '*' + ImageExtension):\n",
    "        if cnt == ImNum:\n",
    "            if ImageExtension=='.mat':\n",
    "                ImgDat=loadmat(image_path)\n",
    "                Img=ImgDat['img']\n",
    "            else:\n",
    "                Img = misc.imread(image_path)\n",
    "        cnt = cnt+1\n",
    "    try:\n",
    "        Img\n",
    "    except NameError:\n",
    "        sys.exit('Error!!! There are no images in ' + loadPathImage + ' that have the extention ' + ImageExtension)\n",
    "    return Img\n",
    "\n",
    "def loadOrGenerateInitialMask(loadPathInitialMask,MaskType,InitialMaskObject,SizeImage):\n",
    "    if MaskType=='H':\n",
    "        StartingMeasurementMask=InitialMaskObject.MaskType + '_' + str(InitialMaskObject.MaskNumber) + '_' + str(InitialMaskObject.RowSz) + '_' + str(InitialMaskObject.ColSz) + '_Percentage_' + str(InitialMaskObject.Percentage);\n",
    "        loadPathInitialMask = loadPathInitialMask + os.path.sep + StartingMeasurementMask                                                               \n",
    "        if not os.path.exists(loadPathInitialMask):                                                                                                                          \n",
    "            sys.exit('Error!!! Check foder .ResultsAndData/InitialSamplingMasks/ for folder ' + loadPathInitialMask)                                                            \n",
    "        Mask = np.load(loadPathInitialMask + os.path.sep + 'SampleMatrix.npy')\n",
    "    else:\n",
    "        Mask = generateInitialMask(InitialMaskObject,SizeImage)\n",
    "    return Mask\n",
    "\n",
    "def computeStopCondFuncVal(ReconValues,MeasuredValues,StopCondParamsObject,ImageType,StopCondFuncVal,MaxIdxsVect,NumSamples,IterNum,BatchSamplingParamsObject):\n",
    "    \n",
    "    if BatchSamplingParamsObject.Do=='N':\n",
    "        Diff=computeDifference(ReconValues[MaxIdxsVect],MeasuredValues[NumSamples-1],ImageType)\n",
    "        if IterNum == 0:\n",
    "            StopCondFuncVal[IterNum,0] = StopCondParamsObject.Beta*Diff\n",
    "        else:\n",
    "            StopCondFuncVal[IterNum,0] = ((1-StopCondParamsObject.Beta)*StopCondFuncVal[IterNum-1,0] + StopCondParamsObject.Beta*Diff)\n",
    "        StopCondFuncVal[IterNum,1] = NumSamples\n",
    "    \n",
    "    else:\n",
    "        Diff=0\n",
    "        for i in range(0,BatchSamplingParamsObject.NumSamplesPerIter):\n",
    "            Diff=computeDifference(ReconValues[MaxIdxsVect[i]],MeasuredValues[NumSamples-1-(BatchSamplingParamsObject.NumSamplesPerIter-i-1)],ImageType)+Diff\n",
    "        Diff = Diff/BatchSamplingParamsObject.NumSamplesPerIter\n",
    "        if IterNum == 0:\n",
    "            StopCondFuncVal[IterNum,0] = StopCondParamsObject.Beta*Diff\n",
    "        else:\n",
    "            StopCondFuncVal[IterNum,0] = ((1-StopCondParamsObject.Beta)*StopCondFuncVal[IterNum-1,0] + StopCondParamsObject.Beta*Diff)\n",
    "        StopCondFuncVal[IterNum,1] = NumSamples\n",
    "    return StopCondFuncVal\n",
    "\n",
    "def checkStopCondFuncThreshold(StopCondParamsObject,StopCondFuncVal,NumSamples,IterNum,SizeImage):\n",
    "    \n",
    "    if StopCondParamsObject.Threshold==0:\n",
    "        if NumSamples>SizeImage[0]*SizeImage[1]*StopCondParamsObject.MaxPercentage/100:\n",
    "            Stop=1\n",
    "        else:\n",
    "            Stop=0\n",
    "\n",
    "    else:\n",
    "        if NumSamples>SizeImage[0]*SizeImage[1]*StopCondParamsObject.MaxPercentage/100:\n",
    "            Stop=1\n",
    "        else:\n",
    "            if np.logical_and(((SizeImage[0]*SizeImage[1])*StopCondParamsObject.MinPercentage/100)<NumSamples,StopCondFuncVal[IterNum,0]<StopCondParamsObject.Threshold):\n",
    "                Stop=0\n",
    "                GradStopCondFunc =np.mean(StopCondFuncVal[IterNum,0]-StopCondFuncVal[IterNum-StopCondParamsObject.JforGradient:IterNum-1,0])\n",
    "                if GradStopCondFunc<0:\n",
    "                    Stop=1\n",
    "            else:\n",
    "                Stop=0\n",
    "    return Stop\n",
    "\n",
    "def computeBeta(SizeImage):\n",
    "    import math\n",
    "    if SizeImage[0]*SizeImage[1]<512**2+1:\n",
    "        Beta = 0.001*(((18-math.log(SizeImage[0]*SizeImage[1],2))/2)+1)\n",
    "    else:\n",
    "        Beta = 0.001/(((math.log(SizeImage[0]*SizeImage[1],2)-18)/2)+1)\n",
    "    return Beta     \n",
    "\n",
    "def updateERDandFindNewLocationFirst(Mask,MeasuredValues,MeasuredIdxs,UnMeasuredIdxs,Theta,SizeImage,TrainingInfoObject,Resolution,ImageType,NumSamples,UpdateERDParamsObject,BatchSamplingParamsObject):\n",
    "\n",
    "    ERDValues,ReconValues,ReconImage = computeFullERD(MeasuredValues,MeasuredIdxs,UnMeasuredIdxs,Theta,SizeImage,TrainingInfoObject,Resolution,ImageType)\n",
    "\n",
    "    NewIdxs,MaxIdxsVect = findNewMeasurementIdxs(Mask,MeasuredIdxs,UnMeasuredIdxs,MeasuredValues,Theta,SizeImage,TrainingInfoObject,Resolution,ImageType,NumSamples,UpdateERDParamsObject,ReconValues,ReconImage,ERDValues,BatchSamplingParamsObject)\n",
    "\n",
    "    return(Mask,MeasuredValues,ERDValues,ReconValues,ReconImage,NewIdxs,MaxIdxsVect)\n",
    "\n",
    "\n",
    "def updateERDandFindNewLocationAfter(Mask,MeasuredValues,MeasuredIdxs,UnMeasuredIdxs,Theta,SizeImage,TrainingInfoObject,Resolution,ImageType,UpdateERDParamsObject,BatchSamplingParamsObject,StopCondFuncVal,IterNum,NumSamples,NewIdxs,ReconValues,ReconImage,ERDValues,MaxIdxsVect):\n",
    "    \n",
    "    if UpdateERDParamsObject.Do == 'N':\n",
    "        ERDValues,ReconValues,ReconImage = computeFullERD(MeasuredValues,MeasuredIdxs,UnMeasuredIdxs,Theta,SizeImage,TrainingInfoObject,Resolution,ImageType)\n",
    "    else:\n",
    "        ERDValues,ReconValues=updateERD(Mask,MeasuredIdxs,UnMeasuredIdxs,MeasuredValues,Theta,SizeImage,TrainingInfoObject,Resolution,ImageType,NewIdxs,NumSamples,UpdateERDParamsObject,ReconValues,ReconImage,ERDValues,MaxIdxsVect,BatchSamplingParamsObject)\n",
    "    \n",
    "    NewIdxs,MaxIdxsVect = findNewMeasurementIdxs(Mask,MeasuredIdxs,UnMeasuredIdxs,MeasuredValues,Theta,SizeImage,TrainingInfoObject,Resolution,ImageType,NumSamples,UpdateERDParamsObject,ReconValues,ReconImage,ERDValues,BatchSamplingParamsObject)\n",
    "\n",
    "    return(Mask,MeasuredValues,ERDValues,ReconValues,ReconImage,NewIdxs,MaxIdxsVect)\n",
    "\n",
    "def performReconOnce(SavePath,TrainingInfoObject,Resolution,SizeImage,ImageType,codePath,ImageSet,ImNum,ImageExtension,SimulationRun,MeasuredIdxs,UnMeasuredIdxs,MeasuredValues,directImagePath):\n",
    "\n",
    "    NeighborValues,NeighborWeights,NeighborDistances = FindNeighbors(TrainingInfoObject,MeasuredIdxs,UnMeasuredIdxs,MeasuredValues,Resolution)\n",
    "    ReconValues,ReconImage = ComputeRecons(TrainingInfoObject,NeighborValues,NeighborWeights,SizeImage,UnMeasuredIdxs,MeasuredIdxs,MeasuredValues)    \n",
    "    \n",
    "    if (SimulationRun==1):\n",
    "        Img = misc.imread(directImagePath)\n",
    "    else: \n",
    "        Img = loadTestImage(codePath,ImageSet,ImNum,ImageExtension,SimulationRun)\n",
    "    \n",
    "    Difference = np.sum(computeDifference(Img,ReconImage,ImageType))\n",
    "    return(Difference,ReconImage)\n",
    "\n",
    "def findStoppingThreshold(trainingDataPath,NumTrainingImages,Best_c,PercentageInitialMask,DesiredTD,reconPercVector,SizeImage):\n",
    "    sys.path.append('code')\n",
    "    SavePathSLADS = trainingDataPath + 'SLADSResults' \n",
    "    Thresh = np.zeros(NumTrainingImages)\n",
    "    count=0\n",
    "    for ImNum in range(0,NumTrainingImages): \n",
    "        LoadPath = SavePathSLADS + os.path.sep + 'Image_' + str(ImNum+1) + '_c_'+ str(Best_c) + os.path.sep\n",
    "        StopCondFuncVal = np.load(LoadPath + 'StopCondFuncVal.npy')\n",
    "        TD = np.load(LoadPath + 'TD.npy')\n",
    "        found=0\n",
    "        for i in range(0,TD.shape[0]):\n",
    "            if TD[i]<DesiredTD and found==0 :\n",
    "                Idx = int((reconPercVector[i]-PercentageInitialMask)*SizeImage[0]*SizeImage[1]/100)\n",
    "                Thresh[ImNum]=StopCondFuncVal[Idx][0]\n",
    "                count=count+1\n",
    "                found=1\n",
    "    Threshold = np.sum(Thresh)/count\n",
    "    sys.path.pop()\n",
    "    return Threshold\n",
    "\n",
    "def generateInitialMask(InitialMaskObject,SizeImage):\n",
    "    if InitialMaskObject.MaskType =='R':\n",
    "        Mask = np.zeros((SizeImage[0],SizeImage[1]))\n",
    "        UnifMatrix = np.random.rand(SizeImage[0],SizeImage[1])\n",
    "        Mask = UnifMatrix<(InitialMaskObject.Percentage/100)\n",
    "    elif InitialMaskObject.MaskType =='U':\n",
    "        Mask = np.zeros((SizeImage[0],SizeImage[1]))\n",
    "        ModVal = int(100/InitialMaskObject.Percentage)\n",
    "        for r in range(0,SizeImage[0]):\n",
    "            for s in range(0,SizeImage[1]): \n",
    "                LinIdx = r*SizeImage[1]+s\n",
    "                if np.remainder(LinIdx,ModVal)==0:\n",
    "                    Mask[r][s]=1\n",
    "    return Mask\n",
    "        \n",
    "def plotImage(Image,Num):\n",
    "    plt.imshow(Image)\n",
    "    #pylab.show()\n",
    "\n",
    "def plotAfterSLADS(Im1,Im2):\n",
    "    plt.figure(1)                \n",
    "    plt.subplot(121)          \n",
    "    plt.imshow(Im1)\n",
    "    plt.title('Sampled Mask')   \n",
    "    plt.subplot(122)        \n",
    "    plt.imshow(Im2)\n",
    "    plt.title('Reconstructed Image')\n",
    "\n",
    "def importImages(dirPath, inputExtension, SizeImage):\n",
    "    if (inputExtension == \".tif\"):\n",
    "        dataFileNames = glob.glob(dirPath + \"/*\" + inputExtension) #Obtain filenames for each set\n",
    "        zLen = len(dataFileNames) #Find total number of files imported\n",
    "        if (zLen == 0):\n",
    "            sys.exit(\"Error!!! There are no files with extension: \" + inputExtension + \" in the directory: \" + dirPath)\n",
    "        dataset = Image.open(dataFileNames[0]).convert('L') #Read in an image as grayscale\n",
    "        dataset = np.asarray(dataset,dtype=np.float64).reshape((dataset.size[1],dataset.size[0])) #Flatten the image\n",
    "        #firstX, firstY = dataset.shape[0], dataset.shape[1] #Obtain the set's dimensions\n",
    "        firstX, firstY = SizeImage[0], SizeImage[1]\n",
    "        datasets = [] #Create an empty array to hold instances of imageData objects\n",
    "        counter = 0\n",
    "        for file in dataFileNames: #For each of the filenames\n",
    "            dataset = Image.open(file).convert('L') #Read in an image as grayscale\n",
    "            datasetXLen, datasetYLen = dataset.size #Obtain the set's dimensions\n",
    "            if ((SizeImage[0], SizeImage[1]) != (datasetXLen, datasetYLen)): #If any of the images differ in dimensions\n",
    "                if(debugInfo): print(\"Warning!!! File:\", dataFileNames[counter], \"has dimensions X: \", datasetXLen, \" Y: \", datasetYLen, \" - Will resize\") #Inform the user\n",
    "                dataset = dataset.resize([SizeImage[0], SizeImage[1]]) #Resize the file to the dimensions specified\n",
    "            dataset = np.asarray(dataset,dtype=np.float64).reshape((dataset.size[1],dataset.size[0])) #Flatten the image\n",
    "            outputName = file\n",
    "            outputName = outputName[outputName.startswith(dirPath) and len(dirPath):]\n",
    "            outputName = re.sub('\\.tif$', '', outputName)\n",
    "            datasets.append(imageData(dataset, outputName))\n",
    "            counter+=1\n",
    "    return datasets\n",
    "\n",
    "\n",
    "def gausKern_parhelper(sigmaVal, WindowSize, area): #Parallel loop for generating a Gaussian kernel\n",
    "    return area*generateGaussianKernel(sigmaVal,WindowSize) #Calculate an \"area\" that the c value will capture based on a gaussian filter\n",
    "\n",
    "def stats_parhelper(NumImages, Mask, codePath, TrainingImageSet, SizeImage, StopCondParamsObject, Theta, TrainingInfoObject, TestingInfoObject, Resolution, ImageType, UpdateERDParamsObject, BatchSamplingParamsObject, trainingPlotFeaturesPath, SimulationRun, ImNum, ImageExtension, PlotResult, Classify, directImagePath, errorPlot, isRunningParallel):    \n",
    "    return runSLADSSimulationOnce(NumImages, Mask, codePath, TrainingImageSet, SizeImage, StopCondParamsObject, Theta, TrainingInfoObject, TestingInfoObject, Resolution, ImageType, UpdateERDParamsObject, BatchSamplingParamsObject, trainingPlotFeaturesPath, SimulationRun, ImNum, ImageExtension, PlotResult, Classify, directImagePath, errorPlot, isRunningParallel)\n",
    "\n",
    "#Return the save path for output plots from testing of end models\n",
    "def testingOutputName(testingFeaturesPath, dataFileName, StopPercentageSLADS, StopPercentageTestingSLADS):\n",
    "    outputName = dataFileName\n",
    "    outputName = outputName[outputName.startswith(dataPath) and len(dataPath):]\n",
    "    outputName = re.sub('\\.png', '', outputName)\n",
    "    outputName = re.sub(testingDataImagesPath, '', outputName)\n",
    "    outputName = outputName + '_c_'+ str(c) + '_StopTrainPerc_' + str(StopPercentageSLADS) + '_StopTestPerc_' + str(StopPercentageTestingSLADS) + '_'\n",
    "    return testingFeaturesPath + outputName\n",
    "\n",
    "def cls(): #Clear console screen\n",
    "    os.system('cls' if os.name=='nt' else 'clear')   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAIN PROGRAM\n",
    "#==================================================================\n",
    "cls() #Clear the screen\n",
    "jNotebook = False #Is the program being run in a jupyter-notebook; Program progress bars will not function correctly if True\n",
    "\n",
    "#GENERAL PARAMETERS: L-01\n",
    "#==================================================================\n",
    "\n",
    "#Is training of a model to be performed\n",
    "trainingModel = True\n",
    "\n",
    "#Is testing of a model to be performed\n",
    "testingModel = True\n",
    "\n",
    "#Should the default multithreading be limited\n",
    "overrideThreads = False\n",
    "\n",
    "#If the default multithreading be limited how many threads should be used\n",
    "num_threads = 1\n",
    "\n",
    "#Should warning/debug information be displayed?\n",
    "debugInfo = False\n",
    "\n",
    "#Type of Image: D - for discrete (binary) image; C - for continuous\n",
    "ImageType = 'C'\n",
    "\n",
    "#What is the file extenstion of the data?\n",
    "inputExtension = \".tif\";\n",
    "\n",
    "#What is the symmetric length all images should be resized to?\n",
    "#(64x64), (128x128), (256x256), (512x512)\n",
    "SizeImage = [64, 64]\n",
    "\n",
    "#Find threshold for stopping function Y/N; If 'Y', set the DesiredTD in L-1\n",
    "FindStopThresh = 'N'\n",
    "\n",
    "#TRAINING MODEL PARAMETERS: L-02\n",
    "#==================================================================\n",
    "\n",
    "#Enter 'X' for TrainingInputDB_X\n",
    "TrainingImageSet = '1'\n",
    "\n",
    "#Should the input data be split into training/testing sets\n",
    "#If true and customTesting is also true, then custom set will be added to the split of the trainingImageSet database\n",
    "#If false and testingModel is true, then only custom set will be used\n",
    "splitInputSet = True\n",
    "\n",
    "#Should error data be plotted for the training data\n",
    "trainingErrorPlot = True\n",
    "\n",
    "#If trainingErrorPlot, should the plot use the testing data to test convergence\n",
    "trainingErrorPlotwTesting = True\n",
    "\n",
    "#Should c value vs distortion metric be plotted; (NOT WORKING CORRECTLY)\n",
    "cPlot = False\n",
    "\n",
    "#Stopping percentage for SLADS (to select C)\n",
    "#Suggested: (64x64):50, (128x128):30, (256x256):20, (512x512):10\n",
    "#Given ttPlotAverageErrors linestyle settings please enter no more than 4 values\n",
    "StopPercentageSLADSArr = np.array([50])\n",
    "\n",
    "#TESTING MODEL PARAMETERS: L-03\n",
    "#==================================================================\n",
    "\n",
    "#Is there a custom testing set of images to be used?\n",
    "customTesting = False\n",
    "\n",
    "#If customTesting set is true, enter 'X' for TestingInputDB_X\n",
    "TestingImageSet = '1'\n",
    "\n",
    "#If customTesting set is false, indicate % of data to use for training; test is 1-(trainSplit/100)\n",
    "trainSplit = 80 \n",
    "\n",
    "#Should error data be plotted for the testing data\n",
    "testingErrorPlot = True\n",
    "\n",
    "#Should the best c found during training be overidden?\n",
    "overrideBestC = False\n",
    "\n",
    "#If the best c should be overiden with a manual value what should that value be?\n",
    "overrideBestCValue = 4\n",
    "\n",
    "#Stopping percentage for SLADS\n",
    "#Default uses the same value as the training model\n",
    "#StopPercentageTestingSLADSArr = StopPercentageSLADSArr\n",
    "StopPercentageTestingSLADSArr = np.array([1, 5, 10, 20, 30, 40, 50, 60, 70, 80])\n",
    "\n",
    "#PROGRAM PARAMETERS: L-1\n",
    "#==================================================================\n",
    "\n",
    "# Sweep range for c (to select best c for RD approximation)\n",
    "c_vec = np.array([2,4,8,16,32])\n",
    "\n",
    "# Sampling mask measurement percentages for training (best left unchanged)\n",
    "MeasurementPercentageVector = np.array([5,10,20,40,80])\n",
    "\n",
    "# Window size for approximate RD summation (best left unchanged)\n",
    "WindowSize = [15,15]\n",
    "\n",
    "# Update ERD or compute full ERD in SLADS (to find best c)\n",
    "# with Update ERD, ERD only updated for a window surrounding new measurement\n",
    "Update_ERD = 'Y' \n",
    "\n",
    "# Smallest ERD update window size permitted\n",
    "MinWindSize = 3  \n",
    "\n",
    "# Largest ERD update window size permitted  \n",
    "MaxWindSize = 10       \n",
    "\n",
    "# Initial Mask for SLADS (to find best c):\n",
    "# Percentage of samples in initial mask\n",
    "PercentageInitialMask = 1\n",
    "\n",
    "# Type of initial mask   \n",
    "# Choices: \n",
    "    # 'U': Uniform mask; can choose any percentage\n",
    "    # 'R': Randomly distributed mask; can choose any percentage\n",
    "    # 'H': low-dsicrepacy mask; can only choose 1% mas\n",
    "MaskType = 'H'                   \n",
    "\n",
    "# Desired total distortion (TD) value (to find threshold on stopping function)\n",
    "# TD = D(X,\\hat(X))/(Number of pixels in image)\n",
    "# D(X,\\hat(X)) is difference between actual image X and reconstructed image \n",
    "# \\hat(X) (summed over all pixels)\n",
    "# For ImageType 'D' in range [0-1] for ImageType 'C' in range [0-max value]\n",
    "DesiredTD=0\n",
    "\n",
    "#What is the file extention desired for the pre-processed data\n",
    "ImageExtension = \".png\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STATIC VARIABLE SETUP\n",
    "#==================================================================\n",
    "\n",
    "#Set the number of cpu threads to be used for generating Gaussian Kernals\n",
    "if not overrideThreads:\n",
    "    num_threads = multiprocessing.cpu_count() #Determine number of available threads\n",
    "\n",
    "#Determine console size\n",
    "if not jNotebook:\n",
    "    consoleRows, consoleColumns = os.popen('stty size', 'r').read().split()\n",
    "else:\n",
    "    consoleRows = 40\n",
    "    consoleColumns = 40\n",
    "\n",
    "#Set global plot parameters\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "    \n",
    "#Convert types as needed\n",
    "c_vec = c_vec.astype(float)\n",
    "MeasurementPercentageVector = MeasurementPercentageVector.astype(float)\n",
    "StopPercentageSLADSArr = StopPercentageSLADSArr.astype(float)\n",
    "PercentageInitialMask = float(PercentageInitialMask)\n",
    "\n",
    "NumReconsSLADS = 10 #UNKNOWN VARIABLE\n",
    "PercOfRD = 50 #UNKNOWN VARIABLE\n",
    "Classify = 'N' #UNKNOWN VARIABLE\n",
    "Resolution = 1 #UNKNOWN VARIABLE\n",
    "\n",
    "falseFlag = False #Hold a False variable for function calls to disable certain parameters\n",
    "\n",
    "#Store training information as object\n",
    "TrainingInfoObject = TrainingInfo()\n",
    "\n",
    "#Initalize the training information as set through specifieed variables\n",
    "if ImageType == 'D':\n",
    "    TrainingInfoObject.initialize('DWM','DWM',2,10,'Gaussian',0,0.25,15)\n",
    "elif ImageType == 'C':\n",
    "    TrainingInfoObject.initialize('CWM','CWM',2,10,'Gaussian',0,0.25,15)\n",
    "\n",
    "#Store testing information as object\n",
    "TestingInfoObject = TestingInfo()\n",
    "TestingInfoObject.initialize()\n",
    "    \n",
    "InitialMaskObject = InitialMask()\n",
    "InitialMaskObject.initialize(SizeImage[0],SizeImage[1],MaskType,1,PercentageInitialMask)\n",
    "    \n",
    "#PATH/DIRECTORY SETUP\n",
    "#==================================================================\n",
    "\n",
    "#Set starting data path\n",
    "codePath = '.' + os.path.sep\n",
    "\n",
    "#Set path to Results and Data Folder\n",
    "resultsDataPath = codePath + 'ResultsAndData' + os.path.sep\n",
    "\n",
    "#Check directory\n",
    "if not os.path.exists(resultsDataPath):                                                                                                                          \n",
    "    sys.exit('Error!!! The folder ' + resultsDataPath + ' does not exist. ')\n",
    "\n",
    "TrainingDBName = 'TrainingDB_' + TrainingImageSet\n",
    "\n",
    "#If split is being conducted\n",
    "if splitInputSet:\n",
    "    dataPath = resultsDataPath + 'InputData' + os.path.sep + 'InputTrainingDB_' + str(TrainingImageSet) + os.path.sep\n",
    "\n",
    "if trainingModel: #If training is to be performed, check the relevant directories\n",
    "    \n",
    "    #Path to the input training image database\n",
    "    dataPath = resultsDataPath + 'InputData' + os.path.sep + 'InputTrainingDB_' + str(TrainingImageSet) + os.path.sep\n",
    "    if not os.path.exists(dataPath):                                                                                                                          \n",
    "        sys.exit('Error!!! The folder ' + dataPath + ' does not exist. Check entry for ' + TrainingImageSet)\n",
    "\n",
    "    #Path to where training resources/features should be saved\n",
    "    trainingFeaturesPath = resultsDataPath + 'TrainingSavedFeatures' + os.path.sep\n",
    "    if os.path.exists(trainingFeaturesPath):\n",
    "        shutil.rmtree(trainingFeaturesPath)\n",
    "    os.makedirs(trainingFeaturesPath)\n",
    "          \n",
    "    if trainingErrorPlot:\n",
    "        trainingPlotFeaturesPath = trainingFeaturesPath + 'Plot' + os.path.sep\n",
    "        if os.path.exists(trainingPlotFeaturesPath):\n",
    "            shutil.rmtree(trainingPlotFeaturesPath)\n",
    "        os.makedirs(trainingPlotFeaturesPath)\n",
    "    \n",
    "    #Set path to where training data should be kept\n",
    "    trainingDataPath = trainingFeaturesPath + 'TrainingDB_' + str(TrainingImageSet) + os.path.sep\n",
    "    if not os.path.exists(trainingDataPath):                                                                                                                          \n",
    "        os.makedirs(trainingDataPath)\n",
    "    \n",
    "    #Set path to where training data images should be kept\n",
    "    trainingDataImagesPath = trainingDataPath + 'Images' + os.path.sep   \n",
    "    if os.path.exists(trainingDataImagesPath):\n",
    "        shutil.rmtree(trainingDataImagesPath)\n",
    "    os.makedirs(trainingDataImagesPath)\n",
    "        \n",
    "    #Clear and setup training model path\n",
    "    if os.path.exists(trainingDataPath + 'FeaturesRegressCoeffs'):\n",
    "        shutil.rmtree(trainingDataPath + 'FeaturesRegressCoeffs')\n",
    "    os.makedirs(trainingDataPath + 'FeaturesRegressCoeffs')\n",
    "    \n",
    "if testingModel: #If testing is to be performed, check the relevant directories\n",
    "     \n",
    "    #Path to where training resources/features should be saved\n",
    "    testingFeaturesPath = resultsDataPath + 'TestingSavedFeatures' + os.path.sep\n",
    "    if os.path.exists(testingFeaturesPath):\n",
    "        shutil.rmtree(testingFeaturesPath)\n",
    "    os.makedirs(testingFeaturesPath)\n",
    "        \n",
    "    #Set path to where testing data should be kept\n",
    "    testingDataPath = testingFeaturesPath + 'TestingDB_' + str(TrainingImageSet) + os.path.sep\n",
    "    if not os.path.exists(testingDataPath):                                                                                                                          \n",
    "        os.makedirs(testingDataPath)\n",
    "        \n",
    "    #Set path to where initial input files are located\n",
    "    if (customTesting): #If a custom training set is going to be used\n",
    "        customTestingDataPath = resultsDataPath + 'InputData' + os.path.sep + 'InputTestingDB_' + str(TestingImageSet) + os.path.sep\n",
    "        if not os.path.exists(customTestingDataPath):\n",
    "            sys.exit('Error!!! The folder ' + customTestingDataPath + ' does not exist. Check entry for ' + customTestingDataPath + ' or set the customTesting flag to false.')\n",
    "    \n",
    "    #Set data path to where data should be split from\n",
    "    if not customTesting and not trainingModel and splitInputSet:\n",
    "        dataPath = resultsDataPath + 'InputData' + os.path.sep + 'InputTrainingDB_' + str(TrainingImageSet) + os.path.sep\n",
    "        \n",
    "    #Set path to where testing data images should be kept \n",
    "    testingDataImagesPath = testingDataPath + 'Images' + os.path.sep   \n",
    "    if os.path.exists(testingDataImagesPath):\n",
    "        shutil.rmtree(testingDataImagesPath)\n",
    "    os.makedirs(testingDataImagesPath)\n",
    "        \n",
    "    #Clear and setup testing model path\n",
    "    if os.path.exists(testingDataPath + 'FeaturesRegressCoeffs'):\n",
    "        shutil.rmtree(testingDataPath + 'FeaturesRegressCoeffs')\n",
    "    os.makedirs(testingDataPath + 'FeaturesRegressCoeffs')\n",
    "\n",
    "    #Set path to save training results\n",
    "    TrainingSavePath = resultsDataPath + 'SLADSSimulationResults' + os.path.sep + 'TrainingDB_' + TrainingImageSet + os.path.sep\n",
    "    if os.path.exists(TrainingSavePath):\n",
    "        shutil.rmtree(TrainingSavePath)\n",
    "    os.makedirs(TrainingSavePath)\n",
    "    \n",
    "    #Set path to save testing results\n",
    "    testingSavePath = resultsDataPath + 'SLADSSimulationResults' + os.path.sep + 'TestingDB_' + TestingImageSet + os.path.sep\n",
    "    if os.path.exists(testingSavePath):\n",
    "        shutil.rmtree(testingSavePath)\n",
    "    os.makedirs(testingSavePath)\n",
    "    \n",
    "    #Set path for saving the resulting images from the testing results\n",
    "    ImagesSavePath = testingSavePath + 'TestingImageResults' + os.path.sep    \n",
    "    if os.path.exists(ImagesSavePath):\n",
    "        shutil.rmtree(ImagesSavePath)  \n",
    "    os.makedirs(ImagesSavePath)\n",
    "\n",
    "    #Set path for saving the resulting training statistics\n",
    "    trainingStatisticsSavePath = TrainingSavePath + 'Statistics' + os.path.sep    \n",
    "    if os.path.exists(trainingStatisticsSavePath):\n",
    "        shutil.rmtree(trainingStatisticsSavePath)  \n",
    "    os.makedirs(trainingStatisticsSavePath)    \n",
    "\n",
    "    #Set path for saving the resulting testing statistics\n",
    "    testingStatisticsSavePath = testingSavePath + 'Statistics' + os.path.sep    \n",
    "    if os.path.exists(testingStatisticsSavePath):\n",
    "        shutil.rmtree(testingStatisticsSavePath)  \n",
    "    os.makedirs(testingStatisticsSavePath)    \n",
    "    \n",
    "    #Set the path for where the initial mask locations are \n",
    "    loadPathInitialMask = resultsDataPath + 'InitialSamplingMasks' #Set initial mask path\n",
    "    if not os.path.exists(loadPathInitialMask):                                                                                                                          \n",
    "        sys.exit('Error!!! The folder ' + loadPathInitialMask + ' does not exist. Check entry for ' + loadPathInitialMask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA IMPORTATION\n",
    "#==================================================================\n",
    "print('#' * int(consoleColumns))\n",
    "print('PRE-PROCESSING DATA')\n",
    "print('#' * int(consoleColumns) + '\\n')\n",
    "#Check validity of training/testing parameters\n",
    "if testingModel and not splitInputSet and not customTesting:\n",
    "    sys.exit(\"ERROR!!! Testing enabled, but neither custom, nor split input sets enabled\")\n",
    "\n",
    "if splitInputSet and not testingModel:\n",
    "    if(debugInfo): print(\"WARNING!!! Testing disabled, but splitInputSet enabled. Only the specified fraction of the training set will be used\")\n",
    "\n",
    "#If any training is to be performed\n",
    "if trainingModel: \n",
    "    datasets = importImages(dataPath, inputExtension, SizeImage) #Import images and perform pre-processing\n",
    "    datasets = shuffle(datasets) #Randomize dataset order\n",
    "    if splitInputSet: #If data should be split\n",
    "        numTrain = int(len(datasets)*(trainSplit/100)) #Find number of training examples; round as int for indexing\n",
    "        trainingData = datasets[0:numTrain] #Split off the training dataset\n",
    "    else: #The data should not be split\n",
    "        trainingData = datasets #Simply pass along the data as the training set\n",
    "        \n",
    "    #Save pre-processed training images   \n",
    "    for i in range(0,len(trainingData)): #For each dataset in the training data\n",
    "        dataset = trainingData[i]\n",
    "        grayImage=Image.fromarray(dataset.data).convert(\"L\") #Reset the values for the image such that they are visible\n",
    "        grayImage.save(trainingDataImagesPath+dataset.name+ImageExtension) #Save the output image\n",
    "        trainingData[i] = misc.imread(trainingDataImagesPath+dataset.name+ImageExtension)\n",
    "        \n",
    "if testingModel:\n",
    "    if splitInputSet and not trainingModel:\n",
    "        datasets = importImages(dataPath, inputExtension, SizeImage) #Import images and perform pre-processing\n",
    "        datasets = shuffle(datasets) #Randomize dataset order\n",
    "        numTrain = int(len(datasets)*(trainSplit/100)) #Find number of training examples; round as int for indexing\n",
    "    \n",
    "    #If only split data from the training set is to be used for testing\n",
    "    if not customTesting and splitInputSet:\n",
    "        testingData = datasets[numTrain:(len(datasets))]\n",
    "\n",
    "    #If split data and custom data should be used for testing\n",
    "    if customTesting and splitInputSet:\n",
    "        testingData = datasets[numTrain:(len(datasets))] + importImages(customTestingDataPath, inputExtension, SizeImage)\n",
    "\n",
    "    #If only custom data should be used for testing\n",
    "    if customTesting and not splitInputSet:\n",
    "        testingData = importImages(customTestingDataPath, inputExtension, SizeImage)\n",
    "    \n",
    "    #Save pre-processed testingData\n",
    "    for i in range(0, len(testingData)): \n",
    "        dataset = testingData[i]\n",
    "        grayImage=Image.fromarray(dataset.data).convert(\"L\") #Reset the values for the image such that they are visible\n",
    "        grayImage.save(testingDataImagesPath+dataset.name+ImageExtension) #Save the output image\n",
    "        testingData[i] = misc.imread(testingDataImagesPath+dataset.name+ImageExtension) #Import into final testing dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#RUN TRAINING\n",
    "#==================================================================  \n",
    "if trainingModel:\n",
    "    print(' \\r') #Reset cursor\n",
    "    print('#' * int(consoleColumns))\n",
    "    print('TRAINING MODEL')\n",
    "    print('#' * int(consoleColumns) + '\\n')\n",
    "    \n",
    "    NumTrainingImages = len(trainingData)\n",
    "    imagePercent = (1/NumTrainingImages)*100\n",
    "    isRunningParallel = True\n",
    "    if trainingErrorPlotwTesting:\n",
    "            if not testingModel:\n",
    "                sys.exit('Error!!! trainningErrorPlotwTesting enabled, but testingModel is not')\n",
    "            else: \n",
    "                testingImageFiles = glob.glob(testingDataImagesPath + \"/*\" + ImageExtension) #Obtain filenames for each set\n",
    "\n",
    "    #For each of the possible sampling percentages\n",
    "    for p in tqdm(range(0,len(StopPercentageSLADSArr)), desc = 'Sampling Percentages', leave = True):\n",
    "        \n",
    "        StopPercentageSLADS = StopPercentageSLADSArr[p]\n",
    "        #Vector of values to determine for the reconstruction\n",
    "\n",
    "        reconPercVector = np.linspace(PercentageInitialMask, StopPercentageSLADS, num=NumReconsSLADS*(StopPercentageSLADS-PercentageInitialMask), endpoint=False)\n",
    "\n",
    "        #For each of the Sampling mask measurement percentages for training\n",
    "        #For each of the images want to maximize the reduction in distortion \n",
    "        #Train regression formula on known images with randomly selected sampling densities\n",
    "        #Estimation of distortion reduction based on a provided weight parameter and the distance between the previous and proposed pixel location\n",
    "        samplingPercent = (imagePercent/np.size(MeasurementPercentageVector))\n",
    "        cPercent = samplingPercent/len(c_vec)\n",
    "\n",
    "        for ImNum in tqdm(range(0,len(trainingData)), desc = 'Training Images', leave = True):\n",
    "            Img = trainingData[ImNum] #Retreive a training image\n",
    "            try:\n",
    "                Img\n",
    "            except NameError:\n",
    "                sys.exit('Error!!! There are no images in the training set!')\n",
    "\n",
    "            for m in tqdm(range(0,np.size(MeasurementPercentageVector)), desc = 'Sampling Densities', leave = True): #For each of the proposed sampling densities\n",
    "                #Create a save folder for the produced information\n",
    "                SaveFolder = 'Image_' + str(ImNum+1) + '_SampPerc_' + str(MeasurementPercentageVector[m])\n",
    "                trainingFeaturesSavePath = trainingDataPath + 'FeaturesRegressCoeffs' + os.path.sep + SaveFolder\n",
    "                if not os.path.exists(trainingFeaturesSavePath):\n",
    "                    os.makedirs(trainingFeaturesSavePath)\n",
    "\n",
    "                #Create a random uniform, boolean mask at the sampling density specified and apply it to the image\n",
    "                Mask = np.zeros((SizeImage[0],SizeImage[1]))\n",
    "                UnifMatrix = np.random.rand(SizeImage[0],SizeImage[1])\n",
    "                Mask = UnifMatrix<(MeasurementPercentageVector[m]/100)\n",
    "                MeasuredIdxs = np.transpose(np.where(Mask==1))\n",
    "                UnMeasuredIdxs = np.transpose(np.where(Mask==0))            \n",
    "                MeasuredValues = Img[Mask==1]\n",
    "\n",
    "                # Find neighbors\n",
    "                NeighborValues,NeighborWeights,NeighborDistances = FindNeighbors(TrainingInfoObject,MeasuredIdxs,UnMeasuredIdxs,MeasuredValues,Resolution)\n",
    "\n",
    "                #Form the reconstructed image\n",
    "                ReconValues,ReconImage = ComputeRecons(TrainingInfoObject,NeighborValues,NeighborWeights,SizeImage,UnMeasuredIdxs,MeasuredIdxs,MeasuredValues)\n",
    "\n",
    "                #Compute the image features\n",
    "                AllPolyFeatures=computeFeatures(UnMeasuredIdxs,SizeImage,NeighborValues,NeighborWeights,NeighborDistances,TrainingInfoObject,ReconValues,ReconImage,Resolution,ImageType)\n",
    "\n",
    "                #Determine a number of points \n",
    "                #unmasked percent * percentage of distortion reduction * imageArea / (100*100)\n",
    "                NumRandChoices = int((100-MeasurementPercentageVector[m])*PercOfRD*SizeImage[1]*SizeImage[0]/(100*100))\n",
    "\n",
    "                #Create a sample based on a number of previously unmeasured points\n",
    "                OrderForRD = random.sample(range(0,UnMeasuredIdxs.shape[0]), NumRandChoices) \n",
    "\n",
    "                #Extract the image features of interest from the randomly selected unmeasured points\n",
    "                PolyFeatures = AllPolyFeatures[OrderForRD,:]\n",
    "\n",
    "                #Compute the differences between the original and reconstructed images\n",
    "                RDPP = computeDifference(Img,ReconImage,ImageType)\n",
    "\n",
    "                #Round differences to nearest integer\n",
    "                RDPP.astype(int)\n",
    "\n",
    "                #Pad the differences\n",
    "                RDPPWithZeros = np.lib.pad(RDPP,(int(np.floor(WindowSize[0]/2)),int(np.floor(WindowSize[1]/2))),'constant',constant_values=0)\n",
    "\n",
    "                #Convert image to an array\n",
    "                ImgAsBlocks = im2col(RDPPWithZeros,WindowSize)\n",
    "\n",
    "                #Flatten 2D mask array to 1-D\n",
    "                MaskVect = np.ravel(Mask)\n",
    "\n",
    "                #Identify the pixels that have not yet been measured\n",
    "                ImgAsBlocksOnlyUnmeasured = ImgAsBlocks[:,np.logical_not(MaskVect)]\n",
    "\n",
    "                temp = np.zeros((WindowSize[0]*WindowSize[1],NumRandChoices))\n",
    "                for n in tqdm(range(0,len(c_vec)), desc = 'c Values', leave = True): #For each of the possible c values\n",
    "                    c = c_vec[n]\n",
    "                    sigma = NeighborDistances[:,0]/c\n",
    "                    cnt = 0;\n",
    "                    \n",
    "                    #parallize runSLADSSimulationOnce\n",
    "                    area = Parallel(n_jobs=num_threads)(delayed(gausKern_parhelper)(sigma[OrderForRD[index]], WindowSize, ImgAsBlocksOnlyUnmeasured[:,OrderForRD[index]]) for index in tqdm(range(0,len(OrderForRD)), desc = 'Gaussian', leave = True)) #Perform task in parallel\n",
    "                    for i in range (0,len(OrderForRD)): temp[:,i] = area[i]\n",
    "                    RD = np.sum(temp, axis=0) #Determine how much \"area of uncertainty\" is possibly removed for a c value\n",
    "\n",
    "                    #Save everything\n",
    "                    SavePath_c = trainingFeaturesSavePath + os.path.sep + 'c_' + str(c)\n",
    "                    if not os.path.exists(SavePath_c):\n",
    "                        os.makedirs(SavePath_c)\n",
    "                    np.save(SavePath_c + os.path.sep + 'RD' + '_StopPerc_' + str(StopPercentageSLADS), RD)        \n",
    "                    np.save(SavePath_c + os.path.sep + 'OrderForRD' + '_StopPerc_' + str(StopPercentageSLADS), OrderForRD)\n",
    "\n",
    "                np.save(trainingFeaturesSavePath + os.path.sep + 'Mask' + '_StopPerc_'+ str(StopPercentageSLADS), Mask)   \n",
    "                np.save(trainingFeaturesSavePath + os.path.sep + 'ReconImage' + '_StopPerc_' + str(StopPercentageSLADS), ReconImage)\n",
    "                np.save(trainingFeaturesSavePath + os.path.sep + 'PolyFeatures' + '_StopPerc_' + str(StopPercentageSLADS), PolyFeatures)\n",
    " \n",
    "    print('\\n\\n\\n\\n\\n' + ('-' * int(consoleColumns)))\n",
    "    print('PERFORM TRAINING')\n",
    "    print('-' * int(consoleColumns) + '\\n')\n",
    "    \n",
    "    #For each of the possible sampling percentages\n",
    "    for p in tqdm(range(0,len(StopPercentageSLADSArr)), desc = 'Sampling Percentages', leave = True):\n",
    "        StopPercentageSLADS = StopPercentageSLADSArr[p]\n",
    "        \n",
    "        #Append the observed polyfeatures for each sampling density and image to a single big array\n",
    "        #NOTE: WHAT ABOUT LEAVING THESE IN MEMORY TO REDUCE TIME?\n",
    "        for i in tqdm(range(0,len(c_vec)), desc = 'c Values', leave = True): #For each of the proposed c values\n",
    "            c = c_vec[i]\n",
    "            FirstLoop = 1\n",
    "            for ImNum in tqdm(range(0,NumTrainingImages), desc = 'Training Images', leave = True): #For each of the possible training images\n",
    "                \n",
    "                for m in tqdm(range(0,np.size(MeasurementPercentageVector)), desc = 'Sampling Densities', leave = True): #For each of the proposed sampling densities\n",
    "                    #Set loading paths\n",
    "                    LoadPath = trainingDataPath + 'FeaturesRegressCoeffs' + os.path.sep + 'Image_' + str(ImNum+1) + '_SampPerc_' + str(MeasurementPercentageVector[m])\n",
    "                    LoadPath_c = LoadPath + os.path.sep + 'c_' + str(c)\n",
    "\n",
    "                    #Load the image polynomial feature \n",
    "                    PolyFeatures = np.load(LoadPath + os.path.sep + 'PolyFeatures' + '_StopPerc_' + str(StopPercentageSLADS)+ '.npy')\n",
    "\n",
    "                    #Load the possible reduction in distortion value\n",
    "                    RD = np.load(LoadPath_c + os.path.sep + 'RD' + '_StopPerc_' + str(StopPercentageSLADS)+ '.npy')\n",
    "\n",
    "                    if ImageType=='D':\n",
    "                        if FirstLoop==1:\n",
    "                            BigPolyFeatures = np.column_stack((PolyFeatures[:,0:25],PolyFeatures[:,26]))\n",
    "                            BigRD = RD\n",
    "                            FirstLoop = 0                  \n",
    "                        else:\n",
    "                            TempPolyFeatures = np.column_stack((PolyFeatures[:,0:25],PolyFeatures[:,26]))                    \n",
    "                            BigPolyFeatures = np.row_stack((BigPolyFeatures,TempPolyFeatures))\n",
    "                            BigRD = np.append(BigRD,RD)\n",
    "\n",
    "                    else: #If image type is continuous\n",
    "                        if FirstLoop==1: #If for the first possible c\n",
    "                            BigPolyFeatures = PolyFeatures\n",
    "                            BigRD = RD\n",
    "                            FirstLoop = 0                  \n",
    "                        else:\n",
    "                            TempPolyFeatures = PolyFeatures               \n",
    "                            BigPolyFeatures = np.row_stack((BigPolyFeatures,TempPolyFeatures))\n",
    "                            BigRD = np.append(BigRD,RD)                    \n",
    "\n",
    "            regr = linear_model.LinearRegression()\n",
    "\n",
    "            #Perform the regression, fitting the observed polynomial fetures to the expected reduction in distortion\n",
    "            regr.fit(BigPolyFeatures, BigRD)\n",
    "\n",
    "            Theta = np.zeros((PolyFeatures.shape[1]))    \n",
    "\n",
    "            if ImageType=='D':            \n",
    "                Theta[0:24]=regr.coef_[0:24]\n",
    "                Theta[26]=regr.coef_[25]\n",
    "            else: #If image type is continuous\n",
    "                Theta = regr.coef_\n",
    "\n",
    "            del BigRD,BigPolyFeatures\n",
    "\n",
    "            SavePath_c = trainingDataPath + 'c_' + str(c)\n",
    "            if not os.path.exists(SavePath_c):\n",
    "                os.makedirs(SavePath_c) \n",
    "            np.save(SavePath_c + os.path.sep + 'Theta' + '_StopPerc_' + str(StopPercentageSLADS), Theta)\n",
    "\n",
    "    print('\\n\\n\\n\\n' + ('-' * int(consoleColumns)))\n",
    "    print('DETERMINING BEST C')\n",
    "    print('-' * int(consoleColumns) + '\\n')            \n",
    "            \n",
    "    #For each of the possible sampling percentages\n",
    "    for p in tqdm(range(0,len(StopPercentageSLADSArr)), desc = 'Sampling Percentages', leave = True):\n",
    "        StopPercentageSLADS = StopPercentageSLADSArr[p]\n",
    "        \n",
    "        UpdateERDParamsObject = UpdateERDParams()\n",
    "        UpdateERDParamsObject.initialize(Update_ERD,MinWindSize,MaxWindSize,1.5)\n",
    "\n",
    "        # Find the best value of c\n",
    "        directImagePath = '' #Direct image support not yet supported for training sets\n",
    "        Best_c,NumImagesForSLADS = performSLADStoFindC(codePath,trainingDataPath,TrainingImageSet,ImageType,ImageExtension,TrainingInfoObject,SizeImage,StopPercentageSLADS,Resolution,c_vec,UpdateERDParamsObject,InitialMaskObject,MaskType,reconPercVector,Classify,directImagePath,consoleRows,cPlot,trainingStatisticsSavePath)\n",
    "        \n",
    "        #Set path to where best c value should be kept\n",
    "        SavePath_bestc = trainingDataPath + 'best_c' + '_StopPerc_' + str(StopPercentageSLADS) + '.npy'\n",
    "        np.save(SavePath_bestc, np.array([Best_c]))\n",
    "\n",
    "        #Directory checking\n",
    "        ThetaSavePath = trainingFeaturesPath + TrainingDBName + os.path.sep + 'c_' + str(Best_c) + os.path.sep\n",
    "        ThetaLoadPath = trainingFeaturesPath + TrainingDBName + os.path.sep + 'c_' + str(Best_c) + os.path.sep\n",
    "        if not os.path.exists(ThetaSavePath):\n",
    "            os.makedirs(ThetaSavePath)\n",
    "\n",
    "        np.save(ThetaSavePath + 'Theta' + '_StopPerc_'+ str(StopPercentageSLADS), Theta)\n",
    "\n",
    "        # Find the Threshold on stopping condition that corresponds to the desired total distortion (TD) value set above\n",
    "        if FindStopThresh=='Y':   \n",
    "            Threshold=findStoppingThreshold(trainingDataPath,NumImagesForSLADS,Best_c,PercentageInitialMask,DesiredTD,reconPercVector,SizeImage)\n",
    "            #print('For a TD of '+ str(DesiredTD) + ' set stopping function threshold to: ' + str(Threshold))\n",
    "            #print('**** Make sure to enter this value in runSimulations.py and in runSLADS.py')\n",
    "            #print('The threshold value is saved in:  ' + ThetaSavePath + ' as Threshold.npy')\n",
    "            np.save(ThetaSavePath + 'Threshold' + '_StopPerc_'+ str(StopPercentageSLADS), Threshold) \n",
    "\n",
    "\n",
    "    #Now that the optimal C value has been found for reconstruction\n",
    "    #Train the model 1 image at a time, perform a reconstruction and watch statistics over all training images\n",
    "    if (trainingErrorPlot):\n",
    "        print('\\n\\n\\n\\n' + ('-' * int(consoleColumns)))\n",
    "        print('PLOTTING MODEL TRAINING CONVERGENCE')\n",
    "        print(('-' * int(consoleColumns)) + '\\n')   \n",
    "        plotXLabel = '# Training Samples' #x label for model training convergence plot\n",
    "        errorPlot = True\n",
    "        \n",
    "        #For each of the possible sampling percentages\n",
    "        for p in tqdm(range(0,len(StopPercentageSLADSArr)), desc = 'Sampling Percentages', leave = True):\n",
    "            StopPercentageSLADS = StopPercentageSLADSArr[p]\n",
    "            Beta = computeBeta(SizeImage)\n",
    "            StopCondParamsObject = StopCondParams()\n",
    "            StopCondParamsObject.initialize(Beta,0,50,2,StopPercentageSLADS)\n",
    "            UpdateERDParamsObject = UpdateERDParams()\n",
    "            UpdateERDParamsObject.initialize(Update_ERD,MinWindSize,MaxWindSize,1.5)\n",
    "            BatchSample = 'N'\n",
    "            BatchSamplingParamsObject = BatchSamplingParams()\n",
    "            if BatchSample=='N':\n",
    "                BatchSamplingParamsObject.initialize(BatchSample,1)\n",
    "            else:\n",
    "                BatchSamplingParamsObject.initialize(BatchSample,NumSamplesPerIter)\n",
    "\n",
    "            SimulationRun = 1\n",
    "            PlotResult = 'N'\n",
    "            regr = linear_model.LinearRegression() #Construct a regression model\n",
    "            loadPathInitialMask = resultsDataPath + 'InitialSamplingMasks' # Load initial measurement mask\n",
    "            FirstLoop = 1\n",
    "            trainingStatisticsSavePathSuffix = '_StopPerc_' + str(StopPercentageSLADS)\n",
    "            \n",
    "\n",
    "            if trainingErrorPlotwTesting:\n",
    "                NumImagesPlot = len(testingImageFiles)\n",
    "                plottingFileNames = testingImageFiles\n",
    "                trainingPlotTitle = 'Averaged Total Testing Image Results'\n",
    "            else: #Using the training data for watching development of model with respect to # training samples\n",
    "                NumImagesPlot = len(trainingImageFiles)\n",
    "                plottingFileNames = trainingImageFiles\n",
    "                trainingPlotTitle = 'Averaged Total Training Image Results'\n",
    "\n",
    "            #Create an object to hold progressive development of model\n",
    "            trainingResultsAverageObject = simulationResults() \n",
    "            trainingResultsAverageObject.initialize()\n",
    "            #For each of the training images add the polynomial features determined for the best c to the model and check reconstruction capability\n",
    "            for ImNum in tqdm(range(0,NumTrainingImages), desc = 'Training Images', leave = True): \n",
    "                #Aggregate together the polynomial features determined for the best c at each of the possible sampling densities\n",
    "                for m in tqdm(range(0,np.size(MeasurementPercentageVector)), desc = 'Sampling Densities', leave = True): #For each of the proposed sampling densities\n",
    "\n",
    "                    LoadPath = trainingDataPath + 'FeaturesRegressCoeffs' + os.path.sep + 'Image_' + str(ImNum+1) + '_SampPerc_' + str(MeasurementPercentageVector[m])\n",
    "                    LoadPath_c = LoadPath + os.path.sep + 'c_' + str(Best_c) #Only using the best c found during the original training\n",
    "                    RD = np.load(LoadPath_c + os.path.sep + 'RD'+ '_StopPerc_'+ str(StopPercentageSLADS)+ '.npy')  #Load the possible reduction in distortion value\n",
    "                    PolyFeatures = np.load(LoadPath + os.path.sep + 'PolyFeatures' + '_StopPerc_'+ str(StopPercentageSLADS)+ '.npy')\n",
    "                    if ImageType=='D':\n",
    "                        if FirstLoop==1:\n",
    "                            BigPolyFeatures = np.column_stack((PolyFeatures[:,0:25],PolyFeatures[:,26]))\n",
    "                            BigRD = RD\n",
    "                            FirstLoop = 0                  \n",
    "                        else:\n",
    "                            TempPolyFeatures = np.column_stack((PolyFeatures[:,0:25],PolyFeatures[:,26]))                    \n",
    "                            BigPolyFeatures = np.row_stack((BigPolyFeatures,TempPolyFeatures))\n",
    "                            BigRD = np.append(BigRD,RD)\n",
    "                    else: #If image type is continuous\n",
    "                        if FirstLoop==1: #If initialization\n",
    "                            BigPolyFeatures = PolyFeatures\n",
    "                            BigRD = RD\n",
    "                            FirstLoop = 0                  \n",
    "                        else:\n",
    "                            TempPolyFeatures = PolyFeatures               \n",
    "                            BigPolyFeatures = np.row_stack((BigPolyFeatures,TempPolyFeatures))\n",
    "                            BigRD = np.append(BigRD,RD)                    \n",
    "\n",
    "                #Perform the regression, fitting the observed polynomial fetures to the expected reduction in distortion\n",
    "                regr = linear_model.LinearRegression() #Construct a new regression model\n",
    "                regr.fit(BigPolyFeatures, BigRD)\n",
    "                Theta = np.zeros((PolyFeatures.shape[1]))    \n",
    "                if ImageType=='D':            \n",
    "                    Theta[0:24]=regr.coef_[0:24]\n",
    "                    Theta[26]=regr.coef_[25]\n",
    "                else: #If image type is continuous\n",
    "                    Theta = regr.coef_               \n",
    "\n",
    "                #Perform SLADS on all of the images, saving statistics of interest in parallel\n",
    "                trainingResultObject = Parallel(n_jobs=num_threads)(delayed(stats_parhelper)(NumImagesPlot, loadOrGenerateInitialMask(loadPathInitialMask,MaskType,InitialMaskObject,SizeImage), codePath, TestingImageSet, SizeImage, StopCondParamsObject, Theta, TrainingInfoObject, TestingInfoObject, Resolution, ImageType, UpdateERDParamsObject, BatchSamplingParamsObject, trainingPlotFeaturesPath, SimulationRun, i, ImageExtension, PlotResult, Classify, plottingFileNames[i], errorPlot, isRunningParallel) for i in tqdm(range(0,NumImagesPlot), desc = 'Avg. Stats.', leave = False))\n",
    "\n",
    "                mseTrainingResults = []\n",
    "                ssimTrainingResults = []\n",
    "                distortTrainingResults = []\n",
    "                for result in trainingResultObject: \n",
    "                    mseTrainingResults.append(result.mseError)\n",
    "                    ssimTrainingResults.append(result.ssimError)\n",
    "                    distortTrainingResults.append(result.totalDistortion)\n",
    "                trainingResultsAverageObject.saveAverageErrorData(np.mean(mseTrainingResults), np.mean(ssimTrainingResults), np.mean(distortTrainingResults))\n",
    "            xAxisValues = np.linspace(1, len(trainingResultsAverageObject.ssimAverageErrors), len(trainingResultsAverageObject.ssimAverageErrors))         \n",
    "            trainingSpecificStatisticsSavePath = trainingStatisticsSavePath + 'StopTrainPerc_' + str(StopPercentageSLADS) + os.path.sep    \n",
    "            \n",
    "            #Directory setup for specific training statistics\n",
    "            if os.path.exists(trainingSpecificStatisticsSavePath): \n",
    "                shutil.rmtree(trainingSpecificStatisticsSavePath)\n",
    "            os.makedirs(trainingSpecificStatisticsSavePath)\n",
    "            \n",
    "            plotErrorData(trainingSpecificStatisticsSavePath, trainingStatisticsSavePathSuffix, trainingResultsAverageObject, xAxisValues, trainingPlotTitle, plotXLabel) #Plot and save the error data obtained during training\n",
    "            del BigRD,BigPolyFeatures #Clean up workspace a bit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN TESTING\n",
    "#==================================================================  \n",
    "if testingModel:\n",
    "    print('\\n\\n' + ('#' * int(consoleColumns)))\n",
    "    print('TESTING MODEL')\n",
    "    print(('#' * int(consoleColumns)) + '\\n')\n",
    "            \n",
    "    #Should the testing results be plotted\n",
    "    PlotResult='Y'\n",
    "\n",
    "    # If you want to use stopping function used, enter threshold (from Training), else leave at 0      \n",
    "    StoppingThrehsold = 0\n",
    "\n",
    "    #Static variables for sladssimulationonce call\n",
    "    isRunningParallel = True\n",
    "    SimulationRun = 1\n",
    "    errorPlot = True\n",
    "    \n",
    "    #Directory setup for training database\n",
    "    TrainingDBPath = trainingFeaturesPath + TrainingDBName\n",
    "    if not os.path.exists(TrainingDBPath): \n",
    "        sys.exit('Error!!! The folder ' + TrainingDBPath + ' does not exist. Check entry for ' + TrainingImageSet)\n",
    "\n",
    "    # Batch Sampling; If 'Y' set number of samples in each step in L-1 (NumSamplesPerIter)\n",
    "    BatchSample = 'N'\n",
    "    BatchSamplingParamsObject = BatchSamplingParams()\n",
    "    if BatchSample=='N':\n",
    "        BatchSamplingParamsObject.initialize(BatchSample,1)\n",
    "    else:\n",
    "        BatchSamplingParamsObject.initialize(BatchSample,NumSamplesPerIter)\n",
    "\n",
    "    UpdateERDParamsObject = UpdateERDParams()\n",
    "    UpdateERDParamsObject.initialize(Update_ERD,MinWindSize,MaxWindSize,1.5)\n",
    "\n",
    "    PercentageInitialMask = float(PercentageInitialMask)\n",
    "\n",
    "    Mask = loadOrGenerateInitialMask(loadPathInitialMask,MaskType,InitialMaskObject,SizeImage)\n",
    "    np.save(testingFeaturesPath + 'InitialMask', Mask)\n",
    "    savemat(testingFeaturesPath + 'InitialMask.mat', dict(Mask=Mask))\n",
    "    \n",
    "    testingPlotTitle = 'Averaged Total Testing Image Results'\n",
    "    plotXLabel = '% Sampled'\n",
    "    \n",
    "    #Create a list for holding Average results for each training sample percentage\n",
    "    trainTestAverageErrors = []\n",
    "    #For each of the possible training sampling percentages (each with their own best c)\n",
    "    for p in tqdm(range(0,len(StopPercentageSLADSArr)), desc = 'Training Sampling Percentages', leave = True):\n",
    "        StopPercentageSLADS = float(StopPercentageSLADSArr[p])\n",
    "        \n",
    "        #Import the best c and theta values; Training for database must have been performed first\n",
    "        if not overrideBestC: #If automatic best c selection\n",
    "            LoadPath_bestc = trainingFeaturesPath + 'TrainingDB_' + str(TestingImageSet) + os.path.sep + 'best_c' + '_StopPerc_' + str(StopPercentageSLADS) + '.npy'\n",
    "            if not os.path.exists(LoadPath_bestc):\n",
    "                sys.exit('Error!!! The best c file ' + SavePath_bestc + ' does not exist. Check entry for ' + SavePath_bestc)\n",
    "            c = np.load(LoadPath_bestc)[0].astype(float)\n",
    "        else: #If manual best c selection\n",
    "            c = overrideBestCValue.astype(float)\n",
    "\n",
    "        #Directory setup for Theta\n",
    "        ThetaLoadPath = trainingFeaturesPath + TrainingDBName + os.path.sep + 'c_' + str(c) + os.path.sep\n",
    "        if not os.path.exists(ThetaLoadPath):                                                                                                                          \n",
    "            sys.exit('Error!!! Check folder ./ResultsAndData/TrainingSavedFeatures/TrainingDB_' + TrainingImageSet + ' for folder c_' + str(c))\n",
    "        \n",
    "        #Load Theta\n",
    "        Theta=np.transpose(np.load(ThetaLoadPath +'Theta' + '_StopPerc_' + str(StopPercentageSLADS) + '.npy'))\n",
    "\n",
    "        Beta = computeBeta(SizeImage)\n",
    "        StopCondParamsObject=StopCondParams()\n",
    "\n",
    "        \n",
    "        #Create an object to hold progressive development of model\n",
    "        testingResultsAverageObject = simulationResults() \n",
    "        testingResultsAverageObject.initialize()\n",
    "        \n",
    "        #For each of the possible testing sampling percentages\n",
    "        for q in tqdm(range(0,len(StopPercentageTestingSLADSArr)), desc = 'Testing Sampling Percentages', leave = True):\n",
    "            StopPercentageTestingSLADS = float(StopPercentageTestingSLADSArr[q])\n",
    "            StopCondParamsObject.initialize(Beta,StoppingThrehsold,50,2,StopPercentageTestingSLADS)\n",
    "            \n",
    "            # Run SLADS simulations\n",
    "            dataFileNames = glob.glob(testingDataImagesPath + \"/*\" + ImageExtension) #Obtain filenames for each set\n",
    "            numberTestFiles = len(dataFileNames)\n",
    "\n",
    "            #Perform SLADS on all of the images, saving statistics of interest in parallel\n",
    "            testingResultObject = Parallel(n_jobs=num_threads)(delayed(stats_parhelper)(numberTestFiles, loadOrGenerateInitialMask(loadPathInitialMask,MaskType,InitialMaskObject,SizeImage), codePath, TestingImageSet, SizeImage, StopCondParamsObject, Theta, TrainingInfoObject, TestingInfoObject, Resolution, ImageType, UpdateERDParamsObject, BatchSamplingParamsObject, testingOutputName(testingFeaturesPath, dataFileNames[i], StopPercentageSLADS, StopPercentageTestingSLADS),SimulationRun, i, ImageExtension, PlotResult, Classify, dataFileNames[i], errorPlot, isRunningParallel) for i in tqdm(range(0,numberTestFiles), desc = 'Testing Images', leave = True))\n",
    "            \n",
    "            #Create holding arrays for the results\n",
    "            mseTestingResults = []\n",
    "            ssimTestingResults = []\n",
    "            distortTestingResults = []\n",
    "            \n",
    "            #Extract results from returned object\n",
    "            for result in testingResultObject: \n",
    "                mseTestingResults.append(result.mseError)\n",
    "                ssimTestingResults.append(result.ssimError)\n",
    "                distortTestingResults.append(result.totalDistortion)\n",
    "                \n",
    "            #Store the Average DMs for all images for a particular testing sampling percentage\n",
    "            testingResultsAverageObject.saveAverageErrorData(np.mean(mseTestingResults), np.mean(ssimTestingResults), np.mean(distortTestingResults))\n",
    "\n",
    "            #Directory setup for individual images final save path\n",
    "            ImagesFinalSavePath = ImagesSavePath + 'StopTrainPerc_' + str(StopPercentageSLADS) + '_StopTestPerc_' + str(StopPercentageTestingSLADS) + os.path.sep\n",
    "            if os.path.exists(ImagesFinalSavePath): \n",
    "                shutil.rmtree(ImagesFinalSavePath)\n",
    "            os.makedirs(ImagesFinalSavePath)\n",
    "\n",
    "            #Obtain filenames for each of the images that were made\n",
    "            dataFileNames = glob.glob(testingFeaturesPath + \"/*\"+ImageExtension) \n",
    "            for j in range(0, len(dataFileNames)): #For each of the files\n",
    "                outputName = dataFileNames[j] #Grab the filename\n",
    "                outputName = re.sub(testingFeaturesPath, '', outputName) #Remove the directory prefix\n",
    "                shutil.move(dataFileNames[j], ImagesFinalSavePath+outputName) #Move them to an Images folder\n",
    "\n",
    "        #Plot the average DMs across the different testing sampling percentages given a particular training sampling percentage\n",
    "        testingSpecificStatisticsSavePathSuffix ='_StopTrainPerc_' + str(StopPercentageSLADS)\n",
    "        testingSpecificStatisticsSavePath = testingStatisticsSavePath + 'StopTrainPerc_' + str(StopPercentageSLADS) + os.path.sep    \n",
    "        \n",
    "        #Directory setup for specific testing statistics\n",
    "        if os.path.exists(testingSpecificStatisticsSavePath): \n",
    "            shutil.rmtree(testingSpecificStatisticsSavePath)\n",
    "        os.makedirs(testingSpecificStatisticsSavePath)      \n",
    "        \n",
    "        plotErrorData(testingSpecificStatisticsSavePath, testingSpecificStatisticsSavePathSuffix, testingResultsAverageObject, StopPercentageTestingSLADSArr.tolist(), testingPlotTitle, plotXLabel) #Plot and save the error data obtained during \n",
    "        trainTestAverageErrors.append(testingResultsAverageObject)\n",
    "        \n",
    "    #Plot the average DMS together in a single plot for all of the training sampling percentages\n",
    "    ttPlotAverageErrors(testingStatisticsSavePath, StopPercentageSLADSArr, StopPercentageTestingSLADSArr, trainTestAverageErrors)\n",
    "\n",
    "#AFTER INTENDED PROCEDURES (TRAINING/TESTING) HAVE BEEN PERFORMED\n",
    "print('\\n\\n\\n' + ('#' * int(consoleColumns)))\n",
    "print('PROGRAM COMPLETE')\n",
    "print('#' * int(consoleColumns) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN TESTING\n",
    "#==================================================================  \n",
    "if testingModel:\n",
    "    print('\\n\\n' + ('#' * int(consoleColumns)))\n",
    "    print('TESTING MODEL')\n",
    "    print(('#' * int(consoleColumns)) + '\\n')\n",
    "            \n",
    "    #Should the testing results be plotted\n",
    "    PlotResult='Y'\n",
    "\n",
    "    # If you want to use stopping function used, enter threshold (from Training), else leave at 0      \n",
    "    StoppingThrehsold = 0\n",
    "\n",
    "    #Static variables for sladssimulationonce call\n",
    "    isRunningParallel = True\n",
    "    SimulationRun = 1\n",
    "    errorPlot = True\n",
    "    \n",
    "    #Directory setup for training database\n",
    "    TrainingDBPath = trainingFeaturesPath + TrainingDBName\n",
    "    if not os.path.exists(TrainingDBPath): \n",
    "        sys.exit('Error!!! The folder ' + TrainingDBPath + ' does not exist. Check entry for ' + TrainingImageSet)\n",
    "\n",
    "    # Batch Sampling; If 'Y' set number of samples in each step in L-1 (NumSamplesPerIter)\n",
    "    BatchSample = 'N'\n",
    "    BatchSamplingParamsObject = BatchSamplingParams()\n",
    "    if BatchSample=='N':\n",
    "        BatchSamplingParamsObject.initialize(BatchSample,1)\n",
    "    else:\n",
    "        BatchSamplingParamsObject.initialize(BatchSample,NumSamplesPerIter)\n",
    "\n",
    "    UpdateERDParamsObject = UpdateERDParams()\n",
    "    UpdateERDParamsObject.initialize(Update_ERD,MinWindSize,MaxWindSize,1.5)\n",
    "\n",
    "    PercentageInitialMask = float(PercentageInitialMask)\n",
    "\n",
    "    Mask = loadOrGenerateInitialMask(loadPathInitialMask,MaskType,InitialMaskObject,SizeImage)\n",
    "    np.save(testingFeaturesPath + 'InitialMask', Mask)\n",
    "    savemat(testingFeaturesPath + 'InitialMask.mat', dict(Mask=Mask))\n",
    "    \n",
    "    testingPlotTitle = 'Averaged Total Testing Image Results'\n",
    "    plotXLabel = '% Sampled'\n",
    "    \n",
    "    #Create a list for holding Average results for each training sample percentage\n",
    "    trainTestAverageErrors = []\n",
    "    \n",
    "    #Sort the StopPercentageTestingSLADSArr from least to greatest\n",
    "    StopPercentageTestingSLADSArr\n",
    "    \n",
    "    #For each of the possible training sampling percentages (each with their own best c)\n",
    "    for p in tqdm(range(0,len(StopPercentageSLADSArr)), desc = 'Training Sampling Percentages', leave = True):\n",
    "        StopPercentageSLADS = float(StopPercentageSLADSArr[p])\n",
    "        \n",
    "        #Import the best c and theta values; Training for database must have been performed first\n",
    "        if not overrideBestC: #If automatic best c selection\n",
    "            LoadPath_bestc = trainingFeaturesPath + 'TrainingDB_' + str(TestingImageSet) + os.path.sep + 'best_c' + '_StopPerc_' + str(StopPercentageSLADS) + '.npy'\n",
    "            if not os.path.exists(LoadPath_bestc):\n",
    "                sys.exit('Error!!! The best c file ' + SavePath_bestc + ' does not exist. Check entry for ' + SavePath_bestc)\n",
    "            c = np.load(LoadPath_bestc)[0].astype(float)\n",
    "        else: #If manual best c selection\n",
    "            c = overrideBestCValue.astype(float)\n",
    "\n",
    "        #Directory setup for Theta\n",
    "        ThetaLoadPath = trainingFeaturesPath + TrainingDBName + os.path.sep + 'c_' + str(c) + os.path.sep\n",
    "        if not os.path.exists(ThetaLoadPath):                                                                                                                          \n",
    "            sys.exit('Error!!! Check folder ./ResultsAndData/TrainingSavedFeatures/TrainingDB_' + TrainingImageSet + ' for folder c_' + str(c))\n",
    "        \n",
    "        #Load Theta\n",
    "        Theta=np.transpose(np.load(ThetaLoadPath +'Theta' + '_StopPerc_' + str(StopPercentageSLADS) + '.npy'))\n",
    "\n",
    "        Beta = computeBeta(SizeImage)\n",
    "        StopCondParamsObject=StopCondParams()\n",
    "\n",
    "        #Create an object to hold progressive development of model\n",
    "        testingResultsAverageObject = simulationResults() \n",
    "        testingResultsAverageObject.initialize()\n",
    "        \n",
    "        #For the highest given Stop Percentage testing SLADS value\n",
    "        StopPercentageTestingSLADS = float(np.max(StopPercentageTestingSLADSArr)\n",
    "        StopCondParamsObject.initialize(Beta,StoppingThrehsold,50,2,StopPercentageTestingSLADS)\n",
    "\n",
    "        # Run SLADS simulations\n",
    "        dataFileNames = glob.glob(testingDataImagesPath + \"/*\" + ImageExtension) #Obtain filenames for each set\n",
    "        numberTestFiles = len(dataFileNames)\n",
    "\n",
    "        #Perform SLADS on all of the images, saving statistics of interest in parallel\n",
    "        testingResultObject = Parallel(n_jobs=num_threads)(delayed(stats_parhelper)(numberTestFiles, loadOrGenerateInitialMask(loadPathInitialMask,MaskType,InitialMaskObject,SizeImage), codePath, TestingImageSet, SizeImage, StopCondParamsObject, Theta, TrainingInfoObject, TestingInfoObject, Resolution, ImageType, UpdateERDParamsObject, BatchSamplingParamsObject, testingOutputName(testingFeaturesPath, dataFileNames[i], StopPercentageSLADS, StopPercentageTestingSLADS),SimulationRun, i, ImageExtension, PlotResult, Classify, dataFileNames[i], errorPlot, isRunningParallel) for i in tqdm(range(0,numberTestFiles), desc = 'Testing Images', leave = True))\n",
    "\n",
    "        #Create holding arrays for the results\n",
    "        mseTestingResults = []\n",
    "        ssimTestingResults = []\n",
    "        distortTestingResults = []\n",
    "\n",
    "        #Extract results from returned object\n",
    "        for result in testingResultObject: \n",
    "            mseTestingResults.append(result.mseError)\n",
    "            ssimTestingResults.append(result.ssimError)\n",
    "            distortTestingResults.append(result.totalDistortion)\n",
    "\n",
    "        #Store the Average DMs for all images for a particular testing sampling percentage\n",
    "        testingResultsAverageObject.saveAverageErrorData(np.mean(mseTestingResults), np.mean(ssimTestingResults), np.mean(distortTestingResults))\n",
    "\n",
    "        #Directory setup for individual images final save path\n",
    "        ImagesFinalSavePath = ImagesSavePath + 'StopTrainPerc_' + str(StopPercentageSLADS) + '_StopTestPerc_' + str(StopPercentageTestingSLADS) + os.path.sep\n",
    "        if os.path.exists(ImagesFinalSavePath): \n",
    "            shutil.rmtree(ImagesFinalSavePath)\n",
    "        os.makedirs(ImagesFinalSavePath)\n",
    "\n",
    "        #Obtain filenames for each of the images that were made\n",
    "        dataFileNames = glob.glob(testingFeaturesPath + \"/*\"+ImageExtension) \n",
    "        for j in range(0, len(dataFileNames)): #For each of the files\n",
    "            outputName = dataFileNames[j] #Grab the filename\n",
    "            outputName = re.sub(testingFeaturesPath, '', outputName) #Remove the directory prefix\n",
    "            shutil.move(dataFileNames[j], ImagesFinalSavePath+outputName) #Move them to an Images folder\n",
    "\n",
    "    #For each of the remaining stopping percentages extract data from the maximum\n",
    "    for q in tqdm(range(0,len(StopPercentageTestingSLADSArr)-1), desc = 'Testing Sampling Percentages', leave = True):\n",
    "                                           \n",
    "                                           \n",
    "                                           \n",
    "                                           \n",
    "                                           \n",
    "                                           \n",
    "                                           \n",
    "    #Plot the average DMs across the different testing sampling percentages given a particular training sampling percentage\n",
    "    testingSpecificStatisticsSavePathSuffix ='_StopTrainPerc_' + str(StopPercentageSLADS)\n",
    "    testingSpecificStatisticsSavePath = testingStatisticsSavePath + 'StopTrainPerc_' + str(StopPercentageSLADS) + os.path.sep    \n",
    "\n",
    "    #Directory setup for specific testing statistics\n",
    "    if os.path.exists(testingSpecificStatisticsSavePath): \n",
    "        shutil.rmtree(testingSpecificStatisticsSavePath)\n",
    "    os.makedirs(testingSpecificStatisticsSavePath)      \n",
    "\n",
    "    plotErrorData(testingSpecificStatisticsSavePath, testingSpecificStatisticsSavePathSuffix, testingResultsAverageObject, StopPercentageTestingSLADSArr.tolist(), testingPlotTitle, plotXLabel) #Plot and save the error data obtained during \n",
    "    trainTestAverageErrors.append(testingResultsAverageObject)\n",
    "\n",
    "    #Plot the average DMS together in a single plot for all of the training sampling percentages\n",
    "    ttPlotAverageErrors(testingStatisticsSavePath, StopPercentageSLADSArr, StopPercentageTestingSLADSArr, trainTestAverageErrors)\n",
    "\n",
    "#AFTER INTENDED PROCEDURES (TRAINING/TESTING) HAVE BEEN PERFORMED\n",
    "print('\\n\\n\\n' + ('#' * int(consoleColumns)))\n",
    "print('PROGRAM COMPLETE')\n",
    "print('#' * int(consoleColumns) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
